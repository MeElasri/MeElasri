{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Function Save Images","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom PIL import Image\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n\n\ndef load_img(filepath):\n    img = Image.open(filepath).convert('RGB')\n    img = img.resize((256, 256), Image.BICUBIC)\n    return img","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-03T22:44:38.679805Z","iopub.execute_input":"2023-02-03T22:44:38.680539Z","iopub.status.idle":"2023-02-03T22:44:38.889132Z","shell.execute_reply.started":"2023-02-03T22:44:38.680446Z","shell.execute_reply":"2023-02-03T22:44:38.888157Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef save_images(prediction, test_input, target, epoch):\n    #prediction = model(test_input, training= True)\n    #prediction = prediction.float().detach()\n    #prediction = prediction.cpu().numpy()\n    test_int = test_input[0].cpu().detach()\n    test_int = test_int.numpy()\n    test_int = np.transpose(test_int, (1, 2, 0))\n    #test_int = test_int.clip(0, 255)\n    \n    tar = target[0].cpu().detach()\n    tar =  tar.numpy()\n    tar = np.transpose(tar, (1, 2, 0)) \n    #tar = tar.clip(0, 255)\n    \n    pred = prediction[0].cpu().detach()\n    pred = pred.numpy()\n    pred = np.transpose(pred, (1, 2, 0))\n    #pred = pred.clip(0, 255)\n    \n    plt.figure(figsize = (15,15))\n    display_list= [test_int, tar, pred]\n    title = [\"Input Image\", \"Ground Truth\", \"Predicton Image\"]\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis(\"off\")\n    plt.savefig(f\"zellige dataset/epoch_{epoch}.jpg\")\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:44:41.894857Z","iopub.execute_input":"2023-02-03T22:44:41.895846Z","iopub.status.idle":"2023-02-03T22:44:41.905109Z","shell.execute_reply.started":"2023-02-03T22:44:41.895806Z","shell.execute_reply":"2023-02-03T22:44:41.903982Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Create Saved Folder","metadata":{}},{"cell_type":"code","source":"import os\nos.makedirs(\"zellige dataset\", exist_ok=False)\n#os.makedirs(\"checkpoint\", exist_ok=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:44:45.525692Z","iopub.execute_input":"2023-02-03T22:44:45.526062Z","iopub.status.idle":"2023-02-03T22:44:45.531087Z","shell.execute_reply.started":"2023-02-03T22:44:45.526029Z","shell.execute_reply":"2023-02-03T22:44:45.529928Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Function Read Images From Dataset","metadata":{}},{"cell_type":"code","source":"from os import listdir\nfrom os.path import join\nimport random\n\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nclass DatasetFromFolder(data.Dataset):\n    def __init__(self, image_dir, direction):\n        super(DatasetFromFolder, self).__init__()\n        self.direction = direction\n        self.a_path = join(image_dir, \"a\")\n        self.b_path = join(image_dir, \"b\")\n        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\n\n        transform_list = [transforms.ToTensor(),\n                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n\n        self.transform = transforms.Compose(transform_list)\n\n    def __getitem__(self, index):\n        a = Image.open(join(self.a_path, self.image_filenames[index])).convert('RGB')\n        b = Image.open(join(self.b_path, self.image_filenames[index])).convert('RGB')\n        a = a.resize((286, 286), Image.BICUBIC)\n        b = b.resize((286, 286), Image.BICUBIC)\n        a = transforms.ToTensor()(a)\n        b = transforms.ToTensor()(b)\n        w_offset = random.randint(0, max(0, 286 - 256 - 1))\n        h_offset = random.randint(0, max(0, 286 - 256 - 1))\n    \n        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n    \n        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n\n        if random.random() < 0.5:\n            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            a = a.index_select(2, idx)\n            b = b.index_select(2, idx)\n\n        if self.direction == \"a2b\":\n            return a, b\n        else:\n            return b, a\n\n    def __len__(self):\n        return len(self.image_filenames)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:44:47.673952Z","iopub.execute_input":"2023-02-03T22:44:47.674310Z","iopub.status.idle":"2023-02-03T22:44:49.587388Z","shell.execute_reply.started":"2023-02-03T22:44:47.674278Z","shell.execute_reply":"2023-02-03T22:44:49.586365Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"from os.path import join\n\n\ndef get_training_set(root_dir, direction):\n    train_dir = join(root_dir, \"Train\")\n\n    return DatasetFromFolder(train_dir, direction)\n\n\ndef get_test_set(root_dir, direction):\n    test_dir = join(root_dir, \"Test\")\n\n    return DatasetFromFolder(test_dir, direction)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:44:51.112831Z","iopub.execute_input":"2023-02-03T22:44:51.113563Z","iopub.status.idle":"2023-02-03T22:44:51.125846Z","shell.execute_reply.started":"2023-02-03T22:44:51.113517Z","shell.execute_reply":"2023-02-03T22:44:51.124642Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Network","metadata":{}},{"cell_type":"markdown","source":"# TransUnet Generator","metadata":{}},{"cell_type":"code","source":"import math\n\nfrom os.path import join as pjoin\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport copy\n#import logging\nimport math\n\nfrom os.path import join as pjoin\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\nfrom torch.nn.modules.utils import _pair\nfrom scipy import ndimage\n\n###########################################\n#########Resnet Network###################\n#########################################\n\nclass StdConv2d(nn.Conv2d):\n\n    def forward(self, x):\n        w = self.weight\n        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n        w = (w - m) / torch.sqrt(v + 1e-5)\n        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n                        self.dilation, self.groups)\n\n\ndef conv3x3(cin, cout, stride=1, groups=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n                     padding=1, bias=bias, groups=groups)\n\n\ndef conv1x1(cin, cout, stride=1, bias=False):\n    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n                     padding=0, bias=bias)\n\n\nclass PreActBottleneck(nn.Module):\n    \"\"\"Pre-activation (v2) bottleneck block.\n    \"\"\"\n\n    def __init__(self, cin, cout=None, cmid=None, stride=1):\n        super().__init__()\n        cout = cout or cin\n        cmid = cmid or cout//4\n\n        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv1 = conv1x1(cin, cmid, bias=False)\n        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n        self.conv3 = conv1x1(cmid, cout, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n        if (stride != 1 or cin != cout):\n            # Projection also with pre-activation according to paper.\n            self.downsample = conv1x1(cin, cout, stride, bias=False)\n            self.gn_proj = nn.GroupNorm(cout, cout)\n\n    def forward(self, x):\n\n        # Residual branch\n        residual = x\n        if hasattr(self, 'downsample'):\n            residual = self.downsample(x)\n            residual = self.gn_proj(residual)\n\n        # Unit's branch\n        y = self.relu(self.gn1(self.conv1(x)))\n        y = self.relu(self.gn2(self.conv2(y)))\n        y = self.gn3(self.conv3(y))\n\n        y = self.relu(residual + y)\n        return y\n\nclass ResNetV2(nn.Module):\n    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n\n    def __init__(self, block_units, width_factor):\n        super().__init__()\n        width = int(64 * width_factor)\n        self.width = width\n\n        self.root = nn.Sequential(OrderedDict([\n            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n            ('relu', nn.ReLU(inplace=True)),\n            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n        ]))\n\n        self.body = nn.Sequential(OrderedDict([\n            ('block1', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n                ))),\n            ('block2', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n                ))),\n            ('block3', nn.Sequential(OrderedDict(\n                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n                ))),\n        ]))\n\n    def forward(self, x):\n        features = []\n        b, c, in_size, _ = x.size()\n        x = self.root(x)\n        features.append(x)\n        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)\n        for i in range(len(self.body)-1):\n            x = self.body[i](x)\n            right_size = int(in_size / 4 / (i+1))\n            if x.size()[2] != right_size:\n                pad = right_size - x.size()[2]\n                assert pad < 3 and pad > 0, \"x {} should {}\".format(x.size(), right_size)\n                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n            else:\n                feat = x\n            features.append(feat)\n        x = self.body[-1](x)\n        return x, features[::-1]\n    \n#####################################################\n############Vision Transformer Net###################\n####################################################\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n\n\nclass Attention(nn.Module):\n    def __init__(self, vis):\n        super(Attention, self).__init__()\n        self.vis = vis\n        self.num_attention_heads = 12\n        self.attention_head_size = int(768 / self.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = Linear(768, self.all_head_size)\n        self.key = Linear(768, self.all_head_size)\n        self.value = Linear(768, self.all_head_size)\n\n        self.out = Linear(768, 768)\n        self.attn_dropout = Dropout(0.0)\n        self.proj_dropout = Dropout(1.0)\n\n        self.softmax = Softmax(dim=-1)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n    \n    def forward(self, hidden_states):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_probs = self.softmax(attention_scores)\n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        attention_output = self.proj_dropout(attention_output)\n        return attention_output, weights\n\n\nclass Mlp(nn.Module):\n    def __init__(self):\n        super(Mlp, self).__init__()\n        self.fc1 = Linear(768, 3072)\n        self.fc2 = Linear(3072, 768)\n        self.act_fn = ACT2FN[\"gelu\"]\n        self.dropout = Dropout(1.0)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.normal_(self.fc1.bias, std=1e-6)\n        nn.init.normal_(self.fc2.bias, std=1e-6)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Embeddings(nn.Module):\n    \"\"\"Construct the embeddings from patch, position embeddings.\n    \"\"\"\n    def __init__(self, img_size, in_channels=3):\n        super(Embeddings, self).__init__()\n        self.num_layers = (3, 4, 9)\n        self.width_factor = 1\n        img_size = _pair(img_size)\n\n        grid_size = (16,16)\n        patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n        patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n        n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  \n        \n        self.hybrid_model = ResNetV2(block_units=self.num_layers, width_factor=self.width_factor)\n        in_channels = self.hybrid_model.width * 16\n        \n        self.patch_embeddings = Conv2d(in_channels=in_channels,\n                                       out_channels=768,\n                                       kernel_size=patch_size,\n                                       stride=patch_size)\n        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, 768))\n\n        self.dropout = Dropout(1.0)\n\n\n    def forward(self, x):\n        x, features = self.hybrid_model(x)\n        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n        x = x.flatten(2)\n        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        return embeddings, features\n\n\nclass Block(nn.Module):\n    def __init__(self, vis):\n        super(Block, self).__init__()\n        self.hidden_size = 768\n        self.attention_norm = LayerNorm(768, eps=1e-6)\n        self.ffn_norm = LayerNorm(768, eps=1e-6)\n        self.ffn = Mlp()\n        self.attn = Attention(vis)\n\n    def forward(self, x):\n        h = x\n        x = self.attention_norm(x)\n        x, weights = self.attn(x)\n        x = x + h\n\n        h = x\n        x = self.ffn_norm(x)\n        x = self.ffn(x)\n        x = x + h\n        return x, weights\n\nclass Encoder(nn.Module):\n    def __init__(self, vis):\n        super(Encoder, self).__init__()\n        self.vis = vis\n        self.layer = nn.ModuleList()\n        self.encoder_norm = LayerNorm(768, eps=1e-6)\n        for _ in range(12):\n            layer = Block(vis)\n            self.layer.append(copy.deepcopy(layer))\n\n    def forward(self, hidden_states):\n        attn_weights = []\n        for layer_block in self.layer:\n            hidden_states, weights = layer_block(hidden_states)\n            if self.vis:\n                attn_weights.append(weights)\n        encoded = self.encoder_norm(hidden_states)\n        return encoded, attn_weights\n\n\nclass Transformer(nn.Module):\n    def __init__(self, img_size, vis):\n        super(Transformer, self).__init__()\n        self.embeddings = Embeddings(img_size=img_size)\n        self.encoder = Encoder(vis)\n\n    def forward(self, input_ids):\n        embedding_output, features = self.embeddings(input_ids)\n        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n        return encoded, attn_weights, features\n\n\nclass Conv2dReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=0,\n            stride=1,\n            use_batchnorm=True,\n    ):\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        bn = nn.BatchNorm2d(out_channels)\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            skip_channels=0,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n        self.conv1 = Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.conv2 = Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, x, skip=None):\n        x = self.up(x)\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass SegmentationHead(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n        super().__init__(conv2d, upsampling)\n\n\nclass DecoderCup(nn.Module):\n    def __init__(self):\n        super().__init__()\n        head_channels = 512\n        self.conv_more = Conv2dReLU(\n            768,\n            head_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=True,\n        )\n        decoder_channels = (256, 128, 64, 16)\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        out_channels = decoder_channels\n\n        skip_channels = [512, 256, 64, 16]\n        self.n_skip = 3\n        for i in range(4-self.n_skip):  # re-select the skip channels according to n_skip\n            skip_channels[3-i]=0\n\n        blocks = [\n            DecoderBlock(in_ch, out_ch, sk_ch) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, hidden_states, features=None):\n        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n        x = hidden_states.permute(0, 2, 1)\n        x = x.contiguous().view(B, hidden, h, w)\n        x = self.conv_more(x)\n        for i, decoder_block in enumerate(self.blocks):\n            if features is not None:\n                skip = features[i] if (i < self.n_skip) else None\n            else:\n                skip = None\n            x = decoder_block(x, skip=skip)\n        return x\n    \nclass Outconv(nn.Module):\n    def __init__(self, in_ch):\n        super(Outconv, self).__init__()\n        self.outconv = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_ch, 3, kernel_size=7, padding=0),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.outconv(x)\n        return x\n\n\n#////////////////////////////////////////////\n#Using Transformers as Encoder\n#///////////////////////////////////////////\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size=256, num_classes=21843, zero_head=False, vis=False):\n        super(VisionTransformer, self).__init__()\n        self.zero_head = zero_head\n        self.transformer = Transformer(img_size, vis)\n        self.decoder = DecoderCup()\n        self.segmentation_head = SegmentationHead(\n            in_channels=16,\n            out_channels=3,\n            kernel_size=3,\n        )\n\n    def forward(self, x):\n        if x.size()[1] == 1:\n            x = x.repeat(1,3,1,1)\n        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n        x = self.decoder(x, features)\n        x = self.segmentation_head(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:44:54.409269Z","iopub.execute_input":"2023-02-03T22:44:54.409635Z","iopub.status.idle":"2023-02-03T22:44:54.556651Z","shell.execute_reply.started":"2023-02-03T22:44:54.409601Z","shell.execute_reply":"2023-02-03T22:44:54.555355Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator Network","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\n\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    elif norm_type == 'switchable':\n        norm_layer = SwitchNorm2d\n    elif norm_type == 'none':\n        norm_layer = None\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == 'lambda':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n    elif opt.lr_policy == 'cosine':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n    return scheduler\n\n\n# update learning rate (called once every epoch)\ndef update_learning_rate(scheduler, optimizer):\n    scheduler.step()\n    lr = optimizer.param_groups[0]['lr']\n    print('learning rate = %.7f' % lr)\n\n\ndef init_weights(net, init_type='normal', gain=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init_type == 'orthogonal':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print('initialize network with %s' % init_type)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\n    net.to(gpu_id)\n    init_weights(net, init_type, gain=init_gain)\n    return net\n\nclass Inconv(nn.Module):\n    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\n        super(Inconv, self).__init__()\n        self.inconv = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0,\n                      bias=use_bias),\n            norm_layer(out_ch),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        x = self.inconv(x)\n        return x\n\nclass Up(nn.Module):\n    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\n        super(Up, self).__init__()\n        self.up = nn.Sequential(\n            nn.ConvTranspose2d(in_ch, out_ch,\n                               kernel_size=3, stride=2,\n                               padding=1, output_padding=1,\n                               bias=use_bias),\n            norm_layer(out_ch),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\n#######################################\n#######Define Discriminator model######\n#######################################\ndef define_D(input_nc, ndf, netD,\n             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\n    net = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if netD == 'basic':\n        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif netD == 'n_layers':\n        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif netD == 'pixel':\n        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    else:\n        raise NotImplementedError('Discriminator model name [%s] is not recognized' % net)\n\n    return init_net(net, init_type, init_gain, gpu_id)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass PixelDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(PixelDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        self.net = [\n            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n            norm_layer(ndf * 2),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n\n        if use_sigmoid:\n            self.net.append(nn.Sigmoid())\n\n        self.net = nn.Sequential(*self.net)\n\n    def forward(self, input):\n        return self.net(input)\n\n\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n        super(GANLoss, self).__init__()\n        self.register_buffer('real_label', torch.tensor(target_real_label))\n        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(input)\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:45:03.762707Z","iopub.execute_input":"2023-02-03T22:45:03.763087Z","iopub.status.idle":"2023-02-03T22:45:03.832539Z","shell.execute_reply.started":"2023-02-03T22:45:03.763051Z","shell.execute_reply":"2023-02-03T22:45:03.829933Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation model (IS | FID | SSIM)","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import adaptive_avg_pool2d\ndef calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n                    cuda=False):\n    model.eval()\n    act=np.empty((len(images), dims))\n    \n    if cuda:\n        batch=images.cuda()\n    else:\n        batch=images\n    pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n    if pred.size(2) != 1 or pred.size(3) != 1:\n        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n    \n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:45:09.843423Z","iopub.execute_input":"2023-02-03T22:45:09.843779Z","iopub.status.idle":"2023-02-03T22:45:09.854097Z","shell.execute_reply.started":"2023-02-03T22:45:09.843748Z","shell.execute_reply":"2023-02-03T22:45:09.853161Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import scipy\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        'Training and test mean vectors have different lengths'\n    assert sigma1.shape == sigma2.shape, \\\n        'Training and test covariances have different dimensions'\n\n    diff = mu1 - mu2\n\n    \n    covmean, _ = scipy.linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = ('fid calculation produces singular product; '\n               'adding %s to diagonal of cov estimates') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    \n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError('Imaginary component {}'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) +\n            np.trace(sigma2) - 2 * tr_covmean)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:45:12.881463Z","iopub.execute_input":"2023-02-03T22:45:12.881811Z","iopub.status.idle":"2023-02-03T22:45:12.893209Z","shell.execute_reply.started":"2023-02-03T22:45:12.881779Z","shell.execute_reply":"2023-02-03T22:45:12.892216Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def calculate_fretchet(images_real,images_fake,model):\n     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n    \n     \"\"\"get fretched distance\"\"\"\n     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n     return fid_value","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:45:16.651599Z","iopub.execute_input":"2023-02-03T22:45:16.651980Z","iopub.status.idle":"2023-02-03T22:45:16.659559Z","shell.execute_reply.started":"2023-02-03T22:45:16.651935Z","shell.execute_reply":"2023-02-03T22:45:16.658503Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Train Step","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\nimport argparse\nimport os\nfrom math import log10\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.backends.cudnn as cudnn\n\n\n# Training settings\nparser = argparse.ArgumentParser(description='TransEdge2zellige')\n#parser.add_argument('--dataset', required=True, help='Zellig_dataset')\nparser.add_argument('--batch_size', type=int, default=1, help='training batch size')\nparser.add_argument('--test_batch_size', type=int, default=1, help='testing batch size')\nparser.add_argument('--direction', type=str, default='b2a', help='a2b or b2a')\nparser.add_argument('--input_nc', type=int, default=3, help='input image channels')\nparser.add_argument('--output_nc', type=int, default=3, help='output image channels')\nparser.add_argument('--ngf', type=int, default=64, help='generator filters in first conv layer')\nparser.add_argument('--ndf', type=int, default=64, help='discriminator filters in first conv layer')\nparser.add_argument('--epoch_count', type=int, default=200, help='the starting epoch count')\nparser.add_argument('--niter', type=int, default=1, help='# of iter at starting learning rate')\nparser.add_argument('--niter_decay', type=int, default=400, help='# of iter to linearly decay learning rate to zero')\nparser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')\nparser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau|cosine')\nparser.add_argument('--n_epochs', type=int, default=200, help='numbers of epochs')\nparser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\nparser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\nparser.add_argument('--cuda', action='store_true', help='use cuda?')\nparser.add_argument('--threads', type=int, default=2, help='number of threads for data loader to use')\nparser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\nparser.add_argument('--lamb', type=int, default=10, help='weight on L1 term in objective')\nparser.add_argument('--lamb2', type=int, default=0.5, help='weight on L2 term in objective')\nparser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\nopt = parser.parse_args()\n\nprint(opt)\n\nprint('===> Loading datasets')\nroot_path = \"/kaggle/input/data-faces/CUHK/\"\ntrain_set = get_training_set(root_path, opt.direction)\nprint(train_set)\ntest_set = get_test_set(root_path, opt.direction)\ntraining_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batch_size, shuffle=True)\ntesting_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.test_batch_size, shuffle=True)\n\ndevice = torch.device(\"cuda\")\nif opt.epoch_count != 1:\n    print('===> Building previous models')\n    net_g=torch.load('/kaggle/input/epoc200/netG_model_epoch_200 (1).pth')\n    net_g = net_g.to(device)\n    net_d = torch.load('/kaggle/input/epoc200/netD_model_epoch_200 (1).pth')   \n    net_d = net_d.to(device)\nelse:\n    \n    print('===> Building models')\n    net_g = VisionTransformer(img_size = 256,num_classes = 9)\n    #net_g = nn.DataParallel(net_g)\n    net_g = net_g.to(device)\n    #net_g.train()\n    net_d = define_D(opt.input_nc + opt.output_nc, opt.ndf, 'pixel', gpu_id=device)\n\ncriterionGAN = GANLoss().to(device)\ncriterionL1 = nn.L1Loss().to(device)\ncriterionL2 = nn.MSELoss().to(device)\ncriterionMSE = nn.MSELoss().to(device)\n\n# setup optimizer\noptimizer_g = optim.Adam(net_g.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\noptimizer_d = optim.Adam(net_d.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\nnet_g_scheduler = get_scheduler(optimizer_g, opt)\nnet_d_scheduler = get_scheduler(optimizer_d, opt)\n\n#fid =[]\n#loss_dis =[]\n#loss_gen = []\n\nfor epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n    # train\n\n    for iteration, batch in enumerate(training_data_loader, 1):\n        # forward\n        real_a, real_b = batch[0].to(device), batch[1].to(device) \n        fake_b = net_g(real_a.float())\n\n        ######################\n        # (1) Update D network\n        ######################\n\n        optimizer_d.zero_grad()\n        \n        # train with fake\n        fake_ab = torch.cat((real_a, fake_b), 1)\n        pred_fake = net_d.forward(fake_ab.detach())\n        loss_d_fake = criterionGAN(pred_fake, False)\n\n        # train with real\n        real_ab = torch.cat((real_a, real_b), 1)\n        pred_real = net_d.forward(real_ab)\n        loss_d_real = criterionGAN(pred_real, True)\n        \n        # Combined D loss\n        loss_d = ((loss_d_fake) + (loss_d_real)) * 0.5\n        \n        loss_d.backward()\n       \n        optimizer_d.step()\n\n        ######################\n        # (2) Update G network\n        ######################\n\n        optimizer_g.zero_grad()\n\n        # First, G(A) should fake the discriminator\n        fake_ab = torch.cat((real_a, fake_b), 1)\n        pred_fake = net_d.forward(fake_ab)\n        loss_g_gan = criterionGAN(pred_fake, True)\n\n        # Second, G(A) = B\n        loss_g_l1 = criterionL1(fake_b, real_b) * opt.lamb\n        loss_g_l2 = criterionL2(pred_fake, real_b)*opt.lamb2\n        #loss_gen.append(loss_g)\n        \n        loss_g = loss_g_gan + loss_g_l1+loss_g_l2\n        \n            \n        \n        loss_g.backward()\n        optimizer_g.step()\n        \n        print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\n            epoch, iteration, len(training_data_loader), loss_d.item(), loss_g.item()))\n    #loss_dis.append(loss_d.item()) \n    #loss_gen.append(loss_g.item())\n\n    #losses.append((loss_disc, loss_gen))\n    update_learning_rate(net_g_scheduler, optimizer_g)\n    update_learning_rate(net_d_scheduler, optimizer_d)\n\n    #for iteration, batch in enumerate(testing_data_loader, 1):\n        # forward\n    batch = iter(testing_data_loader)\n    input, target = batch.next()\n    input = input.to(device)\n    target = target.to(device)\n    \n    print(input.shape)\n\n    # test\n    #avg_psnr = 0\n    #for batch in testing_data_loader:\n        #input, target = batch[0].to(device), batch[1].to(device)\n\n    prediction = net_g(input.float())\n    save_images(prediction, input, target,epoch)\n    fretchet_dist=calculate_fretchet(target,prediction,net_g)\n  #  fretchet_dist=calculate_fretchet(target,prediction,net_g)\n   # print(\"FID:=== \",fretchet_dist)\n        #save_img(prediction,f\"zellige dataset/epoch_{epoch}.jpg\")\n        \n        #mse = criterionMSE(prediction, target)\n        #psnr = 10 * log10(1 / mse.item())\n        #avg_psnr += psnr\n    print(\"===> FID: {%.4f} \"%fretchet_dist.item())\n    #checkpoint\n    if epoch % 100 == 0:\n        if not os.path.exists(\"checkpoint\"):\n            os.mkdir(\"checkpoint\")\n            \n        net_g_model_out_path = \"checkpoint/netG_model_epoch_{}.pth\".format(epoch)\n        net_d_model_out_path = \"checkpoint/netD_model_epoch_{}.pth\".format(epoch)\n        torch.save(net_g, net_g_model_out_path)\n        torch.save(net_d, net_d_model_out_path)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-03T22:45:18.579279Z","iopub.execute_input":"2023-02-03T22:45:18.579717Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Namespace(batch_size=1, beta1=0.5, cuda=False, direction='b2a', epoch_count=200, fff='/root/.local/share/jupyter/runtime/kernel-320c1865-13e6-44f0-8c55-0656c5107a09.json', input_nc=3, lamb=10, lamb2=0.5, lr=0.0001, lr_decay_iters=50, lr_policy='lambda', n_epochs=200, ndf=64, ngf=64, niter=1, niter_decay=400, output_nc=3, seed=123, test_batch_size=1, threads=2)\n===> Loading datasets\n<__main__.DatasetFromFolder object at 0x7f9df1d0d910>\n===> Building previous models\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1, 3, 256, 256])) that is different to the input size (torch.Size([1, 1, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[200](1/88): Loss_D: 0.2597 Loss_G: 1.2177\n===> Epoch[200](2/88): Loss_D: 0.2527 Loss_G: 1.1582\n===> Epoch[200](3/88): Loss_D: 0.2539 Loss_G: 1.0242\n===> Epoch[200](4/88): Loss_D: 0.2502 Loss_G: 1.0766\n===> Epoch[200](5/88): Loss_D: 0.2410 Loss_G: 1.1088\n===> Epoch[200](6/88): Loss_D: 0.1633 Loss_G: 1.6035\n===> Epoch[200](7/88): Loss_D: 0.2355 Loss_G: 1.4038\n===> Epoch[200](8/88): Loss_D: 0.2503 Loss_G: 1.2301\n===> Epoch[200](9/88): Loss_D: 0.2376 Loss_G: 1.1805\n===> Epoch[200](10/88): Loss_D: 0.2391 Loss_G: 1.1730\n===> Epoch[200](11/88): Loss_D: 0.2448 Loss_G: 1.2564\n===> Epoch[200](12/88): Loss_D: 0.2491 Loss_G: 1.0626\n===> Epoch[200](13/88): Loss_D: 0.2625 Loss_G: 0.9276\n===> Epoch[200](14/88): Loss_D: 0.2347 Loss_G: 1.4429\n===> Epoch[200](15/88): Loss_D: 0.2552 Loss_G: 1.2358\n===> Epoch[200](16/88): Loss_D: 0.2507 Loss_G: 1.0947\n===> Epoch[200](17/88): Loss_D: 0.2597 Loss_G: 1.2470\n===> Epoch[200](18/88): Loss_D: 0.2462 Loss_G: 1.2663\n===> Epoch[200](19/88): Loss_D: 0.2488 Loss_G: 1.0876\n===> Epoch[200](20/88): Loss_D: 0.2446 Loss_G: 1.2336\n===> Epoch[200](21/88): Loss_D: 0.2394 Loss_G: 1.2195\n===> Epoch[200](22/88): Loss_D: 0.1773 Loss_G: 1.6206\n===> Epoch[200](23/88): Loss_D: 0.2350 Loss_G: 1.3185\n===> Epoch[200](24/88): Loss_D: 0.2673 Loss_G: 1.1260\n===> Epoch[200](25/88): Loss_D: 0.2430 Loss_G: 1.2713\n===> Epoch[200](26/88): Loss_D: 0.2557 Loss_G: 1.2751\n===> Epoch[200](27/88): Loss_D: 0.2367 Loss_G: 1.2410\n===> Epoch[200](28/88): Loss_D: 0.1889 Loss_G: 1.6640\n===> Epoch[200](29/88): Loss_D: 0.2397 Loss_G: 1.3055\n===> Epoch[200](30/88): Loss_D: 0.2314 Loss_G: 1.4333\n===> Epoch[200](31/88): Loss_D: 0.2503 Loss_G: 1.2225\n===> Epoch[200](32/88): Loss_D: 0.2494 Loss_G: 1.3029\n===> Epoch[200](33/88): Loss_D: 0.2485 Loss_G: 1.3920\n===> Epoch[200](34/88): Loss_D: 0.2460 Loss_G: 1.2053\n===> Epoch[200](35/88): Loss_D: 0.2386 Loss_G: 1.4755\n===> Epoch[200](36/88): Loss_D: 0.2135 Loss_G: 1.4589\n===> Epoch[200](37/88): Loss_D: 0.2469 Loss_G: 1.1990\n===> Epoch[200](38/88): Loss_D: 0.2405 Loss_G: 1.4621\n===> Epoch[200](39/88): Loss_D: 0.2216 Loss_G: 2.3398\n===> Epoch[200](40/88): Loss_D: 0.2627 Loss_G: 1.3341\n===> Epoch[200](41/88): Loss_D: 0.2527 Loss_G: 1.2739\n===> Epoch[200](42/88): Loss_D: 0.2372 Loss_G: 1.2811\n===> Epoch[200](43/88): Loss_D: 0.2312 Loss_G: 1.4128\n===> Epoch[200](44/88): Loss_D: 0.2590 Loss_G: 1.3127\n===> Epoch[200](45/88): Loss_D: 0.2433 Loss_G: 1.3171\n===> Epoch[200](46/88): Loss_D: 0.2490 Loss_G: 1.2539\n===> Epoch[200](47/88): Loss_D: 0.2209 Loss_G: 1.4618\n===> Epoch[200](48/88): Loss_D: 0.2541 Loss_G: 1.3380\n===> Epoch[200](49/88): Loss_D: 0.2383 Loss_G: 1.4161\n===> Epoch[200](50/88): Loss_D: 0.2475 Loss_G: 1.2898\n===> Epoch[200](51/88): Loss_D: 0.2782 Loss_G: 1.1465\n===> Epoch[200](52/88): Loss_D: 0.2272 Loss_G: 1.3225\n===> Epoch[200](53/88): Loss_D: 0.2591 Loss_G: 1.2917\n===> Epoch[200](54/88): Loss_D: 0.2402 Loss_G: 1.1110\n===> Epoch[200](55/88): Loss_D: 0.2623 Loss_G: 1.0623\n===> Epoch[200](56/88): Loss_D: 0.2310 Loss_G: 1.3617\n===> Epoch[200](57/88): Loss_D: 0.2566 Loss_G: 1.2459\n===> Epoch[200](58/88): Loss_D: 0.2344 Loss_G: 1.3554\n===> Epoch[200](59/88): Loss_D: 0.2624 Loss_G: 0.9892\n===> Epoch[200](60/88): Loss_D: 0.2426 Loss_G: 1.4306\n===> Epoch[200](61/88): Loss_D: 0.2530 Loss_G: 1.0955\n===> Epoch[200](62/88): Loss_D: 0.2414 Loss_G: 1.2046\n===> Epoch[200](63/88): Loss_D: 0.2445 Loss_G: 1.2481\n===> Epoch[200](64/88): Loss_D: 0.2195 Loss_G: 1.4512\n===> Epoch[200](65/88): Loss_D: 0.2457 Loss_G: 1.1738\n===> Epoch[200](66/88): Loss_D: 0.2259 Loss_G: 1.2377\n===> Epoch[200](67/88): Loss_D: 0.2488 Loss_G: 1.4177\n===> Epoch[200](68/88): Loss_D: 0.2515 Loss_G: 1.1274\n===> Epoch[200](69/88): Loss_D: 0.2588 Loss_G: 1.0604\n===> Epoch[200](70/88): Loss_D: 0.2283 Loss_G: 1.3555\n===> Epoch[200](71/88): Loss_D: 0.2322 Loss_G: 1.3422\n===> Epoch[200](72/88): Loss_D: 0.2187 Loss_G: 1.4772\n===> Epoch[200](73/88): Loss_D: 0.2602 Loss_G: 1.1902\n===> Epoch[200](74/88): Loss_D: 0.2569 Loss_G: 1.2409\n===> Epoch[200](75/88): Loss_D: 0.1864 Loss_G: 1.8598\n===> Epoch[200](76/88): Loss_D: 0.2424 Loss_G: 1.3810\n===> Epoch[200](77/88): Loss_D: 0.2449 Loss_G: 1.3294\n===> Epoch[200](78/88): Loss_D: 0.1865 Loss_G: 1.6794\n===> Epoch[200](79/88): Loss_D: 0.2501 Loss_G: 1.3806\n===> Epoch[200](80/88): Loss_D: 0.2471 Loss_G: 1.3896\n===> Epoch[200](81/88): Loss_D: 0.2458 Loss_G: 1.1267\n===> Epoch[200](82/88): Loss_D: 0.2094 Loss_G: 1.3882\n===> Epoch[200](83/88): Loss_D: 0.2508 Loss_G: 1.0723\n===> Epoch[200](84/88): Loss_D: 0.2578 Loss_G: 1.2431\n===> Epoch[200](85/88): Loss_D: 0.2597 Loss_G: 1.2340\n===> Epoch[200](86/88): Loss_D: 0.2523 Loss_G: 1.2184\n===> Epoch[200](87/88): Loss_D: 0.2487 Loss_G: 1.2933\n===> Epoch[200](88/88): Loss_D: 0.2382 Loss_G: 1.3081\nlearning rate = 0.0000501\nlearning rate = 0.0000501\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0018} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[201](1/88): Loss_D: 0.2396 Loss_G: 1.1820\n===> Epoch[201](2/88): Loss_D: 0.2445 Loss_G: 1.3492\n===> Epoch[201](3/88): Loss_D: 0.2326 Loss_G: 1.4191\n===> Epoch[201](4/88): Loss_D: 0.2556 Loss_G: 1.2540\n===> Epoch[201](5/88): Loss_D: 0.2010 Loss_G: 1.4185\n===> Epoch[201](6/88): Loss_D: 0.2048 Loss_G: 2.4430\n===> Epoch[201](7/88): Loss_D: 0.2476 Loss_G: 1.2943\n===> Epoch[201](8/88): Loss_D: 0.2041 Loss_G: 1.9923\n===> Epoch[201](9/88): Loss_D: 0.2475 Loss_G: 1.5078\n===> Epoch[201](10/88): Loss_D: 0.2484 Loss_G: 1.4056\n===> Epoch[201](11/88): Loss_D: 0.2570 Loss_G: 1.2779\n===> Epoch[201](12/88): Loss_D: 0.1903 Loss_G: 1.6670\n===> Epoch[201](13/88): Loss_D: 0.2447 Loss_G: 1.2866\n===> Epoch[201](14/88): Loss_D: 0.2525 Loss_G: 1.0259\n===> Epoch[201](15/88): Loss_D: 0.2448 Loss_G: 1.2577\n===> Epoch[201](16/88): Loss_D: 0.2494 Loss_G: 1.3604\n===> Epoch[201](17/88): Loss_D: 0.2479 Loss_G: 1.1936\n===> Epoch[201](18/88): Loss_D: 0.2480 Loss_G: 1.1413\n===> Epoch[201](19/88): Loss_D: 0.2420 Loss_G: 1.2135\n===> Epoch[201](20/88): Loss_D: 0.1982 Loss_G: 1.5461\n===> Epoch[201](21/88): Loss_D: 0.2437 Loss_G: 1.2713\n===> Epoch[201](22/88): Loss_D: 0.2442 Loss_G: 1.4855\n===> Epoch[201](23/88): Loss_D: 0.2567 Loss_G: 1.2151\n===> Epoch[201](24/88): Loss_D: 0.1963 Loss_G: 1.4167\n===> Epoch[201](25/88): Loss_D: 0.2310 Loss_G: 1.2272\n===> Epoch[201](26/88): Loss_D: 0.2387 Loss_G: 1.2530\n===> Epoch[201](27/88): Loss_D: 0.2433 Loss_G: 1.1237\n===> Epoch[201](28/88): Loss_D: 0.2488 Loss_G: 1.2170\n===> Epoch[201](29/88): Loss_D: 0.2431 Loss_G: 1.2999\n===> Epoch[201](30/88): Loss_D: 0.2338 Loss_G: 1.3840\n===> Epoch[201](31/88): Loss_D: 0.2528 Loss_G: 1.1661\n===> Epoch[201](32/88): Loss_D: 0.2633 Loss_G: 1.1341\n===> Epoch[201](33/88): Loss_D: 0.2320 Loss_G: 1.4874\n===> Epoch[201](34/88): Loss_D: 0.2388 Loss_G: 1.2655\n===> Epoch[201](35/88): Loss_D: 0.2511 Loss_G: 1.3009\n===> Epoch[201](36/88): Loss_D: 0.2623 Loss_G: 1.2210\n===> Epoch[201](37/88): Loss_D: 0.2504 Loss_G: 1.1675\n===> Epoch[201](38/88): Loss_D: 0.2491 Loss_G: 1.1384\n===> Epoch[201](39/88): Loss_D: 0.2505 Loss_G: 1.0601\n===> Epoch[201](40/88): Loss_D: 0.2268 Loss_G: 1.5129\n===> Epoch[201](41/88): Loss_D: 0.2155 Loss_G: 1.5093\n===> Epoch[201](42/88): Loss_D: 0.2500 Loss_G: 1.0940\n===> Epoch[201](43/88): Loss_D: 0.2394 Loss_G: 1.2073\n===> Epoch[201](44/88): Loss_D: 0.2444 Loss_G: 1.2632\n===> Epoch[201](45/88): Loss_D: 0.2552 Loss_G: 1.2521\n===> Epoch[201](46/88): Loss_D: 0.2469 Loss_G: 1.2940\n===> Epoch[201](47/88): Loss_D: 0.2275 Loss_G: 1.2998\n===> Epoch[201](48/88): Loss_D: 0.2501 Loss_G: 1.3767\n===> Epoch[201](49/88): Loss_D: 0.2419 Loss_G: 1.2071\n===> Epoch[201](50/88): Loss_D: 0.1988 Loss_G: 1.4835\n===> Epoch[201](51/88): Loss_D: 0.2382 Loss_G: 1.1861\n===> Epoch[201](52/88): Loss_D: 0.2462 Loss_G: 1.2230\n===> Epoch[201](53/88): Loss_D: 0.2429 Loss_G: 1.3509\n===> Epoch[201](54/88): Loss_D: 0.2666 Loss_G: 1.2531\n===> Epoch[201](55/88): Loss_D: 0.2234 Loss_G: 1.1025\n===> Epoch[201](56/88): Loss_D: 0.2405 Loss_G: 1.5173\n===> Epoch[201](57/88): Loss_D: 0.2777 Loss_G: 1.1852\n===> Epoch[201](58/88): Loss_D: 0.2557 Loss_G: 1.1696\n===> Epoch[201](59/88): Loss_D: 0.2537 Loss_G: 1.1411\n===> Epoch[201](60/88): Loss_D: 0.2321 Loss_G: 1.4325\n===> Epoch[201](61/88): Loss_D: 0.2297 Loss_G: 1.4098\n===> Epoch[201](62/88): Loss_D: 0.1681 Loss_G: 1.6021\n===> Epoch[201](63/88): Loss_D: 0.2611 Loss_G: 1.1893\n===> Epoch[201](64/88): Loss_D: 0.2332 Loss_G: 1.3434\n===> Epoch[201](65/88): Loss_D: 0.2351 Loss_G: 1.3930\n===> Epoch[201](66/88): Loss_D: 0.2503 Loss_G: 1.1206\n===> Epoch[201](67/88): Loss_D: 0.2496 Loss_G: 1.3278\n===> Epoch[201](68/88): Loss_D: 0.2239 Loss_G: 1.5762\n===> Epoch[201](69/88): Loss_D: 0.2449 Loss_G: 1.2722\n===> Epoch[201](70/88): Loss_D: 0.2403 Loss_G: 1.2293\n===> Epoch[201](71/88): Loss_D: 0.2561 Loss_G: 1.0598\n===> Epoch[201](72/88): Loss_D: 0.2483 Loss_G: 1.1732\n===> Epoch[201](73/88): Loss_D: 0.2459 Loss_G: 1.1238\n===> Epoch[201](74/88): Loss_D: 0.2458 Loss_G: 1.1279\n===> Epoch[201](75/88): Loss_D: 0.2633 Loss_G: 1.3346\n===> Epoch[201](76/88): Loss_D: 0.2537 Loss_G: 1.2948\n===> Epoch[201](77/88): Loss_D: 0.2603 Loss_G: 1.1579\n===> Epoch[201](78/88): Loss_D: 0.2458 Loss_G: 0.9513\n===> Epoch[201](79/88): Loss_D: 0.2563 Loss_G: 1.1937\n===> Epoch[201](80/88): Loss_D: 0.2300 Loss_G: 1.3714\n===> Epoch[201](81/88): Loss_D: 0.2556 Loss_G: 1.1781\n===> Epoch[201](82/88): Loss_D: 0.2317 Loss_G: 1.4162\n===> Epoch[201](83/88): Loss_D: 0.2507 Loss_G: 1.2437\n===> Epoch[201](84/88): Loss_D: 0.2169 Loss_G: 1.6427\n===> Epoch[201](85/88): Loss_D: 0.2508 Loss_G: 1.2404\n===> Epoch[201](86/88): Loss_D: 0.2257 Loss_G: 1.4920\n===> Epoch[201](87/88): Loss_D: 0.2270 Loss_G: 1.2517\n===> Epoch[201](88/88): Loss_D: 0.2501 Loss_G: 1.1401\nlearning rate = 0.0000499\nlearning rate = 0.0000499\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0001} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[202](1/88): Loss_D: 0.2293 Loss_G: 1.3204\n===> Epoch[202](2/88): Loss_D: 0.2397 Loss_G: 1.1248\n===> Epoch[202](3/88): Loss_D: 0.2636 Loss_G: 0.9059\n===> Epoch[202](4/88): Loss_D: 0.1841 Loss_G: 1.6093\n===> Epoch[202](5/88): Loss_D: 0.2216 Loss_G: 1.4241\n===> Epoch[202](6/88): Loss_D: 0.2622 Loss_G: 1.0607\n===> Epoch[202](7/88): Loss_D: 0.2459 Loss_G: 1.2360\n===> Epoch[202](8/88): Loss_D: 0.2497 Loss_G: 1.3102\n===> Epoch[202](9/88): Loss_D: 0.2356 Loss_G: 1.3301\n===> Epoch[202](10/88): Loss_D: 0.2650 Loss_G: 1.1012\n===> Epoch[202](11/88): Loss_D: 0.2428 Loss_G: 1.1521\n===> Epoch[202](12/88): Loss_D: 0.2517 Loss_G: 1.2528\n===> Epoch[202](13/88): Loss_D: 0.2527 Loss_G: 1.2505\n===> Epoch[202](14/88): Loss_D: 0.2129 Loss_G: 1.4434\n===> Epoch[202](15/88): Loss_D: 0.2710 Loss_G: 1.2573\n===> Epoch[202](16/88): Loss_D: 0.2486 Loss_G: 1.1773\n===> Epoch[202](17/88): Loss_D: 0.2175 Loss_G: 1.4792\n===> Epoch[202](18/88): Loss_D: 0.2271 Loss_G: 1.4089\n===> Epoch[202](19/88): Loss_D: 0.2478 Loss_G: 1.2160\n===> Epoch[202](20/88): Loss_D: 0.2587 Loss_G: 1.2754\n===> Epoch[202](21/88): Loss_D: 0.1952 Loss_G: 1.5641\n===> Epoch[202](22/88): Loss_D: 0.2534 Loss_G: 1.1136\n===> Epoch[202](23/88): Loss_D: 0.1833 Loss_G: 1.7877\n===> Epoch[202](24/88): Loss_D: 0.2724 Loss_G: 1.1359\n===> Epoch[202](25/88): Loss_D: 0.2348 Loss_G: 2.3968\n===> Epoch[202](26/88): Loss_D: 0.2600 Loss_G: 1.3961\n===> Epoch[202](27/88): Loss_D: 0.2521 Loss_G: 1.2999\n===> Epoch[202](28/88): Loss_D: 0.2305 Loss_G: 1.5478\n===> Epoch[202](29/88): Loss_D: 0.2612 Loss_G: 1.2979\n===> Epoch[202](30/88): Loss_D: 0.2594 Loss_G: 1.0389\n===> Epoch[202](31/88): Loss_D: 0.2362 Loss_G: 1.2259\n===> Epoch[202](32/88): Loss_D: 0.2596 Loss_G: 1.2463\n===> Epoch[202](33/88): Loss_D: 0.2581 Loss_G: 1.2448\n===> Epoch[202](34/88): Loss_D: 0.2545 Loss_G: 1.2160\n===> Epoch[202](35/88): Loss_D: 0.2377 Loss_G: 1.3238\n===> Epoch[202](36/88): Loss_D: 0.2398 Loss_G: 1.1362\n===> Epoch[202](37/88): Loss_D: 0.2431 Loss_G: 1.2644\n===> Epoch[202](38/88): Loss_D: 0.2626 Loss_G: 1.0987\n===> Epoch[202](39/88): Loss_D: 0.2430 Loss_G: 1.1819\n===> Epoch[202](40/88): Loss_D: 0.2594 Loss_G: 1.0806\n===> Epoch[202](41/88): Loss_D: 0.2619 Loss_G: 1.0146\n===> Epoch[202](42/88): Loss_D: 0.2442 Loss_G: 1.4417\n===> Epoch[202](43/88): Loss_D: 0.2572 Loss_G: 1.2142\n===> Epoch[202](44/88): Loss_D: 0.2404 Loss_G: 1.2445\n===> Epoch[202](45/88): Loss_D: 0.2322 Loss_G: 1.4329\n===> Epoch[202](46/88): Loss_D: 0.2447 Loss_G: 1.3227\n===> Epoch[202](47/88): Loss_D: 0.2666 Loss_G: 1.0715\n===> Epoch[202](48/88): Loss_D: 0.2320 Loss_G: 1.3516\n===> Epoch[202](49/88): Loss_D: 0.2326 Loss_G: 1.2878\n===> Epoch[202](50/88): Loss_D: 0.2571 Loss_G: 1.1487\n===> Epoch[202](51/88): Loss_D: 0.2421 Loss_G: 1.3400\n===> Epoch[202](52/88): Loss_D: 0.2551 Loss_G: 1.2477\n===> Epoch[202](53/88): Loss_D: 0.2411 Loss_G: 1.2150\n===> Epoch[202](54/88): Loss_D: 0.2395 Loss_G: 1.3216\n===> Epoch[202](55/88): Loss_D: 0.2594 Loss_G: 1.2329\n===> Epoch[202](56/88): Loss_D: 0.2430 Loss_G: 1.1295\n===> Epoch[202](57/88): Loss_D: 0.2375 Loss_G: 1.3316\n===> Epoch[202](58/88): Loss_D: 0.2458 Loss_G: 1.2055\n===> Epoch[202](59/88): Loss_D: 0.2249 Loss_G: 1.3703\n===> Epoch[202](60/88): Loss_D: 0.2493 Loss_G: 1.1944\n===> Epoch[202](61/88): Loss_D: 0.2506 Loss_G: 1.0836\n===> Epoch[202](62/88): Loss_D: 0.2346 Loss_G: 1.1549\n===> Epoch[202](63/88): Loss_D: 0.1850 Loss_G: 1.9464\n===> Epoch[202](64/88): Loss_D: 0.2662 Loss_G: 1.3405\n===> Epoch[202](65/88): Loss_D: 0.2015 Loss_G: 1.5908\n===> Epoch[202](66/88): Loss_D: 0.2369 Loss_G: 1.4850\n===> Epoch[202](67/88): Loss_D: 0.2344 Loss_G: 1.4006\n===> Epoch[202](68/88): Loss_D: 0.2350 Loss_G: 1.2579\n===> Epoch[202](69/88): Loss_D: 0.2501 Loss_G: 1.3325\n===> Epoch[202](70/88): Loss_D: 0.2564 Loss_G: 1.2019\n===> Epoch[202](71/88): Loss_D: 0.2465 Loss_G: 1.2591\n===> Epoch[202](72/88): Loss_D: 0.2469 Loss_G: 1.1085\n===> Epoch[202](73/88): Loss_D: 0.2170 Loss_G: 1.4666\n===> Epoch[202](74/88): Loss_D: 0.2421 Loss_G: 1.5401\n===> Epoch[202](75/88): Loss_D: 0.2431 Loss_G: 1.1447\n===> Epoch[202](76/88): Loss_D: 0.2320 Loss_G: 1.2371\n===> Epoch[202](77/88): Loss_D: 0.2435 Loss_G: 1.2138\n===> Epoch[202](78/88): Loss_D: 0.2539 Loss_G: 1.3437\n===> Epoch[202](79/88): Loss_D: 0.2306 Loss_G: 1.4349\n===> Epoch[202](80/88): Loss_D: 0.2446 Loss_G: 1.2772\n===> Epoch[202](81/88): Loss_D: 0.2043 Loss_G: 1.4574\n===> Epoch[202](82/88): Loss_D: 0.2383 Loss_G: 1.1379\n===> Epoch[202](83/88): Loss_D: 0.2517 Loss_G: 1.1785\n===> Epoch[202](84/88): Loss_D: 0.2381 Loss_G: 1.3596\n===> Epoch[202](85/88): Loss_D: 0.2500 Loss_G: 1.2037\n===> Epoch[202](86/88): Loss_D: 0.2415 Loss_G: 1.1086\n===> Epoch[202](87/88): Loss_D: 0.1609 Loss_G: 1.5976\n===> Epoch[202](88/88): Loss_D: 0.2035 Loss_G: 1.4288\nlearning rate = 0.0000496\nlearning rate = 0.0000496\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0030} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[203](1/88): Loss_D: 0.2442 Loss_G: 1.1019\n===> Epoch[203](2/88): Loss_D: 0.2695 Loss_G: 1.2486\n===> Epoch[203](3/88): Loss_D: 0.2411 Loss_G: 1.4677\n===> Epoch[203](4/88): Loss_D: 0.2337 Loss_G: 1.3126\n===> Epoch[203](5/88): Loss_D: 0.2455 Loss_G: 1.4883\n===> Epoch[203](6/88): Loss_D: 0.2411 Loss_G: 1.2677\n===> Epoch[203](7/88): Loss_D: 0.2397 Loss_G: 1.0222\n===> Epoch[203](8/88): Loss_D: 0.2610 Loss_G: 0.9726\n===> Epoch[203](9/88): Loss_D: 0.2512 Loss_G: 1.1738\n===> Epoch[203](10/88): Loss_D: 0.2470 Loss_G: 1.1564\n===> Epoch[203](11/88): Loss_D: 0.2259 Loss_G: 1.3418\n===> Epoch[203](12/88): Loss_D: 0.2497 Loss_G: 1.2158\n===> Epoch[203](13/88): Loss_D: 0.2500 Loss_G: 1.1457\n===> Epoch[203](14/88): Loss_D: 0.2525 Loss_G: 1.2989\n===> Epoch[203](15/88): Loss_D: 0.2463 Loss_G: 1.1860\n===> Epoch[203](16/88): Loss_D: 0.2480 Loss_G: 1.0820\n===> Epoch[203](17/88): Loss_D: 0.2321 Loss_G: 1.1746\n===> Epoch[203](18/88): Loss_D: 0.2325 Loss_G: 1.4465\n===> Epoch[203](19/88): Loss_D: 0.2523 Loss_G: 1.1474\n===> Epoch[203](20/88): Loss_D: 0.2484 Loss_G: 1.4761\n===> Epoch[203](21/88): Loss_D: 0.2298 Loss_G: 1.3498\n===> Epoch[203](22/88): Loss_D: 0.2254 Loss_G: 1.2104\n===> Epoch[203](23/88): Loss_D: 0.2458 Loss_G: 1.1807\n===> Epoch[203](24/88): Loss_D: 0.2423 Loss_G: 1.3176\n===> Epoch[203](25/88): Loss_D: 0.2607 Loss_G: 1.2379\n===> Epoch[203](26/88): Loss_D: 0.2497 Loss_G: 1.2200\n===> Epoch[203](27/88): Loss_D: 0.2421 Loss_G: 1.1713\n===> Epoch[203](28/88): Loss_D: 0.2567 Loss_G: 1.2861\n===> Epoch[203](29/88): Loss_D: 0.2518 Loss_G: 1.1191\n===> Epoch[203](30/88): Loss_D: 0.2460 Loss_G: 1.3129\n===> Epoch[203](31/88): Loss_D: 0.2532 Loss_G: 1.1598\n===> Epoch[203](32/88): Loss_D: 0.2346 Loss_G: 1.3867\n===> Epoch[203](33/88): Loss_D: 0.2477 Loss_G: 1.0947\n===> Epoch[203](34/88): Loss_D: 0.2463 Loss_G: 1.2619\n===> Epoch[203](35/88): Loss_D: 0.2580 Loss_G: 1.2041\n===> Epoch[203](36/88): Loss_D: 0.2442 Loss_G: 1.3767\n===> Epoch[203](37/88): Loss_D: 0.2543 Loss_G: 1.1484\n===> Epoch[203](38/88): Loss_D: 0.2424 Loss_G: 1.1148\n===> Epoch[203](39/88): Loss_D: 0.2574 Loss_G: 1.0803\n===> Epoch[203](40/88): Loss_D: 0.2312 Loss_G: 1.4017\n===> Epoch[203](41/88): Loss_D: 0.2622 Loss_G: 1.1666\n===> Epoch[203](42/88): Loss_D: 0.2371 Loss_G: 1.1726\n===> Epoch[203](43/88): Loss_D: 0.2109 Loss_G: 1.5378\n===> Epoch[203](44/88): Loss_D: 0.2438 Loss_G: 1.2808\n===> Epoch[203](45/88): Loss_D: 0.1814 Loss_G: 2.0848\n===> Epoch[203](46/88): Loss_D: 0.2334 Loss_G: 1.2434\n===> Epoch[203](47/88): Loss_D: 0.2641 Loss_G: 1.3850\n===> Epoch[203](48/88): Loss_D: 0.2327 Loss_G: 1.3981\n===> Epoch[203](49/88): Loss_D: 0.2198 Loss_G: 1.5203\n===> Epoch[203](50/88): Loss_D: 0.1750 Loss_G: 1.6576\n===> Epoch[203](51/88): Loss_D: 0.2777 Loss_G: 1.1100\n===> Epoch[203](52/88): Loss_D: 0.2370 Loss_G: 1.4650\n===> Epoch[203](53/88): Loss_D: 0.2262 Loss_G: 1.3845\n===> Epoch[203](54/88): Loss_D: 0.2140 Loss_G: 2.3725\n===> Epoch[203](55/88): Loss_D: 0.2552 Loss_G: 1.3382\n===> Epoch[203](56/88): Loss_D: 0.2632 Loss_G: 1.4216\n===> Epoch[203](57/88): Loss_D: 0.2246 Loss_G: 1.4670\n===> Epoch[203](58/88): Loss_D: 0.2526 Loss_G: 1.3315\n===> Epoch[203](59/88): Loss_D: 0.2377 Loss_G: 1.1989\n===> Epoch[203](60/88): Loss_D: 0.2710 Loss_G: 1.1677\n===> Epoch[203](61/88): Loss_D: 0.2344 Loss_G: 1.2978\n===> Epoch[203](62/88): Loss_D: 0.2383 Loss_G: 1.4235\n===> Epoch[203](63/88): Loss_D: 0.2565 Loss_G: 1.2540\n===> Epoch[203](64/88): Loss_D: 0.2021 Loss_G: 1.4502\n===> Epoch[203](65/88): Loss_D: 0.2703 Loss_G: 1.2509\n===> Epoch[203](66/88): Loss_D: 0.2576 Loss_G: 1.2417\n===> Epoch[203](67/88): Loss_D: 0.2429 Loss_G: 1.2884\n===> Epoch[203](68/88): Loss_D: 0.2641 Loss_G: 1.2465\n===> Epoch[203](69/88): Loss_D: 0.1822 Loss_G: 1.7024\n===> Epoch[203](70/88): Loss_D: 0.2423 Loss_G: 1.3689\n===> Epoch[203](71/88): Loss_D: 0.2556 Loss_G: 1.3573\n===> Epoch[203](72/88): Loss_D: 0.2463 Loss_G: 1.3424\n===> Epoch[203](73/88): Loss_D: 0.2614 Loss_G: 1.0712\n===> Epoch[203](74/88): Loss_D: 0.2426 Loss_G: 1.3782\n===> Epoch[203](75/88): Loss_D: 0.2264 Loss_G: 1.5457\n===> Epoch[203](76/88): Loss_D: 0.2489 Loss_G: 1.2738\n===> Epoch[203](77/88): Loss_D: 0.2505 Loss_G: 1.2369\n===> Epoch[203](78/88): Loss_D: 0.2518 Loss_G: 1.0555\n===> Epoch[203](79/88): Loss_D: 0.2593 Loss_G: 1.2811\n===> Epoch[203](80/88): Loss_D: 0.2158 Loss_G: 1.5839\n===> Epoch[203](81/88): Loss_D: 0.2314 Loss_G: 1.5341\n===> Epoch[203](82/88): Loss_D: 0.2474 Loss_G: 1.2035\n===> Epoch[203](83/88): Loss_D: 0.2394 Loss_G: 1.3492\n===> Epoch[203](84/88): Loss_D: 0.2481 Loss_G: 1.2288\n===> Epoch[203](85/88): Loss_D: 0.2368 Loss_G: 1.4149\n===> Epoch[203](86/88): Loss_D: 0.2387 Loss_G: 1.4615\n===> Epoch[203](87/88): Loss_D: 0.2619 Loss_G: 1.2846\n===> Epoch[203](88/88): Loss_D: 0.1862 Loss_G: 1.6045\nlearning rate = 0.0000494\nlearning rate = 0.0000494\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0012} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[204](1/88): Loss_D: 0.1998 Loss_G: 1.9257\n===> Epoch[204](2/88): Loss_D: 0.2156 Loss_G: 1.4684\n===> Epoch[204](3/88): Loss_D: 0.2471 Loss_G: 1.2059\n===> Epoch[204](4/88): Loss_D: 0.2598 Loss_G: 1.2566\n===> Epoch[204](5/88): Loss_D: 0.2468 Loss_G: 1.3191\n===> Epoch[204](6/88): Loss_D: 0.2627 Loss_G: 1.2882\n===> Epoch[204](7/88): Loss_D: 0.2406 Loss_G: 1.3806\n===> Epoch[204](8/88): Loss_D: 0.2412 Loss_G: 1.5035\n===> Epoch[204](9/88): Loss_D: 0.2514 Loss_G: 1.1430\n===> Epoch[204](10/88): Loss_D: 0.2332 Loss_G: 1.4525\n===> Epoch[204](11/88): Loss_D: 0.2293 Loss_G: 1.3148\n===> Epoch[204](12/88): Loss_D: 0.2697 Loss_G: 1.1582\n===> Epoch[204](13/88): Loss_D: 0.2431 Loss_G: 1.3555\n===> Epoch[204](14/88): Loss_D: 0.2517 Loss_G: 1.0597\n===> Epoch[204](15/88): Loss_D: 0.2401 Loss_G: 1.2038\n===> Epoch[204](16/88): Loss_D: 0.2593 Loss_G: 1.1124\n===> Epoch[204](17/88): Loss_D: 0.2471 Loss_G: 1.0109\n===> Epoch[204](18/88): Loss_D: 0.2367 Loss_G: 1.2158\n===> Epoch[204](19/88): Loss_D: 0.1964 Loss_G: 1.4059\n===> Epoch[204](20/88): Loss_D: 0.2371 Loss_G: 1.4543\n===> Epoch[204](21/88): Loss_D: 0.1855 Loss_G: 1.6132\n===> Epoch[204](22/88): Loss_D: 0.2567 Loss_G: 1.2574\n===> Epoch[204](23/88): Loss_D: 0.2466 Loss_G: 1.2680\n===> Epoch[204](24/88): Loss_D: 0.2459 Loss_G: 1.3383\n===> Epoch[204](25/88): Loss_D: 0.2418 Loss_G: 1.2509\n===> Epoch[204](26/88): Loss_D: 0.2651 Loss_G: 1.1645\n===> Epoch[204](27/88): Loss_D: 0.2603 Loss_G: 1.0458\n===> Epoch[204](28/88): Loss_D: 0.2296 Loss_G: 1.5218\n===> Epoch[204](29/88): Loss_D: 0.2335 Loss_G: 1.2937\n===> Epoch[204](30/88): Loss_D: 0.2495 Loss_G: 1.2373\n===> Epoch[204](31/88): Loss_D: 0.2502 Loss_G: 1.0078\n===> Epoch[204](32/88): Loss_D: 0.2374 Loss_G: 1.2755\n===> Epoch[204](33/88): Loss_D: 0.2475 Loss_G: 1.3446\n===> Epoch[204](34/88): Loss_D: 0.2557 Loss_G: 1.1940\n===> Epoch[204](35/88): Loss_D: 0.2499 Loss_G: 1.3233\n===> Epoch[204](36/88): Loss_D: 0.2396 Loss_G: 1.3823\n===> Epoch[204](37/88): Loss_D: 0.2568 Loss_G: 1.0140\n===> Epoch[204](38/88): Loss_D: 0.2593 Loss_G: 1.2664\n===> Epoch[204](39/88): Loss_D: 0.2357 Loss_G: 1.2157\n===> Epoch[204](40/88): Loss_D: 0.2241 Loss_G: 1.4868\n===> Epoch[204](41/88): Loss_D: 0.2219 Loss_G: 1.5501\n===> Epoch[204](42/88): Loss_D: 0.2484 Loss_G: 1.1174\n===> Epoch[204](43/88): Loss_D: 0.2533 Loss_G: 1.2630\n===> Epoch[204](44/88): Loss_D: 0.2537 Loss_G: 1.3073\n===> Epoch[204](45/88): Loss_D: 0.2374 Loss_G: 1.4359\n===> Epoch[204](46/88): Loss_D: 0.2533 Loss_G: 1.1136\n===> Epoch[204](47/88): Loss_D: 0.1829 Loss_G: 1.7044\n===> Epoch[204](48/88): Loss_D: 0.2341 Loss_G: 1.2589\n===> Epoch[204](49/88): Loss_D: 0.2407 Loss_G: 1.1953\n===> Epoch[204](50/88): Loss_D: 0.2309 Loss_G: 2.2521\n===> Epoch[204](51/88): Loss_D: 0.2452 Loss_G: 1.2759\n===> Epoch[204](52/88): Loss_D: 0.2469 Loss_G: 1.2932\n===> Epoch[204](53/88): Loss_D: 0.2623 Loss_G: 1.2655\n===> Epoch[204](54/88): Loss_D: 0.2590 Loss_G: 1.2223\n===> Epoch[204](55/88): Loss_D: 0.2444 Loss_G: 1.1281\n===> Epoch[204](56/88): Loss_D: 0.2423 Loss_G: 1.1115\n===> Epoch[204](57/88): Loss_D: 0.2385 Loss_G: 1.3297\n===> Epoch[204](58/88): Loss_D: 0.2445 Loss_G: 1.2125\n===> Epoch[204](59/88): Loss_D: 0.2440 Loss_G: 1.3399\n===> Epoch[204](60/88): Loss_D: 0.2345 Loss_G: 1.1582\n===> Epoch[204](61/88): Loss_D: 0.1660 Loss_G: 1.5997\n===> Epoch[204](62/88): Loss_D: 0.2463 Loss_G: 1.1637\n===> Epoch[204](63/88): Loss_D: 0.2448 Loss_G: 1.3001\n===> Epoch[204](64/88): Loss_D: 0.2281 Loss_G: 1.3160\n===> Epoch[204](65/88): Loss_D: 0.2139 Loss_G: 1.4770\n===> Epoch[204](66/88): Loss_D: 0.2546 Loss_G: 1.3132\n===> Epoch[204](67/88): Loss_D: 0.2534 Loss_G: 1.1401\n===> Epoch[204](68/88): Loss_D: 0.2913 Loss_G: 1.2427\n===> Epoch[204](69/88): Loss_D: 0.2014 Loss_G: 1.5356\n===> Epoch[204](70/88): Loss_D: 0.2479 Loss_G: 1.3640\n===> Epoch[204](71/88): Loss_D: 0.2373 Loss_G: 1.1146\n===> Epoch[204](72/88): Loss_D: 0.2752 Loss_G: 1.1340\n===> Epoch[204](73/88): Loss_D: 0.1900 Loss_G: 1.6438\n===> Epoch[204](74/88): Loss_D: 0.2526 Loss_G: 1.1521\n===> Epoch[204](75/88): Loss_D: 0.2571 Loss_G: 1.3897\n===> Epoch[204](76/88): Loss_D: 0.2401 Loss_G: 1.2058\n===> Epoch[204](77/88): Loss_D: 0.2591 Loss_G: 1.3153\n===> Epoch[204](78/88): Loss_D: 0.2457 Loss_G: 1.2921\n===> Epoch[204](79/88): Loss_D: 0.2491 Loss_G: 1.3075\n===> Epoch[204](80/88): Loss_D: 0.2321 Loss_G: 1.2385\n===> Epoch[204](81/88): Loss_D: 0.2619 Loss_G: 1.5100\n===> Epoch[204](82/88): Loss_D: 0.2586 Loss_G: 1.2097\n===> Epoch[204](83/88): Loss_D: 0.2642 Loss_G: 1.1027\n===> Epoch[204](84/88): Loss_D: 0.2307 Loss_G: 1.5206\n===> Epoch[204](85/88): Loss_D: 0.2516 Loss_G: 1.2632\n===> Epoch[204](86/88): Loss_D: 0.2441 Loss_G: 1.3105\n===> Epoch[204](87/88): Loss_D: 0.2408 Loss_G: 1.3515\n===> Epoch[204](88/88): Loss_D: 0.2519 Loss_G: 1.2457\nlearning rate = 0.0000491\nlearning rate = 0.0000491\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0107} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[205](1/88): Loss_D: 0.2144 Loss_G: 1.4489\n===> Epoch[205](2/88): Loss_D: 0.2434 Loss_G: 1.1368\n===> Epoch[205](3/88): Loss_D: 0.2508 Loss_G: 1.2419\n===> Epoch[205](4/88): Loss_D: 0.2527 Loss_G: 1.2053\n===> Epoch[205](5/88): Loss_D: 0.2556 Loss_G: 1.0908\n===> Epoch[205](6/88): Loss_D: 0.2420 Loss_G: 1.2003\n===> Epoch[205](7/88): Loss_D: 0.2367 Loss_G: 1.4344\n===> Epoch[205](8/88): Loss_D: 0.2398 Loss_G: 1.4547\n===> Epoch[205](9/88): Loss_D: 0.2381 Loss_G: 1.2450\n===> Epoch[205](10/88): Loss_D: 0.2421 Loss_G: 1.2086\n===> Epoch[205](11/88): Loss_D: 0.2472 Loss_G: 1.2224\n===> Epoch[205](12/88): Loss_D: 0.2500 Loss_G: 1.1298\n===> Epoch[205](13/88): Loss_D: 0.2485 Loss_G: 1.2355\n===> Epoch[205](14/88): Loss_D: 0.2338 Loss_G: 1.2126\n===> Epoch[205](15/88): Loss_D: 0.2205 Loss_G: 1.4764\n===> Epoch[205](16/88): Loss_D: 0.2357 Loss_G: 1.2356\n===> Epoch[205](17/88): Loss_D: 0.2435 Loss_G: 1.2627\n===> Epoch[205](18/88): Loss_D: 0.2580 Loss_G: 1.0690\n===> Epoch[205](19/88): Loss_D: 0.2427 Loss_G: 1.2516\n===> Epoch[205](20/88): Loss_D: 0.2471 Loss_G: 1.1795\n===> Epoch[205](21/88): Loss_D: 0.2554 Loss_G: 1.1275\n===> Epoch[205](22/88): Loss_D: 0.2450 Loss_G: 1.2731\n===> Epoch[205](23/88): Loss_D: 0.2350 Loss_G: 1.2448\n===> Epoch[205](24/88): Loss_D: 0.2551 Loss_G: 1.0271\n===> Epoch[205](25/88): Loss_D: 0.2488 Loss_G: 1.0815\n===> Epoch[205](26/88): Loss_D: 0.2506 Loss_G: 0.9630\n===> Epoch[205](27/88): Loss_D: 0.2295 Loss_G: 1.1462\n===> Epoch[205](28/88): Loss_D: 0.2262 Loss_G: 1.4570\n===> Epoch[205](29/88): Loss_D: 0.2012 Loss_G: 1.4241\n===> Epoch[205](30/88): Loss_D: 0.2325 Loss_G: 1.3003\n===> Epoch[205](31/88): Loss_D: 0.2434 Loss_G: 1.3382\n===> Epoch[205](32/88): Loss_D: 0.2302 Loss_G: 1.3248\n===> Epoch[205](33/88): Loss_D: 0.2578 Loss_G: 0.9951\n===> Epoch[205](34/88): Loss_D: 0.2388 Loss_G: 1.2514\n===> Epoch[205](35/88): Loss_D: 0.2373 Loss_G: 1.1672\n===> Epoch[205](36/88): Loss_D: 0.2396 Loss_G: 1.3554\n===> Epoch[205](37/88): Loss_D: 0.2610 Loss_G: 1.1811\n===> Epoch[205](38/88): Loss_D: 0.2514 Loss_G: 1.0433\n===> Epoch[205](39/88): Loss_D: 0.2359 Loss_G: 1.3203\n===> Epoch[205](40/88): Loss_D: 0.2402 Loss_G: 1.3070\n===> Epoch[205](41/88): Loss_D: 0.2600 Loss_G: 1.0587\n===> Epoch[205](42/88): Loss_D: 0.2314 Loss_G: 1.2648\n===> Epoch[205](43/88): Loss_D: 0.2360 Loss_G: 1.1487\n===> Epoch[205](44/88): Loss_D: 0.2437 Loss_G: 1.3101\n===> Epoch[205](45/88): Loss_D: 0.2538 Loss_G: 1.2418\n===> Epoch[205](46/88): Loss_D: 0.2302 Loss_G: 1.3003\n===> Epoch[205](47/88): Loss_D: 0.2502 Loss_G: 1.1365\n===> Epoch[205](48/88): Loss_D: 0.2380 Loss_G: 1.4504\n===> Epoch[205](49/88): Loss_D: 0.2407 Loss_G: 1.1746\n===> Epoch[205](50/88): Loss_D: 0.2196 Loss_G: 1.1474\n===> Epoch[205](51/88): Loss_D: 0.2197 Loss_G: 2.3600\n===> Epoch[205](52/88): Loss_D: 0.2492 Loss_G: 1.3050\n===> Epoch[205](53/88): Loss_D: 0.2132 Loss_G: 1.4373\n===> Epoch[205](54/88): Loss_D: 0.2748 Loss_G: 1.3251\n===> Epoch[205](55/88): Loss_D: 0.2573 Loss_G: 1.1473\n===> Epoch[205](56/88): Loss_D: 0.2596 Loss_G: 1.2462\n===> Epoch[205](57/88): Loss_D: 0.2319 Loss_G: 1.3911\n===> Epoch[205](58/88): Loss_D: 0.2524 Loss_G: 1.1578\n===> Epoch[205](59/88): Loss_D: 0.2459 Loss_G: 1.2725\n===> Epoch[205](60/88): Loss_D: 0.2645 Loss_G: 1.0946\n===> Epoch[205](61/88): Loss_D: 0.2398 Loss_G: 1.2740\n===> Epoch[205](62/88): Loss_D: 0.2267 Loss_G: 1.2714\n===> Epoch[205](63/88): Loss_D: 0.2532 Loss_G: 1.1452\n===> Epoch[205](64/88): Loss_D: 0.2288 Loss_G: 1.3919\n===> Epoch[205](65/88): Loss_D: 0.2585 Loss_G: 1.2640\n===> Epoch[205](66/88): Loss_D: 0.1869 Loss_G: 1.6608\n===> Epoch[205](67/88): Loss_D: 0.2427 Loss_G: 1.2402\n===> Epoch[205](68/88): Loss_D: 0.2271 Loss_G: 1.2482\n===> Epoch[205](69/88): Loss_D: 0.1845 Loss_G: 1.5250\n===> Epoch[205](70/88): Loss_D: 0.2318 Loss_G: 1.4180\n===> Epoch[205](71/88): Loss_D: 0.2560 Loss_G: 1.1540\n===> Epoch[205](72/88): Loss_D: 0.1753 Loss_G: 1.4911\n===> Epoch[205](73/88): Loss_D: 0.2092 Loss_G: 1.6032\n===> Epoch[205](74/88): Loss_D: 0.2446 Loss_G: 1.2069\n===> Epoch[205](75/88): Loss_D: 0.2413 Loss_G: 1.4001\n===> Epoch[205](76/88): Loss_D: 0.2571 Loss_G: 1.1125\n===> Epoch[205](77/88): Loss_D: 0.2593 Loss_G: 1.2864\n===> Epoch[205](78/88): Loss_D: 0.2562 Loss_G: 1.2780\n===> Epoch[205](79/88): Loss_D: 0.2536 Loss_G: 1.2394\n===> Epoch[205](80/88): Loss_D: 0.2345 Loss_G: 1.4507\n===> Epoch[205](81/88): Loss_D: 0.2237 Loss_G: 1.4905\n===> Epoch[205](82/88): Loss_D: 0.2495 Loss_G: 1.3681\n===> Epoch[205](83/88): Loss_D: 0.2537 Loss_G: 1.2529\n===> Epoch[205](84/88): Loss_D: 0.1834 Loss_G: 2.0342\n===> Epoch[205](85/88): Loss_D: 0.2512 Loss_G: 1.3018\n===> Epoch[205](86/88): Loss_D: 0.2634 Loss_G: 1.2519\n===> Epoch[205](87/88): Loss_D: 0.2656 Loss_G: 1.2108\n===> Epoch[205](88/88): Loss_D: 0.2534 Loss_G: 1.3615\nlearning rate = 0.0000489\nlearning rate = 0.0000489\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0061} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[206](1/88): Loss_D: 0.2552 Loss_G: 1.2895\n===> Epoch[206](2/88): Loss_D: 0.2394 Loss_G: 1.3865\n===> Epoch[206](3/88): Loss_D: 0.2426 Loss_G: 1.3500\n===> Epoch[206](4/88): Loss_D: 0.2581 Loss_G: 1.1896\n===> Epoch[206](5/88): Loss_D: 0.2473 Loss_G: 1.1950\n===> Epoch[206](6/88): Loss_D: 0.2449 Loss_G: 1.1877\n===> Epoch[206](7/88): Loss_D: 0.2361 Loss_G: 1.6314\n===> Epoch[206](8/88): Loss_D: 0.2501 Loss_G: 1.2746\n===> Epoch[206](9/88): Loss_D: 0.2226 Loss_G: 1.4219\n===> Epoch[206](10/88): Loss_D: 0.2460 Loss_G: 1.2042\n===> Epoch[206](11/88): Loss_D: 0.2348 Loss_G: 1.3331\n===> Epoch[206](12/88): Loss_D: 0.2454 Loss_G: 1.1265\n===> Epoch[206](13/88): Loss_D: 0.2458 Loss_G: 1.1395\n===> Epoch[206](14/88): Loss_D: 0.2350 Loss_G: 1.3469\n===> Epoch[206](15/88): Loss_D: 0.2130 Loss_G: 1.4286\n===> Epoch[206](16/88): Loss_D: 0.2277 Loss_G: 1.2817\n===> Epoch[206](17/88): Loss_D: 0.2623 Loss_G: 0.9896\n===> Epoch[206](18/88): Loss_D: 0.2674 Loss_G: 1.0114\n===> Epoch[206](19/88): Loss_D: 0.2569 Loss_G: 1.2728\n===> Epoch[206](20/88): Loss_D: 0.2560 Loss_G: 1.2572\n===> Epoch[206](21/88): Loss_D: 0.2406 Loss_G: 1.1625\n===> Epoch[206](22/88): Loss_D: 0.2351 Loss_G: 1.3717\n===> Epoch[206](23/88): Loss_D: 0.1778 Loss_G: 1.7130\n===> Epoch[206](24/88): Loss_D: 0.2494 Loss_G: 1.2539\n===> Epoch[206](25/88): Loss_D: 0.2526 Loss_G: 1.2250\n===> Epoch[206](26/88): Loss_D: 0.2547 Loss_G: 1.1196\n===> Epoch[206](27/88): Loss_D: 0.2334 Loss_G: 1.3883\n===> Epoch[206](28/88): Loss_D: 0.2362 Loss_G: 1.3509\n===> Epoch[206](29/88): Loss_D: 0.2646 Loss_G: 1.1264\n===> Epoch[206](30/88): Loss_D: 0.2262 Loss_G: 1.2717\n===> Epoch[206](31/88): Loss_D: 0.2122 Loss_G: 1.3981\n===> Epoch[206](32/88): Loss_D: 0.2385 Loss_G: 1.1700\n===> Epoch[206](33/88): Loss_D: 0.2054 Loss_G: 1.4846\n===> Epoch[206](34/88): Loss_D: 0.2476 Loss_G: 1.4928\n===> Epoch[206](35/88): Loss_D: 0.2630 Loss_G: 1.1795\n===> Epoch[206](36/88): Loss_D: 0.2532 Loss_G: 0.9877\n===> Epoch[206](37/88): Loss_D: 0.2330 Loss_G: 1.4445\n===> Epoch[206](38/88): Loss_D: 0.2549 Loss_G: 1.1348\n===> Epoch[206](39/88): Loss_D: 0.2565 Loss_G: 1.2787\n===> Epoch[206](40/88): Loss_D: 0.2525 Loss_G: 1.1772\n===> Epoch[206](41/88): Loss_D: 0.2595 Loss_G: 1.1099\n===> Epoch[206](42/88): Loss_D: 0.2485 Loss_G: 1.2955\n===> Epoch[206](43/88): Loss_D: 0.2469 Loss_G: 1.1425\n===> Epoch[206](44/88): Loss_D: 0.2588 Loss_G: 1.2962\n===> Epoch[206](45/88): Loss_D: 0.2567 Loss_G: 1.1784\n===> Epoch[206](46/88): Loss_D: 0.2449 Loss_G: 1.0586\n===> Epoch[206](47/88): Loss_D: 0.2132 Loss_G: 1.6187\n===> Epoch[206](48/88): Loss_D: 0.2441 Loss_G: 1.2780\n===> Epoch[206](49/88): Loss_D: 0.2621 Loss_G: 1.1481\n===> Epoch[206](50/88): Loss_D: 0.2622 Loss_G: 1.3426\n===> Epoch[206](51/88): Loss_D: 0.2518 Loss_G: 1.1236\n===> Epoch[206](52/88): Loss_D: 0.2393 Loss_G: 1.2318\n===> Epoch[206](53/88): Loss_D: 0.2155 Loss_G: 1.4144\n===> Epoch[206](54/88): Loss_D: 0.2179 Loss_G: 2.3052\n===> Epoch[206](55/88): Loss_D: 0.1760 Loss_G: 1.6894\n===> Epoch[206](56/88): Loss_D: 0.2541 Loss_G: 1.3465\n===> Epoch[206](57/88): Loss_D: 0.2401 Loss_G: 1.4959\n===> Epoch[206](58/88): Loss_D: 0.2374 Loss_G: 1.3716\n===> Epoch[206](59/88): Loss_D: 0.2500 Loss_G: 1.5559\n===> Epoch[206](60/88): Loss_D: 0.2272 Loss_G: 1.4927\n===> Epoch[206](61/88): Loss_D: 0.2494 Loss_G: 1.2629\n===> Epoch[206](62/88): Loss_D: 0.2479 Loss_G: 1.3453\n===> Epoch[206](63/88): Loss_D: 0.2296 Loss_G: 1.4793\n===> Epoch[206](64/88): Loss_D: 0.2464 Loss_G: 1.1740\n===> Epoch[206](65/88): Loss_D: 0.2318 Loss_G: 1.3633\n===> Epoch[206](66/88): Loss_D: 0.2382 Loss_G: 1.3014\n===> Epoch[206](67/88): Loss_D: 0.2302 Loss_G: 1.3356\n===> Epoch[206](68/88): Loss_D: 0.2676 Loss_G: 1.1363\n===> Epoch[206](69/88): Loss_D: 0.2497 Loss_G: 1.2252\n===> Epoch[206](70/88): Loss_D: 0.2407 Loss_G: 1.2309\n===> Epoch[206](71/88): Loss_D: 0.1899 Loss_G: 1.9858\n===> Epoch[206](72/88): Loss_D: 0.2373 Loss_G: 1.6285\n===> Epoch[206](73/88): Loss_D: 0.2413 Loss_G: 1.4679\n===> Epoch[206](74/88): Loss_D: 0.2537 Loss_G: 1.2062\n===> Epoch[206](75/88): Loss_D: 0.2603 Loss_G: 1.2850\n===> Epoch[206](76/88): Loss_D: 0.2682 Loss_G: 1.1136\n===> Epoch[206](77/88): Loss_D: 0.2571 Loss_G: 1.2176\n===> Epoch[206](78/88): Loss_D: 0.2108 Loss_G: 1.3188\n===> Epoch[206](79/88): Loss_D: 0.2291 Loss_G: 1.2332\n===> Epoch[206](80/88): Loss_D: 0.2439 Loss_G: 1.2244\n===> Epoch[206](81/88): Loss_D: 0.2590 Loss_G: 1.0780\n===> Epoch[206](82/88): Loss_D: 0.1914 Loss_G: 1.6660\n===> Epoch[206](83/88): Loss_D: 0.2507 Loss_G: 1.2315\n===> Epoch[206](84/88): Loss_D: 0.2478 Loss_G: 1.2406\n===> Epoch[206](85/88): Loss_D: 0.2625 Loss_G: 1.2504\n===> Epoch[206](86/88): Loss_D: 0.2403 Loss_G: 1.6711\n===> Epoch[206](87/88): Loss_D: 0.2450 Loss_G: 1.1927\n===> Epoch[206](88/88): Loss_D: 0.2410 Loss_G: 1.1046\nlearning rate = 0.0000486\nlearning rate = 0.0000486\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0004} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[207](1/88): Loss_D: 0.2314 Loss_G: 1.4065\n===> Epoch[207](2/88): Loss_D: 0.1810 Loss_G: 1.9830\n===> Epoch[207](3/88): Loss_D: 0.2467 Loss_G: 1.2355\n===> Epoch[207](4/88): Loss_D: 0.2569 Loss_G: 1.1230\n===> Epoch[207](5/88): Loss_D: 0.2649 Loss_G: 1.0776\n===> Epoch[207](6/88): Loss_D: 0.2079 Loss_G: 2.3450\n===> Epoch[207](7/88): Loss_D: 0.2351 Loss_G: 1.1114\n===> Epoch[207](8/88): Loss_D: 0.2453 Loss_G: 1.1395\n===> Epoch[207](9/88): Loss_D: 0.2120 Loss_G: 1.4145\n===> Epoch[207](10/88): Loss_D: 0.2580 Loss_G: 1.0506\n===> Epoch[207](11/88): Loss_D: 0.2484 Loss_G: 1.2716\n===> Epoch[207](12/88): Loss_D: 0.2494 Loss_G: 1.1606\n===> Epoch[207](13/88): Loss_D: 0.2554 Loss_G: 1.1326\n===> Epoch[207](14/88): Loss_D: 0.2545 Loss_G: 1.1986\n===> Epoch[207](15/88): Loss_D: 0.1892 Loss_G: 1.6389\n===> Epoch[207](16/88): Loss_D: 0.2739 Loss_G: 1.1624\n===> Epoch[207](17/88): Loss_D: 0.2672 Loss_G: 1.2442\n===> Epoch[207](18/88): Loss_D: 0.2408 Loss_G: 1.2016\n===> Epoch[207](19/88): Loss_D: 0.2202 Loss_G: 1.5619\n===> Epoch[207](20/88): Loss_D: 0.2415 Loss_G: 1.4339\n===> Epoch[207](21/88): Loss_D: 0.2577 Loss_G: 1.1672\n===> Epoch[207](22/88): Loss_D: 0.2718 Loss_G: 1.2149\n===> Epoch[207](23/88): Loss_D: 0.2578 Loss_G: 1.3757\n===> Epoch[207](24/88): Loss_D: 0.2473 Loss_G: 1.2853\n===> Epoch[207](25/88): Loss_D: 0.2308 Loss_G: 1.3728\n===> Epoch[207](26/88): Loss_D: 0.1747 Loss_G: 1.6398\n===> Epoch[207](27/88): Loss_D: 0.2435 Loss_G: 1.1789\n===> Epoch[207](28/88): Loss_D: 0.2110 Loss_G: 1.7302\n===> Epoch[207](29/88): Loss_D: 0.2411 Loss_G: 1.2494\n===> Epoch[207](30/88): Loss_D: 0.2480 Loss_G: 1.1410\n===> Epoch[207](31/88): Loss_D: 0.2342 Loss_G: 1.3448\n===> Epoch[207](32/88): Loss_D: 0.2608 Loss_G: 1.2354\n===> Epoch[207](33/88): Loss_D: 0.2492 Loss_G: 1.1356\n===> Epoch[207](34/88): Loss_D: 0.2449 Loss_G: 1.2108\n===> Epoch[207](35/88): Loss_D: 0.2486 Loss_G: 1.1812\n===> Epoch[207](36/88): Loss_D: 0.2188 Loss_G: 1.4140\n===> Epoch[207](37/88): Loss_D: 0.2483 Loss_G: 1.3440\n===> Epoch[207](38/88): Loss_D: 0.2375 Loss_G: 1.3143\n===> Epoch[207](39/88): Loss_D: 0.2121 Loss_G: 1.4081\n===> Epoch[207](40/88): Loss_D: 0.2428 Loss_G: 1.3507\n===> Epoch[207](41/88): Loss_D: 0.2477 Loss_G: 1.3551\n===> Epoch[207](42/88): Loss_D: 0.2457 Loss_G: 1.5014\n===> Epoch[207](43/88): Loss_D: 0.2347 Loss_G: 1.3856\n===> Epoch[207](44/88): Loss_D: 0.2313 Loss_G: 1.4638\n===> Epoch[207](45/88): Loss_D: 0.2583 Loss_G: 1.2464\n===> Epoch[207](46/88): Loss_D: 0.2393 Loss_G: 1.2967\n===> Epoch[207](47/88): Loss_D: 0.2461 Loss_G: 1.2302\n===> Epoch[207](48/88): Loss_D: 0.2347 Loss_G: 1.1484\n===> Epoch[207](49/88): Loss_D: 0.2405 Loss_G: 1.2676\n===> Epoch[207](50/88): Loss_D: 0.2481 Loss_G: 1.1987\n===> Epoch[207](51/88): Loss_D: 0.2513 Loss_G: 1.2917\n===> Epoch[207](52/88): Loss_D: 0.2321 Loss_G: 1.3242\n===> Epoch[207](53/88): Loss_D: 0.2364 Loss_G: 1.2524\n===> Epoch[207](54/88): Loss_D: 0.2520 Loss_G: 1.1454\n===> Epoch[207](55/88): Loss_D: 0.2438 Loss_G: 1.2961\n===> Epoch[207](56/88): Loss_D: 0.2295 Loss_G: 1.1909\n===> Epoch[207](57/88): Loss_D: 0.2159 Loss_G: 1.4560\n===> Epoch[207](58/88): Loss_D: 0.2451 Loss_G: 1.1749\n===> Epoch[207](59/88): Loss_D: 0.1887 Loss_G: 1.6681\n===> Epoch[207](60/88): Loss_D: 0.2565 Loss_G: 1.2591\n===> Epoch[207](61/88): Loss_D: 0.2435 Loss_G: 1.1683\n===> Epoch[207](62/88): Loss_D: 0.2453 Loss_G: 1.2671\n===> Epoch[207](63/88): Loss_D: 0.2598 Loss_G: 1.1254\n===> Epoch[207](64/88): Loss_D: 0.2486 Loss_G: 1.2541\n===> Epoch[207](65/88): Loss_D: 0.2385 Loss_G: 1.3260\n===> Epoch[207](66/88): Loss_D: 0.2456 Loss_G: 1.1905\n===> Epoch[207](67/88): Loss_D: 0.2608 Loss_G: 1.1042\n===> Epoch[207](68/88): Loss_D: 0.2584 Loss_G: 1.0067\n===> Epoch[207](69/88): Loss_D: 0.2333 Loss_G: 1.3253\n===> Epoch[207](70/88): Loss_D: 0.2345 Loss_G: 1.5045\n===> Epoch[207](71/88): Loss_D: 0.2442 Loss_G: 1.2690\n===> Epoch[207](72/88): Loss_D: 0.2246 Loss_G: 1.1914\n===> Epoch[207](73/88): Loss_D: 0.2169 Loss_G: 1.1868\n===> Epoch[207](74/88): Loss_D: 0.2494 Loss_G: 1.2542\n===> Epoch[207](75/88): Loss_D: 0.2470 Loss_G: 1.3978\n===> Epoch[207](76/88): Loss_D: 0.2405 Loss_G: 1.2383\n===> Epoch[207](77/88): Loss_D: 0.2413 Loss_G: 1.4511\n===> Epoch[207](78/88): Loss_D: 0.2355 Loss_G: 1.2139\n===> Epoch[207](79/88): Loss_D: 0.2320 Loss_G: 1.4039\n===> Epoch[207](80/88): Loss_D: 0.2462 Loss_G: 1.2656\n===> Epoch[207](81/88): Loss_D: 0.2552 Loss_G: 1.1750\n===> Epoch[207](82/88): Loss_D: 0.2470 Loss_G: 1.2285\n===> Epoch[207](83/88): Loss_D: 0.1813 Loss_G: 1.5952\n===> Epoch[207](84/88): Loss_D: 0.2471 Loss_G: 1.2312\n===> Epoch[207](85/88): Loss_D: 0.2491 Loss_G: 1.2145\n===> Epoch[207](86/88): Loss_D: 0.2611 Loss_G: 0.9642\n===> Epoch[207](87/88): Loss_D: 0.2570 Loss_G: 0.9829\n===> Epoch[207](88/88): Loss_D: 0.2051 Loss_G: 1.4076\nlearning rate = 0.0000484\nlearning rate = 0.0000484\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0111} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[208](1/88): Loss_D: 0.2612 Loss_G: 0.9543\n===> Epoch[208](2/88): Loss_D: 0.2618 Loss_G: 1.1747\n===> Epoch[208](3/88): Loss_D: 0.2467 Loss_G: 1.3373\n===> Epoch[208](4/88): Loss_D: 0.2119 Loss_G: 1.5194\n===> Epoch[208](5/88): Loss_D: 0.2136 Loss_G: 1.4880\n===> Epoch[208](6/88): Loss_D: 0.2569 Loss_G: 1.1895\n===> Epoch[208](7/88): Loss_D: 0.2559 Loss_G: 1.2238\n===> Epoch[208](8/88): Loss_D: 0.1862 Loss_G: 1.6337\n===> Epoch[208](9/88): Loss_D: 0.2560 Loss_G: 1.2331\n===> Epoch[208](10/88): Loss_D: 0.2611 Loss_G: 1.2215\n===> Epoch[208](11/88): Loss_D: 0.2436 Loss_G: 1.2468\n===> Epoch[208](12/88): Loss_D: 0.2296 Loss_G: 1.5720\n===> Epoch[208](13/88): Loss_D: 0.2428 Loss_G: 1.5265\n===> Epoch[208](14/88): Loss_D: 0.2418 Loss_G: 1.2272\n===> Epoch[208](15/88): Loss_D: 0.2205 Loss_G: 1.2749\n===> Epoch[208](16/88): Loss_D: 0.2336 Loss_G: 1.4149\n===> Epoch[208](17/88): Loss_D: 0.2439 Loss_G: 0.9670\n===> Epoch[208](18/88): Loss_D: 0.2472 Loss_G: 1.1450\n===> Epoch[208](19/88): Loss_D: 0.2471 Loss_G: 1.1097\n===> Epoch[208](20/88): Loss_D: 0.2355 Loss_G: 1.2653\n===> Epoch[208](21/88): Loss_D: 0.2464 Loss_G: 1.1094\n===> Epoch[208](22/88): Loss_D: 0.2442 Loss_G: 1.3137\n===> Epoch[208](23/88): Loss_D: 0.2408 Loss_G: 1.2649\n===> Epoch[208](24/88): Loss_D: 0.2401 Loss_G: 1.2028\n===> Epoch[208](25/88): Loss_D: 0.2522 Loss_G: 1.2911\n===> Epoch[208](26/88): Loss_D: 0.2388 Loss_G: 1.2082\n===> Epoch[208](27/88): Loss_D: 0.2552 Loss_G: 1.2226\n===> Epoch[208](28/88): Loss_D: 0.2181 Loss_G: 1.5096\n===> Epoch[208](29/88): Loss_D: 0.2290 Loss_G: 1.3521\n===> Epoch[208](30/88): Loss_D: 0.2497 Loss_G: 1.4579\n===> Epoch[208](31/88): Loss_D: 0.2607 Loss_G: 1.2200\n===> Epoch[208](32/88): Loss_D: 0.2451 Loss_G: 1.2090\n===> Epoch[208](33/88): Loss_D: 0.2447 Loss_G: 1.4077\n===> Epoch[208](34/88): Loss_D: 0.2417 Loss_G: 1.1370\n===> Epoch[208](35/88): Loss_D: 0.2573 Loss_G: 1.0297\n===> Epoch[208](36/88): Loss_D: 0.1910 Loss_G: 1.5999\n===> Epoch[208](37/88): Loss_D: 0.2520 Loss_G: 1.2235\n===> Epoch[208](38/88): Loss_D: 0.2338 Loss_G: 1.3624\n===> Epoch[208](39/88): Loss_D: 0.2443 Loss_G: 1.2810\n===> Epoch[208](40/88): Loss_D: 0.2555 Loss_G: 1.2114\n===> Epoch[208](41/88): Loss_D: 0.2539 Loss_G: 1.0947\n===> Epoch[208](42/88): Loss_D: 0.2264 Loss_G: 1.1389\n===> Epoch[208](43/88): Loss_D: 0.2409 Loss_G: 1.1308\n===> Epoch[208](44/88): Loss_D: 0.2412 Loss_G: 1.2816\n===> Epoch[208](45/88): Loss_D: 0.2554 Loss_G: 1.2254\n===> Epoch[208](46/88): Loss_D: 0.2280 Loss_G: 1.3383\n===> Epoch[208](47/88): Loss_D: 0.2661 Loss_G: 1.0389\n===> Epoch[208](48/88): Loss_D: 0.1744 Loss_G: 1.5441\n===> Epoch[208](49/88): Loss_D: 0.2152 Loss_G: 2.3578\n===> Epoch[208](50/88): Loss_D: 0.2475 Loss_G: 1.2462\n===> Epoch[208](51/88): Loss_D: 0.2449 Loss_G: 1.3202\n===> Epoch[208](52/88): Loss_D: 0.2241 Loss_G: 1.5183\n===> Epoch[208](53/88): Loss_D: 0.2666 Loss_G: 1.2662\n===> Epoch[208](54/88): Loss_D: 0.2639 Loss_G: 1.2094\n===> Epoch[208](55/88): Loss_D: 0.2357 Loss_G: 1.2877\n===> Epoch[208](56/88): Loss_D: 0.2101 Loss_G: 1.4300\n===> Epoch[208](57/88): Loss_D: 0.2518 Loss_G: 1.2117\n===> Epoch[208](58/88): Loss_D: 0.2332 Loss_G: 1.4133\n===> Epoch[208](59/88): Loss_D: 0.2198 Loss_G: 1.4057\n===> Epoch[208](60/88): Loss_D: 0.2280 Loss_G: 1.4642\n===> Epoch[208](61/88): Loss_D: 0.2418 Loss_G: 1.2858\n===> Epoch[208](62/88): Loss_D: 0.2482 Loss_G: 1.2321\n===> Epoch[208](63/88): Loss_D: 0.2483 Loss_G: 1.2552\n===> Epoch[208](64/88): Loss_D: 0.2580 Loss_G: 1.2323\n===> Epoch[208](65/88): Loss_D: 0.2205 Loss_G: 1.3442\n===> Epoch[208](66/88): Loss_D: 0.2326 Loss_G: 1.4521\n===> Epoch[208](67/88): Loss_D: 0.2514 Loss_G: 1.2462\n===> Epoch[208](68/88): Loss_D: 0.2514 Loss_G: 1.1885\n===> Epoch[208](69/88): Loss_D: 0.2465 Loss_G: 1.0872\n===> Epoch[208](70/88): Loss_D: 0.2627 Loss_G: 1.2535\n===> Epoch[208](71/88): Loss_D: 0.2524 Loss_G: 1.2571\n===> Epoch[208](72/88): Loss_D: 0.2457 Loss_G: 1.2802\n===> Epoch[208](73/88): Loss_D: 0.1948 Loss_G: 1.9985\n===> Epoch[208](74/88): Loss_D: 0.2574 Loss_G: 1.3489\n===> Epoch[208](75/88): Loss_D: 0.2340 Loss_G: 1.1887\n===> Epoch[208](76/88): Loss_D: 0.2663 Loss_G: 1.2568\n===> Epoch[208](77/88): Loss_D: 0.2411 Loss_G: 1.4417\n===> Epoch[208](78/88): Loss_D: 0.2492 Loss_G: 1.4459\n===> Epoch[208](79/88): Loss_D: 0.2543 Loss_G: 1.1490\n===> Epoch[208](80/88): Loss_D: 0.2442 Loss_G: 1.3870\n===> Epoch[208](81/88): Loss_D: 0.2403 Loss_G: 1.1578\n===> Epoch[208](82/88): Loss_D: 0.2341 Loss_G: 1.4200\n===> Epoch[208](83/88): Loss_D: 0.2663 Loss_G: 1.1241\n===> Epoch[208](84/88): Loss_D: 0.2534 Loss_G: 1.0132\n===> Epoch[208](85/88): Loss_D: 0.2450 Loss_G: 1.3113\n===> Epoch[208](86/88): Loss_D: 0.2283 Loss_G: 1.2924\n===> Epoch[208](87/88): Loss_D: 0.2542 Loss_G: 1.0888\n===> Epoch[208](88/88): Loss_D: 0.2360 Loss_G: 1.2997\nlearning rate = 0.0000481\nlearning rate = 0.0000481\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0149} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[209](1/88): Loss_D: 0.2355 Loss_G: 1.2353\n===> Epoch[209](2/88): Loss_D: 0.2346 Loss_G: 1.2195\n===> Epoch[209](3/88): Loss_D: 0.2361 Loss_G: 1.4515\n===> Epoch[209](4/88): Loss_D: 0.2505 Loss_G: 1.2200\n===> Epoch[209](5/88): Loss_D: 0.2221 Loss_G: 1.4851\n===> Epoch[209](6/88): Loss_D: 0.2522 Loss_G: 1.2144\n===> Epoch[209](7/88): Loss_D: 0.2420 Loss_G: 1.1904\n===> Epoch[209](8/88): Loss_D: 0.2176 Loss_G: 1.4239\n===> Epoch[209](9/88): Loss_D: 0.2140 Loss_G: 1.4868\n===> Epoch[209](10/88): Loss_D: 0.2163 Loss_G: 1.3434\n===> Epoch[209](11/88): Loss_D: 0.2465 Loss_G: 1.3993\n===> Epoch[209](12/88): Loss_D: 0.2458 Loss_G: 1.4138\n===> Epoch[209](13/88): Loss_D: 0.2356 Loss_G: 1.1478\n===> Epoch[209](14/88): Loss_D: 0.2560 Loss_G: 1.2780\n===> Epoch[209](15/88): Loss_D: 0.2531 Loss_G: 1.2074\n===> Epoch[209](16/88): Loss_D: 0.2562 Loss_G: 1.1562\n===> Epoch[209](17/88): Loss_D: 0.2475 Loss_G: 1.3123\n===> Epoch[209](18/88): Loss_D: 0.1928 Loss_G: 1.8867\n===> Epoch[209](19/88): Loss_D: 0.1742 Loss_G: 1.5606\n===> Epoch[209](20/88): Loss_D: 0.2488 Loss_G: 1.3834\n===> Epoch[209](21/88): Loss_D: 0.2154 Loss_G: 1.4169\n===> Epoch[209](22/88): Loss_D: 0.2384 Loss_G: 1.4226\n===> Epoch[209](23/88): Loss_D: 0.2579 Loss_G: 1.2743\n===> Epoch[209](24/88): Loss_D: 0.2626 Loss_G: 1.2526\n===> Epoch[209](25/88): Loss_D: 0.2662 Loss_G: 1.0592\n===> Epoch[209](26/88): Loss_D: 0.2544 Loss_G: 1.2157\n===> Epoch[209](27/88): Loss_D: 0.2332 Loss_G: 1.4124\n===> Epoch[209](28/88): Loss_D: 0.2394 Loss_G: 1.2417\n===> Epoch[209](29/88): Loss_D: 0.2542 Loss_G: 1.2228\n===> Epoch[209](30/88): Loss_D: 0.2578 Loss_G: 1.1929\n===> Epoch[209](31/88): Loss_D: 0.1745 Loss_G: 1.6097\n===> Epoch[209](32/88): Loss_D: 0.2438 Loss_G: 1.1348\n===> Epoch[209](33/88): Loss_D: 0.2516 Loss_G: 1.1884\n===> Epoch[209](34/88): Loss_D: 0.2617 Loss_G: 1.1262\n===> Epoch[209](35/88): Loss_D: 0.2578 Loss_G: 1.2888\n===> Epoch[209](36/88): Loss_D: 0.2469 Loss_G: 1.0783\n===> Epoch[209](37/88): Loss_D: 0.2609 Loss_G: 1.1272\n===> Epoch[209](38/88): Loss_D: 0.2409 Loss_G: 1.3105\n===> Epoch[209](39/88): Loss_D: 0.2251 Loss_G: 1.4391\n===> Epoch[209](40/88): Loss_D: 0.2537 Loss_G: 1.0527\n===> Epoch[209](41/88): Loss_D: 0.2423 Loss_G: 1.1501\n===> Epoch[209](42/88): Loss_D: 0.2088 Loss_G: 2.3682\n===> Epoch[209](43/88): Loss_D: 0.2495 Loss_G: 1.2331\n===> Epoch[209](44/88): Loss_D: 0.2476 Loss_G: 1.2886\n===> Epoch[209](45/88): Loss_D: 0.2494 Loss_G: 1.3484\n===> Epoch[209](46/88): Loss_D: 0.2357 Loss_G: 1.5117\n===> Epoch[209](47/88): Loss_D: 0.2503 Loss_G: 1.3232\n===> Epoch[209](48/88): Loss_D: 0.2510 Loss_G: 1.3139\n===> Epoch[209](49/88): Loss_D: 0.2434 Loss_G: 1.1586\n===> Epoch[209](50/88): Loss_D: 0.2220 Loss_G: 1.2310\n===> Epoch[209](51/88): Loss_D: 0.2483 Loss_G: 1.1682\n===> Epoch[209](52/88): Loss_D: 0.2526 Loss_G: 1.1684\n===> Epoch[209](53/88): Loss_D: 0.2548 Loss_G: 1.3302\n===> Epoch[209](54/88): Loss_D: 0.2502 Loss_G: 1.1966\n===> Epoch[209](55/88): Loss_D: 0.2472 Loss_G: 1.2981\n===> Epoch[209](56/88): Loss_D: 0.2547 Loss_G: 1.2289\n===> Epoch[209](57/88): Loss_D: 0.2595 Loss_G: 1.0544\n===> Epoch[209](58/88): Loss_D: 0.2586 Loss_G: 1.0961\n===> Epoch[209](59/88): Loss_D: 0.2346 Loss_G: 1.4256\n===> Epoch[209](60/88): Loss_D: 0.2518 Loss_G: 1.2118\n===> Epoch[209](61/88): Loss_D: 0.2462 Loss_G: 1.0655\n===> Epoch[209](62/88): Loss_D: 0.2410 Loss_G: 0.9331\n===> Epoch[209](63/88): Loss_D: 0.2381 Loss_G: 1.3882\n===> Epoch[209](64/88): Loss_D: 0.2482 Loss_G: 1.2120\n===> Epoch[209](65/88): Loss_D: 0.2450 Loss_G: 1.3942\n===> Epoch[209](66/88): Loss_D: 0.2567 Loss_G: 1.1399\n===> Epoch[209](67/88): Loss_D: 0.2698 Loss_G: 1.1572\n===> Epoch[209](68/88): Loss_D: 0.2344 Loss_G: 1.3996\n===> Epoch[209](69/88): Loss_D: 0.2053 Loss_G: 1.5293\n===> Epoch[209](70/88): Loss_D: 0.2440 Loss_G: 1.2543\n===> Epoch[209](71/88): Loss_D: 0.2453 Loss_G: 1.2816\n===> Epoch[209](72/88): Loss_D: 0.2342 Loss_G: 1.3076\n===> Epoch[209](73/88): Loss_D: 0.2571 Loss_G: 1.2203\n===> Epoch[209](74/88): Loss_D: 0.2487 Loss_G: 1.2338\n===> Epoch[209](75/88): Loss_D: 0.1869 Loss_G: 1.6558\n===> Epoch[209](76/88): Loss_D: 0.2436 Loss_G: 1.2353\n===> Epoch[209](77/88): Loss_D: 0.2400 Loss_G: 1.2347\n===> Epoch[209](78/88): Loss_D: 0.2556 Loss_G: 1.1270\n===> Epoch[209](79/88): Loss_D: 0.2538 Loss_G: 1.0878\n===> Epoch[209](80/88): Loss_D: 0.2556 Loss_G: 1.0336\n===> Epoch[209](81/88): Loss_D: 0.2100 Loss_G: 1.3398\n===> Epoch[209](82/88): Loss_D: 0.2359 Loss_G: 1.3765\n===> Epoch[209](83/88): Loss_D: 0.2301 Loss_G: 1.2655\n===> Epoch[209](84/88): Loss_D: 0.2493 Loss_G: 1.0643\n===> Epoch[209](85/88): Loss_D: 0.2326 Loss_G: 1.4371\n===> Epoch[209](86/88): Loss_D: 0.2604 Loss_G: 1.0795\n===> Epoch[209](87/88): Loss_D: 0.2396 Loss_G: 1.2018\n===> Epoch[209](88/88): Loss_D: 0.2539 Loss_G: 1.2594\nlearning rate = 0.0000479\nlearning rate = 0.0000479\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0120} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[210](1/88): Loss_D: 0.2529 Loss_G: 1.2839\n===> Epoch[210](2/88): Loss_D: 0.2107 Loss_G: 1.5141\n===> Epoch[210](3/88): Loss_D: 0.1686 Loss_G: 1.7081\n===> Epoch[210](4/88): Loss_D: 0.2501 Loss_G: 1.1149\n===> Epoch[210](5/88): Loss_D: 0.2134 Loss_G: 1.3767\n===> Epoch[210](6/88): Loss_D: 0.2239 Loss_G: 1.3689\n===> Epoch[210](7/88): Loss_D: 0.2598 Loss_G: 1.1447\n===> Epoch[210](8/88): Loss_D: 0.2817 Loss_G: 1.1565\n===> Epoch[210](9/88): Loss_D: 0.2497 Loss_G: 1.1265\n===> Epoch[210](10/88): Loss_D: 0.2509 Loss_G: 1.3333\n===> Epoch[210](11/88): Loss_D: 0.2368 Loss_G: 1.1702\n===> Epoch[210](12/88): Loss_D: 0.2660 Loss_G: 1.3033\n===> Epoch[210](13/88): Loss_D: 0.2428 Loss_G: 1.4258\n===> Epoch[210](14/88): Loss_D: 0.2466 Loss_G: 1.1969\n===> Epoch[210](15/88): Loss_D: 0.2579 Loss_G: 1.1376\n===> Epoch[210](16/88): Loss_D: 0.2430 Loss_G: 1.1600\n===> Epoch[210](17/88): Loss_D: 0.2551 Loss_G: 0.9469\n===> Epoch[210](18/88): Loss_D: 0.2403 Loss_G: 1.1919\n===> Epoch[210](19/88): Loss_D: 0.2540 Loss_G: 1.0884\n===> Epoch[210](20/88): Loss_D: 0.2565 Loss_G: 1.1273\n===> Epoch[210](21/88): Loss_D: 0.2249 Loss_G: 1.2546\n===> Epoch[210](22/88): Loss_D: 0.2431 Loss_G: 1.3906\n===> Epoch[210](23/88): Loss_D: 0.2521 Loss_G: 1.0228\n===> Epoch[210](24/88): Loss_D: 0.2356 Loss_G: 1.4641\n===> Epoch[210](25/88): Loss_D: 0.2413 Loss_G: 1.2871\n===> Epoch[210](26/88): Loss_D: 0.2330 Loss_G: 1.1823\n===> Epoch[210](27/88): Loss_D: 0.2394 Loss_G: 1.1566\n===> Epoch[210](28/88): Loss_D: 0.2487 Loss_G: 1.2706\n===> Epoch[210](29/88): Loss_D: 0.2467 Loss_G: 1.2517\n===> Epoch[210](30/88): Loss_D: 0.2490 Loss_G: 1.0579\n===> Epoch[210](31/88): Loss_D: 0.2459 Loss_G: 1.2574\n===> Epoch[210](32/88): Loss_D: 0.2497 Loss_G: 1.1348\n===> Epoch[210](33/88): Loss_D: 0.2386 Loss_G: 1.2280\n===> Epoch[210](34/88): Loss_D: 0.2403 Loss_G: 1.1897\n===> Epoch[210](35/88): Loss_D: 0.2101 Loss_G: 1.4007\n===> Epoch[210](36/88): Loss_D: 0.2303 Loss_G: 1.3186\n===> Epoch[210](37/88): Loss_D: 0.2407 Loss_G: 1.2910\n===> Epoch[210](38/88): Loss_D: 0.2572 Loss_G: 1.0290\n===> Epoch[210](39/88): Loss_D: 0.2525 Loss_G: 1.0145\n===> Epoch[210](40/88): Loss_D: 0.2374 Loss_G: 1.2628\n===> Epoch[210](41/88): Loss_D: 0.2432 Loss_G: 1.1952\n===> Epoch[210](42/88): Loss_D: 0.2429 Loss_G: 1.3583\n===> Epoch[210](43/88): Loss_D: 0.2401 Loss_G: 1.2944\n===> Epoch[210](44/88): Loss_D: 0.2658 Loss_G: 1.1143\n===> Epoch[210](45/88): Loss_D: 0.2395 Loss_G: 1.2519\n===> Epoch[210](46/88): Loss_D: 0.2481 Loss_G: 1.3264\n===> Epoch[210](47/88): Loss_D: 0.2519 Loss_G: 1.1912\n===> Epoch[210](48/88): Loss_D: 0.2198 Loss_G: 1.3302\n===> Epoch[210](49/88): Loss_D: 0.2063 Loss_G: 1.3401\n===> Epoch[210](50/88): Loss_D: 0.2394 Loss_G: 1.1542\n===> Epoch[210](51/88): Loss_D: 0.2551 Loss_G: 1.0212\n===> Epoch[210](52/88): Loss_D: 0.2573 Loss_G: 1.2745\n===> Epoch[210](53/88): Loss_D: 0.2338 Loss_G: 1.1947\n===> Epoch[210](54/88): Loss_D: 0.2406 Loss_G: 1.2503\n===> Epoch[210](55/88): Loss_D: 0.1928 Loss_G: 1.4951\n===> Epoch[210](56/88): Loss_D: 0.2294 Loss_G: 1.2407\n===> Epoch[210](57/88): Loss_D: 0.2198 Loss_G: 1.4848\n===> Epoch[210](58/88): Loss_D: 0.2404 Loss_G: 1.3379\n===> Epoch[210](59/88): Loss_D: 0.2548 Loss_G: 1.2205\n===> Epoch[210](60/88): Loss_D: 0.2396 Loss_G: 1.3784\n===> Epoch[210](61/88): Loss_D: 0.2394 Loss_G: 1.2343\n===> Epoch[210](62/88): Loss_D: 0.2375 Loss_G: 1.4939\n===> Epoch[210](63/88): Loss_D: 0.2572 Loss_G: 1.1607\n===> Epoch[210](64/88): Loss_D: 0.2369 Loss_G: 1.3295\n===> Epoch[210](65/88): Loss_D: 0.1914 Loss_G: 1.8597\n===> Epoch[210](66/88): Loss_D: 0.2666 Loss_G: 1.1615\n===> Epoch[210](67/88): Loss_D: 0.2227 Loss_G: 1.4649\n===> Epoch[210](68/88): Loss_D: 0.2580 Loss_G: 1.2946\n===> Epoch[210](69/88): Loss_D: 0.2579 Loss_G: 1.2326\n===> Epoch[210](70/88): Loss_D: 0.2456 Loss_G: 1.2122\n===> Epoch[210](71/88): Loss_D: 0.2350 Loss_G: 1.2702\n===> Epoch[210](72/88): Loss_D: 0.2701 Loss_G: 1.2752\n===> Epoch[210](73/88): Loss_D: 0.1869 Loss_G: 1.6087\n===> Epoch[210](74/88): Loss_D: 0.2626 Loss_G: 1.2834\n===> Epoch[210](75/88): Loss_D: 0.2472 Loss_G: 1.1582\n===> Epoch[210](76/88): Loss_D: 0.2398 Loss_G: 1.4333\n===> Epoch[210](77/88): Loss_D: 0.2481 Loss_G: 1.2822\n===> Epoch[210](78/88): Loss_D: 0.2204 Loss_G: 2.3264\n===> Epoch[210](79/88): Loss_D: 0.2430 Loss_G: 1.2548\n===> Epoch[210](80/88): Loss_D: 0.2438 Loss_G: 1.3838\n===> Epoch[210](81/88): Loss_D: 0.2412 Loss_G: 1.1749\n===> Epoch[210](82/88): Loss_D: 0.2479 Loss_G: 1.3729\n===> Epoch[210](83/88): Loss_D: 0.2357 Loss_G: 1.2063\n===> Epoch[210](84/88): Loss_D: 0.2381 Loss_G: 1.1093\n===> Epoch[210](85/88): Loss_D: 0.2545 Loss_G: 0.9477\n===> Epoch[210](86/88): Loss_D: 0.1635 Loss_G: 1.5781\n===> Epoch[210](87/88): Loss_D: 0.2533 Loss_G: 1.1320\n===> Epoch[210](88/88): Loss_D: 0.2649 Loss_G: 1.1780\nlearning rate = 0.0000476\nlearning rate = 0.0000476\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0007} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[211](1/88): Loss_D: 0.2608 Loss_G: 1.2351\n===> Epoch[211](2/88): Loss_D: 0.2541 Loss_G: 1.1777\n===> Epoch[211](3/88): Loss_D: 0.2513 Loss_G: 1.0647\n===> Epoch[211](4/88): Loss_D: 0.2466 Loss_G: 1.2652\n===> Epoch[211](5/88): Loss_D: 0.2083 Loss_G: 1.4185\n===> Epoch[211](6/88): Loss_D: 0.2345 Loss_G: 1.1931\n===> Epoch[211](7/88): Loss_D: 0.2546 Loss_G: 1.2055\n===> Epoch[211](8/88): Loss_D: 0.2496 Loss_G: 1.2138\n===> Epoch[211](9/88): Loss_D: 0.2410 Loss_G: 1.3951\n===> Epoch[211](10/88): Loss_D: 0.2561 Loss_G: 1.1330\n===> Epoch[211](11/88): Loss_D: 0.2480 Loss_G: 1.0975\n===> Epoch[211](12/88): Loss_D: 0.2510 Loss_G: 1.2349\n===> Epoch[211](13/88): Loss_D: 0.2380 Loss_G: 1.1297\n===> Epoch[211](14/88): Loss_D: 0.1676 Loss_G: 1.5599\n===> Epoch[211](15/88): Loss_D: 0.2727 Loss_G: 1.2151\n===> Epoch[211](16/88): Loss_D: 0.2566 Loss_G: 1.2922\n===> Epoch[211](17/88): Loss_D: 0.1778 Loss_G: 1.6021\n===> Epoch[211](18/88): Loss_D: 0.2124 Loss_G: 1.3243\n===> Epoch[211](19/88): Loss_D: 0.2475 Loss_G: 1.1202\n===> Epoch[211](20/88): Loss_D: 0.2372 Loss_G: 1.3453\n===> Epoch[211](21/88): Loss_D: 0.2363 Loss_G: 1.1933\n===> Epoch[211](22/88): Loss_D: 0.2392 Loss_G: 1.3469\n===> Epoch[211](23/88): Loss_D: 0.2403 Loss_G: 1.2179\n===> Epoch[211](24/88): Loss_D: 0.2540 Loss_G: 1.1806\n===> Epoch[211](25/88): Loss_D: 0.2418 Loss_G: 1.2838\n===> Epoch[211](26/88): Loss_D: 0.2506 Loss_G: 1.2847\n===> Epoch[211](27/88): Loss_D: 0.2470 Loss_G: 1.2055\n===> Epoch[211](28/88): Loss_D: 0.2423 Loss_G: 1.1151\n===> Epoch[211](29/88): Loss_D: 0.2409 Loss_G: 1.1457\n===> Epoch[211](30/88): Loss_D: 0.2501 Loss_G: 0.9367\n===> Epoch[211](31/88): Loss_D: 0.2229 Loss_G: 1.3818\n===> Epoch[211](32/88): Loss_D: 0.2305 Loss_G: 1.3172\n===> Epoch[211](33/88): Loss_D: 0.2581 Loss_G: 1.2733\n===> Epoch[211](34/88): Loss_D: 0.2491 Loss_G: 1.2483\n===> Epoch[211](35/88): Loss_D: 0.2129 Loss_G: 1.3729\n===> Epoch[211](36/88): Loss_D: 0.2333 Loss_G: 1.3288\n===> Epoch[211](37/88): Loss_D: 0.2327 Loss_G: 1.5204\n===> Epoch[211](38/88): Loss_D: 0.2724 Loss_G: 1.0724\n===> Epoch[211](39/88): Loss_D: 0.2541 Loss_G: 1.1507\n===> Epoch[211](40/88): Loss_D: 0.2531 Loss_G: 1.2393\n===> Epoch[211](41/88): Loss_D: 0.2563 Loss_G: 1.1264\n===> Epoch[211](42/88): Loss_D: 0.2399 Loss_G: 1.3774\n===> Epoch[211](43/88): Loss_D: 0.2428 Loss_G: 1.1792\n===> Epoch[211](44/88): Loss_D: 0.2400 Loss_G: 1.2471\n===> Epoch[211](45/88): Loss_D: 0.2509 Loss_G: 1.2403\n===> Epoch[211](46/88): Loss_D: 0.2410 Loss_G: 1.1529\n===> Epoch[211](47/88): Loss_D: 0.1739 Loss_G: 1.9197\n===> Epoch[211](48/88): Loss_D: 0.2372 Loss_G: 1.3724\n===> Epoch[211](49/88): Loss_D: 0.2339 Loss_G: 1.5357\n===> Epoch[211](50/88): Loss_D: 0.2479 Loss_G: 1.3114\n===> Epoch[211](51/88): Loss_D: 0.2358 Loss_G: 1.2843\n===> Epoch[211](52/88): Loss_D: 0.2535 Loss_G: 1.1957\n===> Epoch[211](53/88): Loss_D: 0.2540 Loss_G: 1.2836\n===> Epoch[211](54/88): Loss_D: 0.2383 Loss_G: 1.1526\n===> Epoch[211](55/88): Loss_D: 0.2462 Loss_G: 1.2877\n===> Epoch[211](56/88): Loss_D: 0.2095 Loss_G: 1.3482\n===> Epoch[211](57/88): Loss_D: 0.2322 Loss_G: 1.4333\n===> Epoch[211](58/88): Loss_D: 0.2363 Loss_G: 1.2578\n===> Epoch[211](59/88): Loss_D: 0.2380 Loss_G: 1.0942\n===> Epoch[211](60/88): Loss_D: 0.2407 Loss_G: 1.3509\n===> Epoch[211](61/88): Loss_D: 0.2504 Loss_G: 1.0893\n===> Epoch[211](62/88): Loss_D: 0.2528 Loss_G: 1.2850\n===> Epoch[211](63/88): Loss_D: 0.2635 Loss_G: 1.0201\n===> Epoch[211](64/88): Loss_D: 0.2469 Loss_G: 1.3346\n===> Epoch[211](65/88): Loss_D: 0.2459 Loss_G: 1.2308\n===> Epoch[211](66/88): Loss_D: 0.2256 Loss_G: 1.2842\n===> Epoch[211](67/88): Loss_D: 0.2352 Loss_G: 1.3699\n===> Epoch[211](68/88): Loss_D: 0.2332 Loss_G: 1.4788\n===> Epoch[211](69/88): Loss_D: 0.2573 Loss_G: 1.2551\n===> Epoch[211](70/88): Loss_D: 0.2415 Loss_G: 1.4308\n===> Epoch[211](71/88): Loss_D: 0.2557 Loss_G: 1.0356\n===> Epoch[211](72/88): Loss_D: 0.2667 Loss_G: 1.1467\n===> Epoch[211](73/88): Loss_D: 0.2225 Loss_G: 1.4064\n===> Epoch[211](74/88): Loss_D: 0.2615 Loss_G: 1.1868\n===> Epoch[211](75/88): Loss_D: 0.2273 Loss_G: 1.3691\n===> Epoch[211](76/88): Loss_D: 0.2543 Loss_G: 1.2321\n===> Epoch[211](77/88): Loss_D: 0.2537 Loss_G: 1.1220\n===> Epoch[211](78/88): Loss_D: 0.2586 Loss_G: 1.1895\n===> Epoch[211](79/88): Loss_D: 0.2347 Loss_G: 1.2543\n===> Epoch[211](80/88): Loss_D: 0.1752 Loss_G: 1.5759\n===> Epoch[211](81/88): Loss_D: 0.2206 Loss_G: 2.3001\n===> Epoch[211](82/88): Loss_D: 0.1822 Loss_G: 1.7856\n===> Epoch[211](83/88): Loss_D: 0.2635 Loss_G: 1.3335\n===> Epoch[211](84/88): Loss_D: 0.2476 Loss_G: 1.1714\n===> Epoch[211](85/88): Loss_D: 0.2406 Loss_G: 1.3327\n===> Epoch[211](86/88): Loss_D: 0.2597 Loss_G: 1.2457\n===> Epoch[211](87/88): Loss_D: 0.2686 Loss_G: 1.1948\n===> Epoch[211](88/88): Loss_D: 0.2436 Loss_G: 1.3197\nlearning rate = 0.0000474\nlearning rate = 0.0000474\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0002} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[212](1/88): Loss_D: 0.2692 Loss_G: 1.0818\n===> Epoch[212](2/88): Loss_D: 0.2522 Loss_G: 1.3217\n===> Epoch[212](3/88): Loss_D: 0.2238 Loss_G: 1.4747\n===> Epoch[212](4/88): Loss_D: 0.2487 Loss_G: 1.1133\n===> Epoch[212](5/88): Loss_D: 0.2452 Loss_G: 1.2960\n===> Epoch[212](6/88): Loss_D: 0.2602 Loss_G: 1.2413\n===> Epoch[212](7/88): Loss_D: 0.2617 Loss_G: 1.2373\n===> Epoch[212](8/88): Loss_D: 0.2194 Loss_G: 1.3242\n===> Epoch[212](9/88): Loss_D: 0.2388 Loss_G: 1.4176\n===> Epoch[212](10/88): Loss_D: 0.2223 Loss_G: 1.1569\n===> Epoch[212](11/88): Loss_D: 0.1792 Loss_G: 1.9807\n===> Epoch[212](12/88): Loss_D: 0.2558 Loss_G: 1.2948\n===> Epoch[212](13/88): Loss_D: 0.2570 Loss_G: 1.1873\n===> Epoch[212](14/88): Loss_D: 0.2670 Loss_G: 1.2640\n===> Epoch[212](15/88): Loss_D: 0.2475 Loss_G: 1.0943\n===> Epoch[212](16/88): Loss_D: 0.2426 Loss_G: 1.2761\n===> Epoch[212](17/88): Loss_D: 0.2364 Loss_G: 1.4093\n===> Epoch[212](18/88): Loss_D: 0.2639 Loss_G: 1.3076\n===> Epoch[212](19/88): Loss_D: 0.1928 Loss_G: 1.6244\n===> Epoch[212](20/88): Loss_D: 0.2479 Loss_G: 1.0302\n===> Epoch[212](21/88): Loss_D: 0.2591 Loss_G: 1.1535\n===> Epoch[212](22/88): Loss_D: 0.2463 Loss_G: 1.1003\n===> Epoch[212](23/88): Loss_D: 0.2091 Loss_G: 2.2635\n===> Epoch[212](24/88): Loss_D: 0.2527 Loss_G: 1.1196\n===> Epoch[212](25/88): Loss_D: 0.2556 Loss_G: 1.1610\n===> Epoch[212](26/88): Loss_D: 0.2431 Loss_G: 1.1367\n===> Epoch[212](27/88): Loss_D: 0.2450 Loss_G: 1.3636\n===> Epoch[212](28/88): Loss_D: 0.2388 Loss_G: 1.3977\n===> Epoch[212](29/88): Loss_D: 0.2415 Loss_G: 1.4676\n===> Epoch[212](30/88): Loss_D: 0.2509 Loss_G: 1.2801\n===> Epoch[212](31/88): Loss_D: 0.2428 Loss_G: 1.1825\n===> Epoch[212](32/88): Loss_D: 0.2521 Loss_G: 1.1011\n===> Epoch[212](33/88): Loss_D: 0.1878 Loss_G: 1.5596\n===> Epoch[212](34/88): Loss_D: 0.2514 Loss_G: 1.2883\n===> Epoch[212](35/88): Loss_D: 0.2439 Loss_G: 1.3980\n===> Epoch[212](36/88): Loss_D: 0.2502 Loss_G: 1.0724\n===> Epoch[212](37/88): Loss_D: 0.2446 Loss_G: 1.3639\n===> Epoch[212](38/88): Loss_D: 0.2487 Loss_G: 1.0097\n===> Epoch[212](39/88): Loss_D: 0.2570 Loss_G: 1.1523\n===> Epoch[212](40/88): Loss_D: 0.2500 Loss_G: 1.0359\n===> Epoch[212](41/88): Loss_D: 0.2438 Loss_G: 1.2896\n===> Epoch[212](42/88): Loss_D: 0.2614 Loss_G: 1.0008\n===> Epoch[212](43/88): Loss_D: 0.2542 Loss_G: 0.9951\n===> Epoch[212](44/88): Loss_D: 0.2598 Loss_G: 1.1047\n===> Epoch[212](45/88): Loss_D: 0.2521 Loss_G: 1.1179\n===> Epoch[212](46/88): Loss_D: 0.2341 Loss_G: 1.3417\n===> Epoch[212](47/88): Loss_D: 0.1985 Loss_G: 1.6385\n===> Epoch[212](48/88): Loss_D: 0.2727 Loss_G: 1.2159\n===> Epoch[212](49/88): Loss_D: 0.2334 Loss_G: 1.1956\n===> Epoch[212](50/88): Loss_D: 0.2527 Loss_G: 1.2606\n===> Epoch[212](51/88): Loss_D: 0.2537 Loss_G: 1.1766\n===> Epoch[212](52/88): Loss_D: 0.2229 Loss_G: 1.4793\n===> Epoch[212](53/88): Loss_D: 0.2572 Loss_G: 1.2180\n===> Epoch[212](54/88): Loss_D: 0.2574 Loss_G: 1.2431\n===> Epoch[212](55/88): Loss_D: 0.2507 Loss_G: 1.2335\n===> Epoch[212](56/88): Loss_D: 0.2361 Loss_G: 1.2838\n===> Epoch[212](57/88): Loss_D: 0.2467 Loss_G: 1.1833\n===> Epoch[212](58/88): Loss_D: 0.2575 Loss_G: 1.2159\n===> Epoch[212](59/88): Loss_D: 0.2327 Loss_G: 1.4476\n===> Epoch[212](60/88): Loss_D: 0.2462 Loss_G: 1.3593\n===> Epoch[212](61/88): Loss_D: 0.2481 Loss_G: 1.2348\n===> Epoch[212](62/88): Loss_D: 0.2281 Loss_G: 1.4105\n===> Epoch[212](63/88): Loss_D: 0.2381 Loss_G: 1.1960\n===> Epoch[212](64/88): Loss_D: 0.2477 Loss_G: 1.2645\n===> Epoch[212](65/88): Loss_D: 0.2519 Loss_G: 1.2023\n===> Epoch[212](66/88): Loss_D: 0.2370 Loss_G: 1.4649\n===> Epoch[212](67/88): Loss_D: 0.1704 Loss_G: 1.5436\n===> Epoch[212](68/88): Loss_D: 0.2263 Loss_G: 1.2988\n===> Epoch[212](69/88): Loss_D: 0.2314 Loss_G: 1.3988\n===> Epoch[212](70/88): Loss_D: 0.2718 Loss_G: 1.1469\n===> Epoch[212](71/88): Loss_D: 0.2547 Loss_G: 1.1637\n===> Epoch[212](72/88): Loss_D: 0.2444 Loss_G: 1.1706\n===> Epoch[212](73/88): Loss_D: 0.2498 Loss_G: 1.1864\n===> Epoch[212](74/88): Loss_D: 0.2302 Loss_G: 1.3527\n===> Epoch[212](75/88): Loss_D: 0.2594 Loss_G: 1.3185\n===> Epoch[212](76/88): Loss_D: 0.2267 Loss_G: 1.4233\n===> Epoch[212](77/88): Loss_D: 0.2458 Loss_G: 1.1806\n===> Epoch[212](78/88): Loss_D: 0.2381 Loss_G: 1.3245\n===> Epoch[212](79/88): Loss_D: 0.2508 Loss_G: 1.1883\n===> Epoch[212](80/88): Loss_D: 0.2408 Loss_G: 1.2372\n===> Epoch[212](81/88): Loss_D: 0.2489 Loss_G: 1.1665\n===> Epoch[212](82/88): Loss_D: 0.2330 Loss_G: 1.2096\n===> Epoch[212](83/88): Loss_D: 0.2371 Loss_G: 1.3256\n===> Epoch[212](84/88): Loss_D: 0.2505 Loss_G: 1.1113\n===> Epoch[212](85/88): Loss_D: 0.2256 Loss_G: 1.3777\n===> Epoch[212](86/88): Loss_D: 0.2485 Loss_G: 1.2636\n===> Epoch[212](87/88): Loss_D: 0.2508 Loss_G: 1.1970\n===> Epoch[212](88/88): Loss_D: 0.2410 Loss_G: 1.3072\nlearning rate = 0.0000471\nlearning rate = 0.0000471\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0013} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[213](1/88): Loss_D: 0.2046 Loss_G: 1.2158\n===> Epoch[213](2/88): Loss_D: 0.2448 Loss_G: 1.2142\n===> Epoch[213](3/88): Loss_D: 0.2653 Loss_G: 1.1210\n===> Epoch[213](4/88): Loss_D: 0.2210 Loss_G: 1.3414\n===> Epoch[213](5/88): Loss_D: 0.2436 Loss_G: 1.3025\n===> Epoch[213](6/88): Loss_D: 0.2557 Loss_G: 1.1114\n===> Epoch[213](7/88): Loss_D: 0.2592 Loss_G: 1.0939\n===> Epoch[213](8/88): Loss_D: 0.2314 Loss_G: 1.2653\n===> Epoch[213](9/88): Loss_D: 0.2365 Loss_G: 1.1715\n===> Epoch[213](10/88): Loss_D: 0.2490 Loss_G: 1.1677\n===> Epoch[213](11/88): Loss_D: 0.2409 Loss_G: 1.3020\n===> Epoch[213](12/88): Loss_D: 0.2534 Loss_G: 1.1188\n===> Epoch[213](13/88): Loss_D: 0.2404 Loss_G: 1.2073\n===> Epoch[213](14/88): Loss_D: 0.2548 Loss_G: 1.0741\n===> Epoch[213](15/88): Loss_D: 0.2433 Loss_G: 1.2385\n===> Epoch[213](16/88): Loss_D: 0.2496 Loss_G: 1.0579\n===> Epoch[213](17/88): Loss_D: 0.2220 Loss_G: 1.3202\n===> Epoch[213](18/88): Loss_D: 0.2627 Loss_G: 1.1313\n===> Epoch[213](19/88): Loss_D: 0.2401 Loss_G: 1.4191\n===> Epoch[213](20/88): Loss_D: 0.2398 Loss_G: 1.1563\n===> Epoch[213](21/88): Loss_D: 0.2325 Loss_G: 1.3338\n===> Epoch[213](22/88): Loss_D: 0.2238 Loss_G: 1.3918\n===> Epoch[213](23/88): Loss_D: 0.2453 Loss_G: 1.3669\n===> Epoch[213](24/88): Loss_D: 0.2696 Loss_G: 1.0638\n===> Epoch[213](25/88): Loss_D: 0.2212 Loss_G: 1.4051\n===> Epoch[213](26/88): Loss_D: 0.2436 Loss_G: 1.2951\n===> Epoch[213](27/88): Loss_D: 0.2472 Loss_G: 1.1840\n===> Epoch[213](28/88): Loss_D: 0.2411 Loss_G: 1.2616\n===> Epoch[213](29/88): Loss_D: 0.2498 Loss_G: 1.2259\n===> Epoch[213](30/88): Loss_D: 0.2454 Loss_G: 1.1597\n===> Epoch[213](31/88): Loss_D: 0.2412 Loss_G: 1.3917\n===> Epoch[213](32/88): Loss_D: 0.2494 Loss_G: 1.1959\n===> Epoch[213](33/88): Loss_D: 0.2471 Loss_G: 1.2290\n===> Epoch[213](34/88): Loss_D: 0.2387 Loss_G: 1.2777\n===> Epoch[213](35/88): Loss_D: 0.2406 Loss_G: 1.1251\n===> Epoch[213](36/88): Loss_D: 0.2591 Loss_G: 1.1746\n===> Epoch[213](37/88): Loss_D: 0.2165 Loss_G: 1.1761\n===> Epoch[213](38/88): Loss_D: 0.2325 Loss_G: 1.2354\n===> Epoch[213](39/88): Loss_D: 0.2420 Loss_G: 1.1272\n===> Epoch[213](40/88): Loss_D: 0.2133 Loss_G: 1.5791\n===> Epoch[213](41/88): Loss_D: 0.2606 Loss_G: 1.1020\n===> Epoch[213](42/88): Loss_D: 0.2576 Loss_G: 1.2754\n===> Epoch[213](43/88): Loss_D: 0.2561 Loss_G: 1.2971\n===> Epoch[213](44/88): Loss_D: 0.2474 Loss_G: 1.4250\n===> Epoch[213](45/88): Loss_D: 0.2512 Loss_G: 1.2568\n===> Epoch[213](46/88): Loss_D: 0.2569 Loss_G: 1.3243\n===> Epoch[213](47/88): Loss_D: 0.2534 Loss_G: 1.1693\n===> Epoch[213](48/88): Loss_D: 0.2312 Loss_G: 2.2207\n===> Epoch[213](49/88): Loss_D: 0.2183 Loss_G: 1.5293\n===> Epoch[213](50/88): Loss_D: 0.2175 Loss_G: 1.5543\n===> Epoch[213](51/88): Loss_D: 0.2536 Loss_G: 1.2171\n===> Epoch[213](52/88): Loss_D: 0.2417 Loss_G: 1.2479\n===> Epoch[213](53/88): Loss_D: 0.2676 Loss_G: 1.2643\n===> Epoch[213](54/88): Loss_D: 0.2455 Loss_G: 1.4967\n===> Epoch[213](55/88): Loss_D: 0.2276 Loss_G: 1.4993\n===> Epoch[213](56/88): Loss_D: 0.2507 Loss_G: 1.2665\n===> Epoch[213](57/88): Loss_D: 0.1597 Loss_G: 1.5967\n===> Epoch[213](58/88): Loss_D: 0.2031 Loss_G: 1.5383\n===> Epoch[213](59/88): Loss_D: 0.2549 Loss_G: 1.2703\n===> Epoch[213](60/88): Loss_D: 0.2566 Loss_G: 1.3305\n===> Epoch[213](61/88): Loss_D: 0.1819 Loss_G: 1.9462\n===> Epoch[213](62/88): Loss_D: 0.2442 Loss_G: 1.3614\n===> Epoch[213](63/88): Loss_D: 0.2314 Loss_G: 1.3340\n===> Epoch[213](64/88): Loss_D: 0.2049 Loss_G: 1.3566\n===> Epoch[213](65/88): Loss_D: 0.2509 Loss_G: 1.0012\n===> Epoch[213](66/88): Loss_D: 0.2390 Loss_G: 1.3287\n===> Epoch[213](67/88): Loss_D: 0.2593 Loss_G: 1.1676\n===> Epoch[213](68/88): Loss_D: 0.2321 Loss_G: 1.2586\n===> Epoch[213](69/88): Loss_D: 0.2471 Loss_G: 1.1120\n===> Epoch[213](70/88): Loss_D: 0.2346 Loss_G: 1.3195\n===> Epoch[213](71/88): Loss_D: 0.2605 Loss_G: 1.2034\n===> Epoch[213](72/88): Loss_D: 0.2491 Loss_G: 1.2541\n===> Epoch[213](73/88): Loss_D: 0.2451 Loss_G: 1.1333\n===> Epoch[213](74/88): Loss_D: 0.2600 Loss_G: 1.0370\n===> Epoch[213](75/88): Loss_D: 0.1907 Loss_G: 1.6888\n===> Epoch[213](76/88): Loss_D: 0.2550 Loss_G: 1.0577\n===> Epoch[213](77/88): Loss_D: 0.2468 Loss_G: 1.3016\n===> Epoch[213](78/88): Loss_D: 0.2303 Loss_G: 1.1592\n===> Epoch[213](79/88): Loss_D: 0.2593 Loss_G: 1.2719\n===> Epoch[213](80/88): Loss_D: 0.1935 Loss_G: 1.5026\n===> Epoch[213](81/88): Loss_D: 0.2376 Loss_G: 1.3777\n===> Epoch[213](82/88): Loss_D: 0.2564 Loss_G: 1.0765\n===> Epoch[213](83/88): Loss_D: 0.2512 Loss_G: 1.3628\n===> Epoch[213](84/88): Loss_D: 0.2602 Loss_G: 1.0979\n===> Epoch[213](85/88): Loss_D: 0.2417 Loss_G: 1.4233\n===> Epoch[213](86/88): Loss_D: 0.2549 Loss_G: 1.2298\n===> Epoch[213](87/88): Loss_D: 0.2584 Loss_G: 1.1016\n===> Epoch[213](88/88): Loss_D: 0.2449 Loss_G: 1.2377\nlearning rate = 0.0000469\nlearning rate = 0.0000469\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0062} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[214](1/88): Loss_D: 0.2492 Loss_G: 1.2847\n===> Epoch[214](2/88): Loss_D: 0.2363 Loss_G: 1.2503\n===> Epoch[214](3/88): Loss_D: 0.2461 Loss_G: 1.1615\n===> Epoch[214](4/88): Loss_D: 0.2379 Loss_G: 1.1764\n===> Epoch[214](5/88): Loss_D: 0.2579 Loss_G: 1.0460\n===> Epoch[214](6/88): Loss_D: 0.2491 Loss_G: 1.0827\n===> Epoch[214](7/88): Loss_D: 0.2292 Loss_G: 1.1430\n===> Epoch[214](8/88): Loss_D: 0.2498 Loss_G: 1.2515\n===> Epoch[214](9/88): Loss_D: 0.2268 Loss_G: 1.4959\n===> Epoch[214](10/88): Loss_D: 0.2063 Loss_G: 1.4746\n===> Epoch[214](11/88): Loss_D: 0.2660 Loss_G: 1.3413\n===> Epoch[214](12/88): Loss_D: 0.2652 Loss_G: 1.0465\n===> Epoch[214](13/88): Loss_D: 0.2523 Loss_G: 1.2608\n===> Epoch[214](14/88): Loss_D: 0.2535 Loss_G: 1.2646\n===> Epoch[214](15/88): Loss_D: 0.2557 Loss_G: 1.2387\n===> Epoch[214](16/88): Loss_D: 0.2619 Loss_G: 1.0777\n===> Epoch[214](17/88): Loss_D: 0.2454 Loss_G: 1.2850\n===> Epoch[214](18/88): Loss_D: 0.2452 Loss_G: 1.0572\n===> Epoch[214](19/88): Loss_D: 0.2467 Loss_G: 1.1960\n===> Epoch[214](20/88): Loss_D: 0.2236 Loss_G: 1.1396\n===> Epoch[214](21/88): Loss_D: 0.2336 Loss_G: 1.4222\n===> Epoch[214](22/88): Loss_D: 0.2378 Loss_G: 1.2284\n===> Epoch[214](23/88): Loss_D: 0.2442 Loss_G: 1.3000\n===> Epoch[214](24/88): Loss_D: 0.2059 Loss_G: 1.4206\n===> Epoch[214](25/88): Loss_D: 0.2489 Loss_G: 1.1082\n===> Epoch[214](26/88): Loss_D: 0.2386 Loss_G: 1.3361\n===> Epoch[214](27/88): Loss_D: 0.2557 Loss_G: 1.0596\n===> Epoch[214](28/88): Loss_D: 0.2427 Loss_G: 1.0613\n===> Epoch[214](29/88): Loss_D: 0.2477 Loss_G: 1.0866\n===> Epoch[214](30/88): Loss_D: 0.2265 Loss_G: 1.4190\n===> Epoch[214](31/88): Loss_D: 0.2540 Loss_G: 1.0244\n===> Epoch[214](32/88): Loss_D: 0.2564 Loss_G: 1.2726\n===> Epoch[214](33/88): Loss_D: 0.2544 Loss_G: 1.2152\n===> Epoch[214](34/88): Loss_D: 0.2337 Loss_G: 1.4115\n===> Epoch[214](35/88): Loss_D: 0.2278 Loss_G: 1.3325\n===> Epoch[214](36/88): Loss_D: 0.2461 Loss_G: 1.2188\n===> Epoch[214](37/88): Loss_D: 0.2340 Loss_G: 1.2828\n===> Epoch[214](38/88): Loss_D: 0.2489 Loss_G: 1.1502\n===> Epoch[214](39/88): Loss_D: 0.2363 Loss_G: 1.2065\n===> Epoch[214](40/88): Loss_D: 0.2397 Loss_G: 1.1460\n===> Epoch[214](41/88): Loss_D: 0.2142 Loss_G: 1.5358\n===> Epoch[214](42/88): Loss_D: 0.2369 Loss_G: 1.2810\n===> Epoch[214](43/88): Loss_D: 0.2564 Loss_G: 1.0310\n===> Epoch[214](44/88): Loss_D: 0.2497 Loss_G: 1.0097\n===> Epoch[214](45/88): Loss_D: 0.2449 Loss_G: 1.1858\n===> Epoch[214](46/88): Loss_D: 0.2448 Loss_G: 1.2404\n===> Epoch[214](47/88): Loss_D: 0.2220 Loss_G: 2.2966\n===> Epoch[214](48/88): Loss_D: 0.2356 Loss_G: 1.2187\n===> Epoch[214](49/88): Loss_D: 0.2643 Loss_G: 1.1970\n===> Epoch[214](50/88): Loss_D: 0.2397 Loss_G: 1.1294\n===> Epoch[214](51/88): Loss_D: 0.2573 Loss_G: 1.1101\n===> Epoch[214](52/88): Loss_D: 0.2339 Loss_G: 1.4698\n===> Epoch[214](53/88): Loss_D: 0.1814 Loss_G: 1.8437\n===> Epoch[214](54/88): Loss_D: 0.2533 Loss_G: 1.1380\n===> Epoch[214](55/88): Loss_D: 0.2433 Loss_G: 1.4156\n===> Epoch[214](56/88): Loss_D: 0.2456 Loss_G: 1.2942\n===> Epoch[214](57/88): Loss_D: 0.2201 Loss_G: 1.4267\n===> Epoch[214](58/88): Loss_D: 0.2149 Loss_G: 1.4114\n===> Epoch[214](59/88): Loss_D: 0.1915 Loss_G: 1.4762\n===> Epoch[214](60/88): Loss_D: 0.2754 Loss_G: 1.2283\n===> Epoch[214](61/88): Loss_D: 0.2569 Loss_G: 1.2119\n===> Epoch[214](62/88): Loss_D: 0.2676 Loss_G: 1.2711\n===> Epoch[214](63/88): Loss_D: 0.2666 Loss_G: 1.2662\n===> Epoch[214](64/88): Loss_D: 0.2378 Loss_G: 1.2790\n===> Epoch[214](65/88): Loss_D: 0.2136 Loss_G: 1.4695\n===> Epoch[214](66/88): Loss_D: 0.2468 Loss_G: 1.1965\n===> Epoch[214](67/88): Loss_D: 0.2184 Loss_G: 1.5114\n===> Epoch[214](68/88): Loss_D: 0.2393 Loss_G: 1.3022\n===> Epoch[214](69/88): Loss_D: 0.2304 Loss_G: 1.3561\n===> Epoch[214](70/88): Loss_D: 0.2635 Loss_G: 0.9650\n===> Epoch[214](71/88): Loss_D: 0.2572 Loss_G: 1.2390\n===> Epoch[214](72/88): Loss_D: 0.2571 Loss_G: 1.1760\n===> Epoch[214](73/88): Loss_D: 0.2405 Loss_G: 1.4044\n===> Epoch[214](74/88): Loss_D: 0.2361 Loss_G: 1.2919\n===> Epoch[214](75/88): Loss_D: 0.2615 Loss_G: 1.0171\n===> Epoch[214](76/88): Loss_D: 0.2350 Loss_G: 1.3830\n===> Epoch[214](77/88): Loss_D: 0.2566 Loss_G: 1.2138\n===> Epoch[214](78/88): Loss_D: 0.1825 Loss_G: 1.5342\n===> Epoch[214](79/88): Loss_D: 0.2625 Loss_G: 1.2707\n===> Epoch[214](80/88): Loss_D: 0.2323 Loss_G: 1.4524\n===> Epoch[214](81/88): Loss_D: 0.1826 Loss_G: 1.5950\n===> Epoch[214](82/88): Loss_D: 0.2578 Loss_G: 1.1329\n===> Epoch[214](83/88): Loss_D: 0.2622 Loss_G: 1.2284\n===> Epoch[214](84/88): Loss_D: 0.2456 Loss_G: 1.1902\n===> Epoch[214](85/88): Loss_D: 0.2245 Loss_G: 1.4408\n===> Epoch[214](86/88): Loss_D: 0.2455 Loss_G: 1.2168\n===> Epoch[214](87/88): Loss_D: 0.2415 Loss_G: 1.1899\n===> Epoch[214](88/88): Loss_D: 0.2199 Loss_G: 1.1927\nlearning rate = 0.0000466\nlearning rate = 0.0000466\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0000} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[215](1/88): Loss_D: 0.2528 Loss_G: 1.1566\n===> Epoch[215](2/88): Loss_D: 0.2371 Loss_G: 1.2036\n===> Epoch[215](3/88): Loss_D: 0.2038 Loss_G: 1.8419\n===> Epoch[215](4/88): Loss_D: 0.2436 Loss_G: 1.3093\n===> Epoch[215](5/88): Loss_D: 0.2558 Loss_G: 1.1734\n===> Epoch[215](6/88): Loss_D: 0.2103 Loss_G: 1.3991\n===> Epoch[215](7/88): Loss_D: 0.2606 Loss_G: 1.2579\n===> Epoch[215](8/88): Loss_D: 0.2563 Loss_G: 1.2209\n===> Epoch[215](9/88): Loss_D: 0.2646 Loss_G: 1.1821\n===> Epoch[215](10/88): Loss_D: 0.2499 Loss_G: 1.1902\n===> Epoch[215](11/88): Loss_D: 0.2338 Loss_G: 1.4050\n===> Epoch[215](12/88): Loss_D: 0.2496 Loss_G: 1.2040\n===> Epoch[215](13/88): Loss_D: 0.2482 Loss_G: 1.1707\n===> Epoch[215](14/88): Loss_D: 0.2468 Loss_G: 1.3743\n===> Epoch[215](15/88): Loss_D: 0.2415 Loss_G: 1.2896\n===> Epoch[215](16/88): Loss_D: 0.2421 Loss_G: 0.9625\n===> Epoch[215](17/88): Loss_D: 0.2508 Loss_G: 1.0297\n===> Epoch[215](18/88): Loss_D: 0.2620 Loss_G: 1.0508\n===> Epoch[215](19/88): Loss_D: 0.2526 Loss_G: 1.0258\n===> Epoch[215](20/88): Loss_D: 0.2389 Loss_G: 1.0827\n===> Epoch[215](21/88): Loss_D: 0.2369 Loss_G: 1.2590\n===> Epoch[215](22/88): Loss_D: 0.2396 Loss_G: 1.3773\n===> Epoch[215](23/88): Loss_D: 0.2503 Loss_G: 1.2229\n===> Epoch[215](24/88): Loss_D: 0.2320 Loss_G: 1.4315\n===> Epoch[215](25/88): Loss_D: 0.2480 Loss_G: 1.2746\n===> Epoch[215](26/88): Loss_D: 0.2376 Loss_G: 1.3936\n===> Epoch[215](27/88): Loss_D: 0.2480 Loss_G: 1.1122\n===> Epoch[215](28/88): Loss_D: 0.2429 Loss_G: 1.3053\n===> Epoch[215](29/88): Loss_D: 0.2350 Loss_G: 1.1465\n===> Epoch[215](30/88): Loss_D: 0.2471 Loss_G: 1.1936\n===> Epoch[215](31/88): Loss_D: 0.1593 Loss_G: 1.6003\n===> Epoch[215](32/88): Loss_D: 0.2261 Loss_G: 1.2789\n===> Epoch[215](33/88): Loss_D: 0.2478 Loss_G: 1.2232\n===> Epoch[215](34/88): Loss_D: 0.1990 Loss_G: 1.5972\n===> Epoch[215](35/88): Loss_D: 0.2167 Loss_G: 1.5485\n===> Epoch[215](36/88): Loss_D: 0.1695 Loss_G: 1.5170\n===> Epoch[215](37/88): Loss_D: 0.2466 Loss_G: 1.4353\n===> Epoch[215](38/88): Loss_D: 0.2604 Loss_G: 1.3526\n===> Epoch[215](39/88): Loss_D: 0.2573 Loss_G: 1.3415\n===> Epoch[215](40/88): Loss_D: 0.2503 Loss_G: 1.2901\n===> Epoch[215](41/88): Loss_D: 0.2026 Loss_G: 2.2406\n===> Epoch[215](42/88): Loss_D: 0.2583 Loss_G: 1.1006\n===> Epoch[215](43/88): Loss_D: 0.2642 Loss_G: 1.1317\n===> Epoch[215](44/88): Loss_D: 0.2508 Loss_G: 1.2502\n===> Epoch[215](45/88): Loss_D: 0.2579 Loss_G: 1.2368\n===> Epoch[215](46/88): Loss_D: 0.2536 Loss_G: 1.2187\n===> Epoch[215](47/88): Loss_D: 0.2389 Loss_G: 1.2515\n===> Epoch[215](48/88): Loss_D: 0.2412 Loss_G: 1.4228\n===> Epoch[215](49/88): Loss_D: 0.2482 Loss_G: 1.2229\n===> Epoch[215](50/88): Loss_D: 0.2452 Loss_G: 1.4499\n===> Epoch[215](51/88): Loss_D: 0.2669 Loss_G: 1.1058\n===> Epoch[215](52/88): Loss_D: 0.2576 Loss_G: 1.1544\n===> Epoch[215](53/88): Loss_D: 0.2557 Loss_G: 1.1855\n===> Epoch[215](54/88): Loss_D: 0.2462 Loss_G: 1.1508\n===> Epoch[215](55/88): Loss_D: 0.2453 Loss_G: 1.2375\n===> Epoch[215](56/88): Loss_D: 0.1853 Loss_G: 1.6616\n===> Epoch[215](57/88): Loss_D: 0.2550 Loss_G: 1.2789\n===> Epoch[215](58/88): Loss_D: 0.2656 Loss_G: 1.1234\n===> Epoch[215](59/88): Loss_D: 0.2502 Loss_G: 1.1580\n===> Epoch[215](60/88): Loss_D: 0.2359 Loss_G: 1.3380\n===> Epoch[215](61/88): Loss_D: 0.2439 Loss_G: 1.0908\n===> Epoch[215](62/88): Loss_D: 0.2306 Loss_G: 1.5085\n===> Epoch[215](63/88): Loss_D: 0.2543 Loss_G: 1.1205\n===> Epoch[215](64/88): Loss_D: 0.2367 Loss_G: 1.4048\n===> Epoch[215](65/88): Loss_D: 0.2495 Loss_G: 1.0279\n===> Epoch[215](66/88): Loss_D: 0.2359 Loss_G: 1.2052\n===> Epoch[215](67/88): Loss_D: 0.2248 Loss_G: 1.4057\n===> Epoch[215](68/88): Loss_D: 0.2338 Loss_G: 1.2232\n===> Epoch[215](69/88): Loss_D: 0.2470 Loss_G: 1.1919\n===> Epoch[215](70/88): Loss_D: 0.2370 Loss_G: 1.3639\n===> Epoch[215](71/88): Loss_D: 0.2452 Loss_G: 1.2741\n===> Epoch[215](72/88): Loss_D: 0.2205 Loss_G: 1.3964\n===> Epoch[215](73/88): Loss_D: 0.2715 Loss_G: 1.1898\n===> Epoch[215](74/88): Loss_D: 0.2349 Loss_G: 1.2430\n===> Epoch[215](75/88): Loss_D: 0.2505 Loss_G: 1.2180\n===> Epoch[215](76/88): Loss_D: 0.2471 Loss_G: 1.1793\n===> Epoch[215](77/88): Loss_D: 0.2415 Loss_G: 1.3281\n===> Epoch[215](78/88): Loss_D: 0.2468 Loss_G: 1.1918\n===> Epoch[215](79/88): Loss_D: 0.2581 Loss_G: 1.1940\n===> Epoch[215](80/88): Loss_D: 0.2558 Loss_G: 1.2286\n===> Epoch[215](81/88): Loss_D: 0.2592 Loss_G: 1.1678\n===> Epoch[215](82/88): Loss_D: 0.2315 Loss_G: 1.0613\n===> Epoch[215](83/88): Loss_D: 0.2363 Loss_G: 1.2659\n===> Epoch[215](84/88): Loss_D: 0.2176 Loss_G: 1.2845\n===> Epoch[215](85/88): Loss_D: 0.2614 Loss_G: 0.9789\n===> Epoch[215](86/88): Loss_D: 0.2356 Loss_G: 1.2067\n===> Epoch[215](87/88): Loss_D: 0.2351 Loss_G: 1.3570\n===> Epoch[215](88/88): Loss_D: 0.2148 Loss_G: 1.2346\nlearning rate = 0.0000464\nlearning rate = 0.0000464\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0033} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[216](1/88): Loss_D: 0.2388 Loss_G: 1.1967\n===> Epoch[216](2/88): Loss_D: 0.2467 Loss_G: 1.3141\n===> Epoch[216](3/88): Loss_D: 0.2420 Loss_G: 1.3306\n===> Epoch[216](4/88): Loss_D: 0.2522 Loss_G: 1.0759\n===> Epoch[216](5/88): Loss_D: 0.2378 Loss_G: 1.3474\n===> Epoch[216](6/88): Loss_D: 0.2348 Loss_G: 1.2345\n===> Epoch[216](7/88): Loss_D: 0.2568 Loss_G: 1.1313\n===> Epoch[216](8/88): Loss_D: 0.2289 Loss_G: 1.2321\n===> Epoch[216](9/88): Loss_D: 0.2482 Loss_G: 1.3316\n===> Epoch[216](10/88): Loss_D: 0.2360 Loss_G: 1.3860\n===> Epoch[216](11/88): Loss_D: 0.2581 Loss_G: 1.1114\n===> Epoch[216](12/88): Loss_D: 0.2558 Loss_G: 1.1233\n===> Epoch[216](13/88): Loss_D: 0.2460 Loss_G: 1.1945\n===> Epoch[216](14/88): Loss_D: 0.1754 Loss_G: 1.9224\n===> Epoch[216](15/88): Loss_D: 0.2458 Loss_G: 1.2174\n===> Epoch[216](16/88): Loss_D: 0.2394 Loss_G: 1.0559\n===> Epoch[216](17/88): Loss_D: 0.2441 Loss_G: 1.3562\n===> Epoch[216](18/88): Loss_D: 0.2392 Loss_G: 1.0873\n===> Epoch[216](19/88): Loss_D: 0.2564 Loss_G: 1.2100\n===> Epoch[216](20/88): Loss_D: 0.2492 Loss_G: 1.1248\n===> Epoch[216](21/88): Loss_D: 0.2553 Loss_G: 1.1968\n===> Epoch[216](22/88): Loss_D: 0.2352 Loss_G: 1.1936\n===> Epoch[216](23/88): Loss_D: 0.2386 Loss_G: 1.3351\n===> Epoch[216](24/88): Loss_D: 0.2372 Loss_G: 1.1294\n===> Epoch[216](25/88): Loss_D: 0.2271 Loss_G: 1.3829\n===> Epoch[216](26/88): Loss_D: 0.2677 Loss_G: 1.3143\n===> Epoch[216](27/88): Loss_D: 0.1987 Loss_G: 1.5237\n===> Epoch[216](28/88): Loss_D: 0.2227 Loss_G: 1.3716\n===> Epoch[216](29/88): Loss_D: 0.2230 Loss_G: 1.5034\n===> Epoch[216](30/88): Loss_D: 0.2618 Loss_G: 1.2466\n===> Epoch[216](31/88): Loss_D: 0.2368 Loss_G: 1.3031\n===> Epoch[216](32/88): Loss_D: 0.2499 Loss_G: 1.2264\n===> Epoch[216](33/88): Loss_D: 0.1800 Loss_G: 1.5020\n===> Epoch[216](34/88): Loss_D: 0.2617 Loss_G: 1.2932\n===> Epoch[216](35/88): Loss_D: 0.2634 Loss_G: 1.2607\n===> Epoch[216](36/88): Loss_D: 0.2444 Loss_G: 1.3616\n===> Epoch[216](37/88): Loss_D: 0.2705 Loss_G: 1.1374\n===> Epoch[216](38/88): Loss_D: 0.2374 Loss_G: 1.2436\n===> Epoch[216](39/88): Loss_D: 0.1793 Loss_G: 1.5825\n===> Epoch[216](40/88): Loss_D: 0.2563 Loss_G: 1.2235\n===> Epoch[216](41/88): Loss_D: 0.2305 Loss_G: 1.4313\n===> Epoch[216](42/88): Loss_D: 0.2189 Loss_G: 1.4696\n===> Epoch[216](43/88): Loss_D: 0.2324 Loss_G: 1.3205\n===> Epoch[216](44/88): Loss_D: 0.2436 Loss_G: 1.2111\n===> Epoch[216](45/88): Loss_D: 0.2055 Loss_G: 1.5420\n===> Epoch[216](46/88): Loss_D: 0.2224 Loss_G: 1.4860\n===> Epoch[216](47/88): Loss_D: 0.2467 Loss_G: 1.1778\n===> Epoch[216](48/88): Loss_D: 0.2536 Loss_G: 1.2402\n===> Epoch[216](49/88): Loss_D: 0.2505 Loss_G: 1.1432\n===> Epoch[216](50/88): Loss_D: 0.2553 Loss_G: 1.2105\n===> Epoch[216](51/88): Loss_D: 0.2456 Loss_G: 1.2581\n===> Epoch[216](52/88): Loss_D: 0.2530 Loss_G: 1.1712\n===> Epoch[216](53/88): Loss_D: 0.2636 Loss_G: 1.0780\n===> Epoch[216](54/88): Loss_D: 0.2425 Loss_G: 1.1996\n===> Epoch[216](55/88): Loss_D: 0.2392 Loss_G: 1.2688\n===> Epoch[216](56/88): Loss_D: 0.1924 Loss_G: 1.6667\n===> Epoch[216](57/88): Loss_D: 0.2444 Loss_G: 1.1866\n===> Epoch[216](58/88): Loss_D: 0.2551 Loss_G: 1.2190\n===> Epoch[216](59/88): Loss_D: 0.2436 Loss_G: 1.2878\n===> Epoch[216](60/88): Loss_D: 0.2472 Loss_G: 1.1620\n===> Epoch[216](61/88): Loss_D: 0.2582 Loss_G: 1.0276\n===> Epoch[216](62/88): Loss_D: 0.2508 Loss_G: 1.1874\n===> Epoch[216](63/88): Loss_D: 0.2251 Loss_G: 1.1262\n===> Epoch[216](64/88): Loss_D: 0.2575 Loss_G: 1.1069\n===> Epoch[216](65/88): Loss_D: 0.2449 Loss_G: 1.1327\n===> Epoch[216](66/88): Loss_D: 0.2508 Loss_G: 1.0812\n===> Epoch[216](67/88): Loss_D: 0.2440 Loss_G: 1.4007\n===> Epoch[216](68/88): Loss_D: 0.2515 Loss_G: 1.1955\n===> Epoch[216](69/88): Loss_D: 0.2361 Loss_G: 1.2258\n===> Epoch[216](70/88): Loss_D: 0.2554 Loss_G: 1.2963\n===> Epoch[216](71/88): Loss_D: 0.2433 Loss_G: 1.3078\n===> Epoch[216](72/88): Loss_D: 0.2535 Loss_G: 1.0732\n===> Epoch[216](73/88): Loss_D: 0.2308 Loss_G: 1.3835\n===> Epoch[216](74/88): Loss_D: 0.2351 Loss_G: 1.2282\n===> Epoch[216](75/88): Loss_D: 0.2280 Loss_G: 1.5137\n===> Epoch[216](76/88): Loss_D: 0.2532 Loss_G: 1.1951\n===> Epoch[216](77/88): Loss_D: 0.2515 Loss_G: 1.2362\n===> Epoch[216](78/88): Loss_D: 0.2439 Loss_G: 1.2523\n===> Epoch[216](79/88): Loss_D: 0.2488 Loss_G: 1.1745\n===> Epoch[216](80/88): Loss_D: 0.2305 Loss_G: 1.2031\n===> Epoch[216](81/88): Loss_D: 0.2498 Loss_G: 1.1602\n===> Epoch[216](82/88): Loss_D: 0.2467 Loss_G: 1.0911\n===> Epoch[216](83/88): Loss_D: 0.2318 Loss_G: 1.3730\n===> Epoch[216](84/88): Loss_D: 0.2312 Loss_G: 1.3570\n===> Epoch[216](85/88): Loss_D: 0.2011 Loss_G: 2.3305\n===> Epoch[216](86/88): Loss_D: 0.2415 Loss_G: 1.2557\n===> Epoch[216](87/88): Loss_D: 0.2468 Loss_G: 1.1866\n===> Epoch[216](88/88): Loss_D: 0.2524 Loss_G: 0.9706\nlearning rate = 0.0000461\nlearning rate = 0.0000461\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0033} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[217](1/88): Loss_D: 0.2659 Loss_G: 1.0871\n===> Epoch[217](2/88): Loss_D: 0.2371 Loss_G: 1.4392\n===> Epoch[217](3/88): Loss_D: 0.2554 Loss_G: 1.1211\n===> Epoch[217](4/88): Loss_D: 0.2390 Loss_G: 1.0880\n===> Epoch[217](5/88): Loss_D: 0.2406 Loss_G: 1.3805\n===> Epoch[217](6/88): Loss_D: 0.2658 Loss_G: 1.0941\n===> Epoch[217](7/88): Loss_D: 0.1629 Loss_G: 1.5240\n===> Epoch[217](8/88): Loss_D: 0.2543 Loss_G: 1.0648\n===> Epoch[217](9/88): Loss_D: 0.2629 Loss_G: 1.2439\n===> Epoch[217](10/88): Loss_D: 0.2661 Loss_G: 1.2776\n===> Epoch[217](11/88): Loss_D: 0.2526 Loss_G: 1.0428\n===> Epoch[217](12/88): Loss_D: 0.2474 Loss_G: 1.1812\n===> Epoch[217](13/88): Loss_D: 0.2464 Loss_G: 1.2295\n===> Epoch[217](14/88): Loss_D: 0.2502 Loss_G: 1.1154\n===> Epoch[217](15/88): Loss_D: 0.1852 Loss_G: 2.4121\n===> Epoch[217](16/88): Loss_D: 0.2455 Loss_G: 1.2326\n===> Epoch[217](17/88): Loss_D: 0.1740 Loss_G: 1.5427\n===> Epoch[217](18/88): Loss_D: 0.2431 Loss_G: 1.2297\n===> Epoch[217](19/88): Loss_D: 0.2460 Loss_G: 1.5200\n===> Epoch[217](20/88): Loss_D: 0.2604 Loss_G: 1.3104\n===> Epoch[217](21/88): Loss_D: 0.2630 Loss_G: 1.0548\n===> Epoch[217](22/88): Loss_D: 0.2353 Loss_G: 1.2405\n===> Epoch[217](23/88): Loss_D: 0.2210 Loss_G: 1.4252\n===> Epoch[217](24/88): Loss_D: 0.2333 Loss_G: 1.3335\n===> Epoch[217](25/88): Loss_D: 0.2464 Loss_G: 1.3504\n===> Epoch[217](26/88): Loss_D: 0.2466 Loss_G: 1.1015\n===> Epoch[217](27/88): Loss_D: 0.2456 Loss_G: 1.2463\n===> Epoch[217](28/88): Loss_D: 0.2532 Loss_G: 1.2671\n===> Epoch[217](29/88): Loss_D: 0.2556 Loss_G: 1.2641\n===> Epoch[217](30/88): Loss_D: 0.2526 Loss_G: 1.2497\n===> Epoch[217](31/88): Loss_D: 0.2493 Loss_G: 1.2095\n===> Epoch[217](32/88): Loss_D: 0.2307 Loss_G: 1.4010\n===> Epoch[217](33/88): Loss_D: 0.2464 Loss_G: 1.1573\n===> Epoch[217](34/88): Loss_D: 0.2488 Loss_G: 1.1452\n===> Epoch[217](35/88): Loss_D: 0.2452 Loss_G: 1.3062\n===> Epoch[217](36/88): Loss_D: 0.2195 Loss_G: 1.3962\n===> Epoch[217](37/88): Loss_D: 0.2286 Loss_G: 1.2736\n===> Epoch[217](38/88): Loss_D: 0.2336 Loss_G: 1.1789\n===> Epoch[217](39/88): Loss_D: 0.2398 Loss_G: 1.4210\n===> Epoch[217](40/88): Loss_D: 0.2362 Loss_G: 1.1779\n===> Epoch[217](41/88): Loss_D: 0.2407 Loss_G: 1.2343\n===> Epoch[217](42/88): Loss_D: 0.2524 Loss_G: 1.1141\n===> Epoch[217](43/88): Loss_D: 0.2523 Loss_G: 1.2245\n===> Epoch[217](44/88): Loss_D: 0.2601 Loss_G: 1.2529\n===> Epoch[217](45/88): Loss_D: 0.2425 Loss_G: 1.3223\n===> Epoch[217](46/88): Loss_D: 0.1786 Loss_G: 1.6630\n===> Epoch[217](47/88): Loss_D: 0.2488 Loss_G: 1.3372\n===> Epoch[217](48/88): Loss_D: 0.2308 Loss_G: 1.2663\n===> Epoch[217](49/88): Loss_D: 0.2373 Loss_G: 1.4326\n===> Epoch[217](50/88): Loss_D: 0.2319 Loss_G: 1.1092\n===> Epoch[217](51/88): Loss_D: 0.2531 Loss_G: 1.1968\n===> Epoch[217](52/88): Loss_D: 0.2600 Loss_G: 1.2541\n===> Epoch[217](53/88): Loss_D: 0.2490 Loss_G: 1.1980\n===> Epoch[217](54/88): Loss_D: 0.2661 Loss_G: 1.1562\n===> Epoch[217](55/88): Loss_D: 0.2432 Loss_G: 1.2006\n===> Epoch[217](56/88): Loss_D: 0.2465 Loss_G: 1.2341\n===> Epoch[217](57/88): Loss_D: 0.2381 Loss_G: 1.2200\n===> Epoch[217](58/88): Loss_D: 0.2445 Loss_G: 1.2372\n===> Epoch[217](59/88): Loss_D: 0.2385 Loss_G: 1.2737\n===> Epoch[217](60/88): Loss_D: 0.2443 Loss_G: 1.2229\n===> Epoch[217](61/88): Loss_D: 0.2301 Loss_G: 1.3169\n===> Epoch[217](62/88): Loss_D: 0.2507 Loss_G: 1.2167\n===> Epoch[217](63/88): Loss_D: 0.2313 Loss_G: 1.3083\n===> Epoch[217](64/88): Loss_D: 0.2480 Loss_G: 1.1502\n===> Epoch[217](65/88): Loss_D: 0.2326 Loss_G: 1.4307\n===> Epoch[217](66/88): Loss_D: 0.2512 Loss_G: 1.2099\n===> Epoch[217](67/88): Loss_D: 0.2452 Loss_G: 1.2223\n===> Epoch[217](68/88): Loss_D: 0.1964 Loss_G: 1.7983\n===> Epoch[217](69/88): Loss_D: 0.2524 Loss_G: 1.1988\n===> Epoch[217](70/88): Loss_D: 0.2465 Loss_G: 1.2349\n===> Epoch[217](71/88): Loss_D: 0.2659 Loss_G: 1.1155\n===> Epoch[217](72/88): Loss_D: 0.2198 Loss_G: 1.4426\n===> Epoch[217](73/88): Loss_D: 0.2549 Loss_G: 1.1108\n===> Epoch[217](74/88): Loss_D: 0.2495 Loss_G: 1.2167\n===> Epoch[217](75/88): Loss_D: 0.2483 Loss_G: 1.0168\n===> Epoch[217](76/88): Loss_D: 0.2348 Loss_G: 1.4496\n===> Epoch[217](77/88): Loss_D: 0.2350 Loss_G: 1.3955\n===> Epoch[217](78/88): Loss_D: 0.1928 Loss_G: 1.5881\n===> Epoch[217](79/88): Loss_D: 0.2334 Loss_G: 1.3037\n===> Epoch[217](80/88): Loss_D: 0.2571 Loss_G: 0.9130\n===> Epoch[217](81/88): Loss_D: 0.2542 Loss_G: 1.2517\n===> Epoch[217](82/88): Loss_D: 0.2473 Loss_G: 1.1043\n===> Epoch[217](83/88): Loss_D: 0.2564 Loss_G: 1.0555\n===> Epoch[217](84/88): Loss_D: 0.2547 Loss_G: 1.1950\n===> Epoch[217](85/88): Loss_D: 0.2572 Loss_G: 1.1200\n===> Epoch[217](86/88): Loss_D: 0.2612 Loss_G: 1.2370\n===> Epoch[217](87/88): Loss_D: 0.2440 Loss_G: 1.2513\n===> Epoch[217](88/88): Loss_D: 0.2537 Loss_G: 1.0866\nlearning rate = 0.0000459\nlearning rate = 0.0000459\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0050} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[218](1/88): Loss_D: 0.2486 Loss_G: 1.2232\n===> Epoch[218](2/88): Loss_D: 0.2536 Loss_G: 1.1564\n===> Epoch[218](3/88): Loss_D: 0.2525 Loss_G: 1.1202\n===> Epoch[218](4/88): Loss_D: 0.2484 Loss_G: 1.1743\n===> Epoch[218](5/88): Loss_D: 0.2448 Loss_G: 1.1535\n===> Epoch[218](6/88): Loss_D: 0.2321 Loss_G: 1.4717\n===> Epoch[218](7/88): Loss_D: 0.2552 Loss_G: 1.2262\n===> Epoch[218](8/88): Loss_D: 0.2403 Loss_G: 1.2291\n===> Epoch[218](9/88): Loss_D: 0.2525 Loss_G: 1.1125\n===> Epoch[218](10/88): Loss_D: 0.2435 Loss_G: 1.3230\n===> Epoch[218](11/88): Loss_D: 0.2496 Loss_G: 1.2401\n===> Epoch[218](12/88): Loss_D: 0.1846 Loss_G: 1.6178\n===> Epoch[218](13/88): Loss_D: 0.2491 Loss_G: 1.2180\n===> Epoch[218](14/88): Loss_D: 0.2387 Loss_G: 1.3524\n===> Epoch[218](15/88): Loss_D: 0.2534 Loss_G: 1.1911\n===> Epoch[218](16/88): Loss_D: 0.2238 Loss_G: 1.3569\n===> Epoch[218](17/88): Loss_D: 0.2359 Loss_G: 1.1885\n===> Epoch[218](18/88): Loss_D: 0.2177 Loss_G: 1.4590\n===> Epoch[218](19/88): Loss_D: 0.2536 Loss_G: 1.0328\n===> Epoch[218](20/88): Loss_D: 0.2556 Loss_G: 1.0853\n===> Epoch[218](21/88): Loss_D: 0.2563 Loss_G: 1.2452\n===> Epoch[218](22/88): Loss_D: 0.2294 Loss_G: 1.1310\n===> Epoch[218](23/88): Loss_D: 0.2334 Loss_G: 1.3542\n===> Epoch[218](24/88): Loss_D: 0.2528 Loss_G: 1.1656\n===> Epoch[218](25/88): Loss_D: 0.2354 Loss_G: 1.4324\n===> Epoch[218](26/88): Loss_D: 0.2214 Loss_G: 1.2889\n===> Epoch[218](27/88): Loss_D: 0.2489 Loss_G: 1.2267\n===> Epoch[218](28/88): Loss_D: 0.2322 Loss_G: 1.4024\n===> Epoch[218](29/88): Loss_D: 0.2502 Loss_G: 1.1158\n===> Epoch[218](30/88): Loss_D: 0.2506 Loss_G: 1.1793\n===> Epoch[218](31/88): Loss_D: 0.2288 Loss_G: 1.4480\n===> Epoch[218](32/88): Loss_D: 0.2480 Loss_G: 1.2164\n===> Epoch[218](33/88): Loss_D: 0.2445 Loss_G: 1.2001\n===> Epoch[218](34/88): Loss_D: 0.2289 Loss_G: 1.1921\n===> Epoch[218](35/88): Loss_D: 0.2416 Loss_G: 1.0612\n===> Epoch[218](36/88): Loss_D: 0.2408 Loss_G: 1.2336\n===> Epoch[218](37/88): Loss_D: 0.2613 Loss_G: 1.0649\n===> Epoch[218](38/88): Loss_D: 0.2380 Loss_G: 1.3166\n===> Epoch[218](39/88): Loss_D: 0.2366 Loss_G: 1.2250\n===> Epoch[218](40/88): Loss_D: 0.2583 Loss_G: 1.2070\n===> Epoch[218](41/88): Loss_D: 0.2386 Loss_G: 1.1723\n===> Epoch[218](42/88): Loss_D: 0.2488 Loss_G: 1.3135\n===> Epoch[218](43/88): Loss_D: 0.2308 Loss_G: 1.3345\n===> Epoch[218](44/88): Loss_D: 0.2268 Loss_G: 1.2069\n===> Epoch[218](45/88): Loss_D: 0.2490 Loss_G: 0.9913\n===> Epoch[218](46/88): Loss_D: 0.1772 Loss_G: 1.6814\n===> Epoch[218](47/88): Loss_D: 0.2366 Loss_G: 1.4391\n===> Epoch[218](48/88): Loss_D: 0.2352 Loss_G: 1.2676\n===> Epoch[218](49/88): Loss_D: 0.2538 Loss_G: 1.3154\n===> Epoch[218](50/88): Loss_D: 0.2651 Loss_G: 1.0714\n===> Epoch[218](51/88): Loss_D: 0.2572 Loss_G: 1.3143\n===> Epoch[218](52/88): Loss_D: 0.2446 Loss_G: 1.2147\n===> Epoch[218](53/88): Loss_D: 0.2362 Loss_G: 1.4141\n===> Epoch[218](54/88): Loss_D: 0.2365 Loss_G: 1.3982\n===> Epoch[218](55/88): Loss_D: 0.2551 Loss_G: 1.1975\n===> Epoch[218](56/88): Loss_D: 0.2147 Loss_G: 2.1950\n===> Epoch[218](57/88): Loss_D: 0.2504 Loss_G: 1.2521\n===> Epoch[218](58/88): Loss_D: 0.2458 Loss_G: 1.2240\n===> Epoch[218](59/88): Loss_D: 0.2420 Loss_G: 1.3726\n===> Epoch[218](60/88): Loss_D: 0.2692 Loss_G: 1.1119\n===> Epoch[218](61/88): Loss_D: 0.2544 Loss_G: 1.1961\n===> Epoch[218](62/88): Loss_D: 0.2468 Loss_G: 1.1599\n===> Epoch[218](63/88): Loss_D: 0.2579 Loss_G: 1.2363\n===> Epoch[218](64/88): Loss_D: 0.2467 Loss_G: 1.1795\n===> Epoch[218](65/88): Loss_D: 0.2473 Loss_G: 1.1377\n===> Epoch[218](66/88): Loss_D: 0.2459 Loss_G: 1.2211\n===> Epoch[218](67/88): Loss_D: 0.2352 Loss_G: 1.4652\n===> Epoch[218](68/88): Loss_D: 0.2271 Loss_G: 1.1107\n===> Epoch[218](69/88): Loss_D: 0.2442 Loss_G: 1.2454\n===> Epoch[218](70/88): Loss_D: 0.1741 Loss_G: 1.5525\n===> Epoch[218](71/88): Loss_D: 0.2286 Loss_G: 1.4041\n===> Epoch[218](72/88): Loss_D: 0.2543 Loss_G: 1.1071\n===> Epoch[218](73/88): Loss_D: 0.2333 Loss_G: 1.3609\n===> Epoch[218](74/88): Loss_D: 0.2453 Loss_G: 1.2456\n===> Epoch[218](75/88): Loss_D: 0.2335 Loss_G: 1.3936\n===> Epoch[218](76/88): Loss_D: 0.2489 Loss_G: 1.2218\n===> Epoch[218](77/88): Loss_D: 0.2433 Loss_G: 1.1931\n===> Epoch[218](78/88): Loss_D: 0.2546 Loss_G: 1.1841\n===> Epoch[218](79/88): Loss_D: 0.2277 Loss_G: 1.3250\n===> Epoch[218](80/88): Loss_D: 0.2590 Loss_G: 1.0851\n===> Epoch[218](81/88): Loss_D: 0.2548 Loss_G: 1.0928\n===> Epoch[218](82/88): Loss_D: 0.1724 Loss_G: 1.9977\n===> Epoch[218](83/88): Loss_D: 0.1981 Loss_G: 1.6175\n===> Epoch[218](84/88): Loss_D: 0.2404 Loss_G: 1.0859\n===> Epoch[218](85/88): Loss_D: 0.2461 Loss_G: 1.1884\n===> Epoch[218](86/88): Loss_D: 0.2492 Loss_G: 1.1571\n===> Epoch[218](87/88): Loss_D: 0.2512 Loss_G: 1.3749\n===> Epoch[218](88/88): Loss_D: 0.2153 Loss_G: 1.4142\nlearning rate = 0.0000456\nlearning rate = 0.0000456\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0002} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[219](1/88): Loss_D: 0.2626 Loss_G: 1.2682\n===> Epoch[219](2/88): Loss_D: 0.2605 Loss_G: 1.1926\n===> Epoch[219](3/88): Loss_D: 0.2407 Loss_G: 1.2885\n===> Epoch[219](4/88): Loss_D: 0.2504 Loss_G: 1.1584\n===> Epoch[219](5/88): Loss_D: 0.2567 Loss_G: 1.2208\n===> Epoch[219](6/88): Loss_D: 0.2540 Loss_G: 1.1620\n===> Epoch[219](7/88): Loss_D: 0.2523 Loss_G: 1.0597\n===> Epoch[219](8/88): Loss_D: 0.2318 Loss_G: 1.4711\n===> Epoch[219](9/88): Loss_D: 0.1815 Loss_G: 1.6208\n===> Epoch[219](10/88): Loss_D: 0.2063 Loss_G: 1.7653\n===> Epoch[219](11/88): Loss_D: 0.2437 Loss_G: 1.1492\n===> Epoch[219](12/88): Loss_D: 0.2340 Loss_G: 1.3167\n===> Epoch[219](13/88): Loss_D: 0.2359 Loss_G: 1.3578\n===> Epoch[219](14/88): Loss_D: 0.2495 Loss_G: 1.2267\n===> Epoch[219](15/88): Loss_D: 0.2112 Loss_G: 1.4795\n===> Epoch[219](16/88): Loss_D: 0.2516 Loss_G: 1.2438\n===> Epoch[219](17/88): Loss_D: 0.2546 Loss_G: 1.2414\n===> Epoch[219](18/88): Loss_D: 0.2498 Loss_G: 1.2184\n===> Epoch[219](19/88): Loss_D: 0.2536 Loss_G: 1.1372\n===> Epoch[219](20/88): Loss_D: 0.2534 Loss_G: 1.0209\n===> Epoch[219](21/88): Loss_D: 0.2144 Loss_G: 1.3096\n===> Epoch[219](22/88): Loss_D: 0.2473 Loss_G: 1.2508\n===> Epoch[219](23/88): Loss_D: 0.2193 Loss_G: 2.1555\n===> Epoch[219](24/88): Loss_D: 0.2447 Loss_G: 1.3057\n===> Epoch[219](25/88): Loss_D: 0.2322 Loss_G: 1.3805\n===> Epoch[219](26/88): Loss_D: 0.2372 Loss_G: 1.4320\n===> Epoch[219](27/88): Loss_D: 0.2464 Loss_G: 1.2320\n===> Epoch[219](28/88): Loss_D: 0.2283 Loss_G: 1.0746\n===> Epoch[219](29/88): Loss_D: 0.2513 Loss_G: 1.1813\n===> Epoch[219](30/88): Loss_D: 0.2449 Loss_G: 1.1398\n===> Epoch[219](31/88): Loss_D: 0.2565 Loss_G: 1.1239\n===> Epoch[219](32/88): Loss_D: 0.2509 Loss_G: 1.1521\n===> Epoch[219](33/88): Loss_D: 0.2555 Loss_G: 1.2322\n===> Epoch[219](34/88): Loss_D: 0.2438 Loss_G: 1.2172\n===> Epoch[219](35/88): Loss_D: 0.1708 Loss_G: 1.5887\n===> Epoch[219](36/88): Loss_D: 0.2623 Loss_G: 1.1119\n===> Epoch[219](37/88): Loss_D: 0.2542 Loss_G: 1.2330\n===> Epoch[219](38/88): Loss_D: 0.2188 Loss_G: 1.4949\n===> Epoch[219](39/88): Loss_D: 0.2568 Loss_G: 1.1208\n===> Epoch[219](40/88): Loss_D: 0.2510 Loss_G: 1.4221\n===> Epoch[219](41/88): Loss_D: 0.2568 Loss_G: 1.3347\n===> Epoch[219](42/88): Loss_D: 0.2627 Loss_G: 1.2435\n===> Epoch[219](43/88): Loss_D: 0.2315 Loss_G: 1.1873\n===> Epoch[219](44/88): Loss_D: 0.2359 Loss_G: 1.3780\n===> Epoch[219](45/88): Loss_D: 0.2374 Loss_G: 1.3839\n===> Epoch[219](46/88): Loss_D: 0.2388 Loss_G: 1.2098\n===> Epoch[219](47/88): Loss_D: 0.2579 Loss_G: 1.2032\n===> Epoch[219](48/88): Loss_D: 0.2466 Loss_G: 1.2528\n===> Epoch[219](49/88): Loss_D: 0.2444 Loss_G: 1.0393\n===> Epoch[219](50/88): Loss_D: 0.2451 Loss_G: 1.2427\n===> Epoch[219](51/88): Loss_D: 0.2637 Loss_G: 0.9601\n===> Epoch[219](52/88): Loss_D: 0.2335 Loss_G: 1.5056\n===> Epoch[219](53/88): Loss_D: 0.2434 Loss_G: 1.1528\n===> Epoch[219](54/88): Loss_D: 0.2522 Loss_G: 1.2576\n===> Epoch[219](55/88): Loss_D: 0.2558 Loss_G: 1.1096\n===> Epoch[219](56/88): Loss_D: 0.2494 Loss_G: 1.2383\n===> Epoch[219](57/88): Loss_D: 0.2501 Loss_G: 1.1992\n===> Epoch[219](58/88): Loss_D: 0.2309 Loss_G: 1.2930\n===> Epoch[219](59/88): Loss_D: 0.2389 Loss_G: 1.0567\n===> Epoch[219](60/88): Loss_D: 0.2515 Loss_G: 1.3385\n===> Epoch[219](61/88): Loss_D: 0.2505 Loss_G: 1.2384\n===> Epoch[219](62/88): Loss_D: 0.2378 Loss_G: 1.3563\n===> Epoch[219](63/88): Loss_D: 0.2386 Loss_G: 1.1754\n===> Epoch[219](64/88): Loss_D: 0.2533 Loss_G: 1.1417\n===> Epoch[219](65/88): Loss_D: 0.2134 Loss_G: 1.4851\n===> Epoch[219](66/88): Loss_D: 0.2320 Loss_G: 1.3745\n===> Epoch[219](67/88): Loss_D: 0.2655 Loss_G: 1.2148\n===> Epoch[219](68/88): Loss_D: 0.2737 Loss_G: 1.1407\n===> Epoch[219](69/88): Loss_D: 0.2584 Loss_G: 1.2743\n===> Epoch[219](70/88): Loss_D: 0.2552 Loss_G: 1.1839\n===> Epoch[219](71/88): Loss_D: 0.2526 Loss_G: 1.2008\n===> Epoch[219](72/88): Loss_D: 0.2543 Loss_G: 1.0199\n===> Epoch[219](73/88): Loss_D: 0.2321 Loss_G: 1.3439\n===> Epoch[219](74/88): Loss_D: 0.2497 Loss_G: 1.2372\n===> Epoch[219](75/88): Loss_D: 0.2335 Loss_G: 1.3725\n===> Epoch[219](76/88): Loss_D: 0.2546 Loss_G: 1.2208\n===> Epoch[219](77/88): Loss_D: 0.2389 Loss_G: 1.3191\n===> Epoch[219](78/88): Loss_D: 0.1940 Loss_G: 1.5389\n===> Epoch[219](79/88): Loss_D: 0.2443 Loss_G: 1.0322\n===> Epoch[219](80/88): Loss_D: 0.2298 Loss_G: 1.3068\n===> Epoch[219](81/88): Loss_D: 0.1862 Loss_G: 1.6662\n===> Epoch[219](82/88): Loss_D: 0.2413 Loss_G: 1.2714\n===> Epoch[219](83/88): Loss_D: 0.2502 Loss_G: 1.0921\n===> Epoch[219](84/88): Loss_D: 0.2369 Loss_G: 1.3725\n===> Epoch[219](85/88): Loss_D: 0.2459 Loss_G: 1.2127\n===> Epoch[219](86/88): Loss_D: 0.2517 Loss_G: 1.2755\n===> Epoch[219](87/88): Loss_D: 0.2244 Loss_G: 1.4668\n===> Epoch[219](88/88): Loss_D: 0.2400 Loss_G: 1.3370\nlearning rate = 0.0000454\nlearning rate = 0.0000454\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0005} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[220](1/88): Loss_D: 0.2465 Loss_G: 1.2444\n===> Epoch[220](2/88): Loss_D: 0.2458 Loss_G: 1.1293\n===> Epoch[220](3/88): Loss_D: 0.2222 Loss_G: 1.4554\n===> Epoch[220](4/88): Loss_D: 0.2464 Loss_G: 1.1733\n===> Epoch[220](5/88): Loss_D: 0.2529 Loss_G: 1.1940\n===> Epoch[220](6/88): Loss_D: 0.2397 Loss_G: 1.4296\n===> Epoch[220](7/88): Loss_D: 0.2257 Loss_G: 1.4378\n===> Epoch[220](8/88): Loss_D: 0.2344 Loss_G: 1.2851\n===> Epoch[220](9/88): Loss_D: 0.2437 Loss_G: 1.1295\n===> Epoch[220](10/88): Loss_D: 0.2570 Loss_G: 1.2208\n===> Epoch[220](11/88): Loss_D: 0.2454 Loss_G: 1.2376\n===> Epoch[220](12/88): Loss_D: 0.2565 Loss_G: 1.2243\n===> Epoch[220](13/88): Loss_D: 0.1839 Loss_G: 1.8493\n===> Epoch[220](14/88): Loss_D: 0.2597 Loss_G: 1.1037\n===> Epoch[220](15/88): Loss_D: 0.2307 Loss_G: 1.4784\n===> Epoch[220](16/88): Loss_D: 0.2482 Loss_G: 1.3030\n===> Epoch[220](17/88): Loss_D: 0.2204 Loss_G: 1.3815\n===> Epoch[220](18/88): Loss_D: 0.2333 Loss_G: 1.3668\n===> Epoch[220](19/88): Loss_D: 0.2583 Loss_G: 1.0265\n===> Epoch[220](20/88): Loss_D: 0.2589 Loss_G: 1.2277\n===> Epoch[220](21/88): Loss_D: 0.2584 Loss_G: 1.1731\n===> Epoch[220](22/88): Loss_D: 0.2537 Loss_G: 1.2424\n===> Epoch[220](23/88): Loss_D: 0.2335 Loss_G: 1.1477\n===> Epoch[220](24/88): Loss_D: 0.2572 Loss_G: 1.2526\n===> Epoch[220](25/88): Loss_D: 0.2555 Loss_G: 1.1785\n===> Epoch[220](26/88): Loss_D: 0.2336 Loss_G: 1.2377\n===> Epoch[220](27/88): Loss_D: 0.2518 Loss_G: 1.1766\n===> Epoch[220](28/88): Loss_D: 0.2539 Loss_G: 1.2064\n===> Epoch[220](29/88): Loss_D: 0.1792 Loss_G: 1.5692\n===> Epoch[220](30/88): Loss_D: 0.2565 Loss_G: 1.1971\n===> Epoch[220](31/88): Loss_D: 0.2467 Loss_G: 1.1964\n===> Epoch[220](32/88): Loss_D: 0.2625 Loss_G: 1.0794\n===> Epoch[220](33/88): Loss_D: 0.2548 Loss_G: 0.9795\n===> Epoch[220](34/88): Loss_D: 0.2417 Loss_G: 1.2503\n===> Epoch[220](35/88): Loss_D: 0.2151 Loss_G: 1.1680\n===> Epoch[220](36/88): Loss_D: 0.2449 Loss_G: 1.2221\n===> Epoch[220](37/88): Loss_D: 0.2461 Loss_G: 1.2433\n===> Epoch[220](38/88): Loss_D: 0.2400 Loss_G: 1.2297\n===> Epoch[220](39/88): Loss_D: 0.2419 Loss_G: 1.1390\n===> Epoch[220](40/88): Loss_D: 0.2045 Loss_G: 1.2587\n===> Epoch[220](41/88): Loss_D: 0.2601 Loss_G: 1.0811\n===> Epoch[220](42/88): Loss_D: 0.1746 Loss_G: 1.5883\n===> Epoch[220](43/88): Loss_D: 0.2307 Loss_G: 1.1797\n===> Epoch[220](44/88): Loss_D: 0.2472 Loss_G: 1.4016\n===> Epoch[220](45/88): Loss_D: 0.2622 Loss_G: 1.2415\n===> Epoch[220](46/88): Loss_D: 0.2444 Loss_G: 1.1333\n===> Epoch[220](47/88): Loss_D: 0.2466 Loss_G: 1.3197\n===> Epoch[220](48/88): Loss_D: 0.2394 Loss_G: 1.2368\n===> Epoch[220](49/88): Loss_D: 0.2328 Loss_G: 1.4813\n===> Epoch[220](50/88): Loss_D: 0.2457 Loss_G: 1.3970\n===> Epoch[220](51/88): Loss_D: 0.2010 Loss_G: 1.3865\n===> Epoch[220](52/88): Loss_D: 0.2463 Loss_G: 1.1261\n===> Epoch[220](53/88): Loss_D: 0.2122 Loss_G: 1.4981\n===> Epoch[220](54/88): Loss_D: 0.2347 Loss_G: 1.1879\n===> Epoch[220](55/88): Loss_D: 0.2482 Loss_G: 1.2598\n===> Epoch[220](56/88): Loss_D: 0.2517 Loss_G: 1.1447\n===> Epoch[220](57/88): Loss_D: 0.2168 Loss_G: 2.3869\n===> Epoch[220](58/88): Loss_D: 0.2594 Loss_G: 1.3932\n===> Epoch[220](59/88): Loss_D: 0.2336 Loss_G: 1.2695\n===> Epoch[220](60/88): Loss_D: 0.2439 Loss_G: 1.2702\n===> Epoch[220](61/88): Loss_D: 0.1729 Loss_G: 1.6302\n===> Epoch[220](62/88): Loss_D: 0.2342 Loss_G: 1.4437\n===> Epoch[220](63/88): Loss_D: 0.2756 Loss_G: 1.3178\n===> Epoch[220](64/88): Loss_D: 0.2370 Loss_G: 1.4826\n===> Epoch[220](65/88): Loss_D: 0.2488 Loss_G: 1.2971\n===> Epoch[220](66/88): Loss_D: 0.2483 Loss_G: 1.4051\n===> Epoch[220](67/88): Loss_D: 0.2484 Loss_G: 1.2306\n===> Epoch[220](68/88): Loss_D: 0.2503 Loss_G: 1.0620\n===> Epoch[220](69/88): Loss_D: 0.2610 Loss_G: 1.0655\n===> Epoch[220](70/88): Loss_D: 0.2260 Loss_G: 1.4044\n===> Epoch[220](71/88): Loss_D: 0.2379 Loss_G: 1.2600\n===> Epoch[220](72/88): Loss_D: 0.2560 Loss_G: 1.0903\n===> Epoch[220](73/88): Loss_D: 0.2387 Loss_G: 1.3828\n===> Epoch[220](74/88): Loss_D: 0.2600 Loss_G: 1.2414\n===> Epoch[220](75/88): Loss_D: 0.2571 Loss_G: 1.1833\n===> Epoch[220](76/88): Loss_D: 0.2529 Loss_G: 1.0187\n===> Epoch[220](77/88): Loss_D: 0.2357 Loss_G: 1.3018\n===> Epoch[220](78/88): Loss_D: 0.2527 Loss_G: 0.9242\n===> Epoch[220](79/88): Loss_D: 0.2296 Loss_G: 1.1879\n===> Epoch[220](80/88): Loss_D: 0.2314 Loss_G: 1.3350\n===> Epoch[220](81/88): Loss_D: 0.2527 Loss_G: 1.1845\n===> Epoch[220](82/88): Loss_D: 0.2539 Loss_G: 1.2023\n===> Epoch[220](83/88): Loss_D: 0.2525 Loss_G: 1.2562\n===> Epoch[220](84/88): Loss_D: 0.2501 Loss_G: 1.2706\n===> Epoch[220](85/88): Loss_D: 0.2512 Loss_G: 1.1039\n===> Epoch[220](86/88): Loss_D: 0.2404 Loss_G: 1.2290\n===> Epoch[220](87/88): Loss_D: 0.2451 Loss_G: 1.1080\n===> Epoch[220](88/88): Loss_D: 0.2234 Loss_G: 1.4066\nlearning rate = 0.0000451\nlearning rate = 0.0000451\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0001} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[221](1/88): Loss_D: 0.2519 Loss_G: 0.9800\n===> Epoch[221](2/88): Loss_D: 0.2480 Loss_G: 1.2595\n===> Epoch[221](3/88): Loss_D: 0.2499 Loss_G: 1.1068\n===> Epoch[221](4/88): Loss_D: 0.2482 Loss_G: 1.1784\n===> Epoch[221](5/88): Loss_D: 0.2298 Loss_G: 1.4289\n===> Epoch[221](6/88): Loss_D: 0.2269 Loss_G: 1.5777\n===> Epoch[221](7/88): Loss_D: 0.2461 Loss_G: 1.2377\n===> Epoch[221](8/88): Loss_D: 0.2497 Loss_G: 1.1832\n===> Epoch[221](9/88): Loss_D: 0.2277 Loss_G: 1.2321\n===> Epoch[221](10/88): Loss_D: 0.2376 Loss_G: 1.2657\n===> Epoch[221](11/88): Loss_D: 0.2350 Loss_G: 1.3890\n===> Epoch[221](12/88): Loss_D: 0.2515 Loss_G: 1.1597\n===> Epoch[221](13/88): Loss_D: 0.2452 Loss_G: 1.0777\n===> Epoch[221](14/88): Loss_D: 0.2335 Loss_G: 1.4045\n===> Epoch[221](15/88): Loss_D: 0.2442 Loss_G: 1.0475\n===> Epoch[221](16/88): Loss_D: 0.2445 Loss_G: 1.3383\n===> Epoch[221](17/88): Loss_D: 0.2433 Loss_G: 1.1713\n===> Epoch[221](18/88): Loss_D: 0.2605 Loss_G: 1.1320\n===> Epoch[221](19/88): Loss_D: 0.2514 Loss_G: 1.0990\n===> Epoch[221](20/88): Loss_D: 0.2401 Loss_G: 1.2399\n===> Epoch[221](21/88): Loss_D: 0.2407 Loss_G: 1.0738\n===> Epoch[221](22/88): Loss_D: 0.2226 Loss_G: 1.3799\n===> Epoch[221](23/88): Loss_D: 0.2572 Loss_G: 1.0512\n===> Epoch[221](24/88): Loss_D: 0.2504 Loss_G: 1.2033\n===> Epoch[221](25/88): Loss_D: 0.2042 Loss_G: 1.7786\n===> Epoch[221](26/88): Loss_D: 0.2468 Loss_G: 1.1965\n===> Epoch[221](27/88): Loss_D: 0.1847 Loss_G: 1.5856\n===> Epoch[221](28/88): Loss_D: 0.2666 Loss_G: 1.3088\n===> Epoch[221](29/88): Loss_D: 0.2711 Loss_G: 1.2629\n===> Epoch[221](30/88): Loss_D: 0.2602 Loss_G: 1.0279\n===> Epoch[221](31/88): Loss_D: 0.2280 Loss_G: 1.3407\n===> Epoch[221](32/88): Loss_D: 0.2453 Loss_G: 1.2432\n===> Epoch[221](33/88): Loss_D: 0.2000 Loss_G: 1.4331\n===> Epoch[221](34/88): Loss_D: 0.2565 Loss_G: 1.1995\n===> Epoch[221](35/88): Loss_D: 0.2537 Loss_G: 1.2026\n===> Epoch[221](36/88): Loss_D: 0.2321 Loss_G: 1.2563\n===> Epoch[221](37/88): Loss_D: 0.2437 Loss_G: 1.1577\n===> Epoch[221](38/88): Loss_D: 0.2629 Loss_G: 1.2268\n===> Epoch[221](39/88): Loss_D: 0.2408 Loss_G: 1.2794\n===> Epoch[221](40/88): Loss_D: 0.2339 Loss_G: 1.0655\n===> Epoch[221](41/88): Loss_D: 0.2510 Loss_G: 1.1839\n===> Epoch[221](42/88): Loss_D: 0.2560 Loss_G: 1.0814\n===> Epoch[221](43/88): Loss_D: 0.2364 Loss_G: 1.3324\n===> Epoch[221](44/88): Loss_D: 0.2401 Loss_G: 1.1647\n===> Epoch[221](45/88): Loss_D: 0.2520 Loss_G: 1.0784\n===> Epoch[221](46/88): Loss_D: 0.2342 Loss_G: 1.1725\n===> Epoch[221](47/88): Loss_D: 0.2433 Loss_G: 1.1582\n===> Epoch[221](48/88): Loss_D: 0.2314 Loss_G: 1.2705\n===> Epoch[221](49/88): Loss_D: 0.2407 Loss_G: 1.1736\n===> Epoch[221](50/88): Loss_D: 0.2297 Loss_G: 1.2986\n===> Epoch[221](51/88): Loss_D: 0.2464 Loss_G: 1.3023\n===> Epoch[221](52/88): Loss_D: 0.2311 Loss_G: 1.3052\n===> Epoch[221](53/88): Loss_D: 0.2532 Loss_G: 1.1960\n===> Epoch[221](54/88): Loss_D: 0.2234 Loss_G: 1.4495\n===> Epoch[221](55/88): Loss_D: 0.2564 Loss_G: 1.1972\n===> Epoch[221](56/88): Loss_D: 0.2539 Loss_G: 1.1963\n===> Epoch[221](57/88): Loss_D: 0.2477 Loss_G: 1.2962\n===> Epoch[221](58/88): Loss_D: 0.2563 Loss_G: 1.2481\n===> Epoch[221](59/88): Loss_D: 0.2434 Loss_G: 1.1093\n===> Epoch[221](60/88): Loss_D: 0.2465 Loss_G: 1.2030\n===> Epoch[221](61/88): Loss_D: 0.2506 Loss_G: 1.1932\n===> Epoch[221](62/88): Loss_D: 0.2662 Loss_G: 0.9971\n===> Epoch[221](63/88): Loss_D: 0.1877 Loss_G: 1.6946\n===> Epoch[221](64/88): Loss_D: 0.2538 Loss_G: 1.2098\n===> Epoch[221](65/88): Loss_D: 0.2439 Loss_G: 1.1822\n===> Epoch[221](66/88): Loss_D: 0.2507 Loss_G: 1.1652\n===> Epoch[221](67/88): Loss_D: 0.2318 Loss_G: 1.0847\n===> Epoch[221](68/88): Loss_D: 0.2212 Loss_G: 1.3699\n===> Epoch[221](69/88): Loss_D: 0.2501 Loss_G: 1.2049\n===> Epoch[221](70/88): Loss_D: 0.2455 Loss_G: 1.2202\n===> Epoch[221](71/88): Loss_D: 0.2638 Loss_G: 0.9288\n===> Epoch[221](72/88): Loss_D: 0.2226 Loss_G: 1.3659\n===> Epoch[221](73/88): Loss_D: 0.2456 Loss_G: 1.0850\n===> Epoch[221](74/88): Loss_D: 0.1786 Loss_G: 1.5169\n===> Epoch[221](75/88): Loss_D: 0.2526 Loss_G: 1.0997\n===> Epoch[221](76/88): Loss_D: 0.2738 Loss_G: 1.1376\n===> Epoch[221](77/88): Loss_D: 0.2472 Loss_G: 1.2347\n===> Epoch[221](78/88): Loss_D: 0.2314 Loss_G: 1.3693\n===> Epoch[221](79/88): Loss_D: 0.2375 Loss_G: 1.3102\n===> Epoch[221](80/88): Loss_D: 0.2437 Loss_G: 1.3043\n===> Epoch[221](81/88): Loss_D: 0.2562 Loss_G: 1.0985\n===> Epoch[221](82/88): Loss_D: 0.2578 Loss_G: 1.2740\n===> Epoch[221](83/88): Loss_D: 0.2198 Loss_G: 2.2783\n===> Epoch[221](84/88): Loss_D: 0.1959 Loss_G: 1.5016\n===> Epoch[221](85/88): Loss_D: 0.2588 Loss_G: 1.2710\n===> Epoch[221](86/88): Loss_D: 0.2727 Loss_G: 1.3443\n===> Epoch[221](87/88): Loss_D: 0.2511 Loss_G: 1.5324\n===> Epoch[221](88/88): Loss_D: 0.2679 Loss_G: 1.1031\nlearning rate = 0.0000449\nlearning rate = 0.0000449\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0057} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[222](1/88): Loss_D: 0.2507 Loss_G: 1.2183\n===> Epoch[222](2/88): Loss_D: 0.2346 Loss_G: 1.3164\n===> Epoch[222](3/88): Loss_D: 0.2418 Loss_G: 1.2088\n===> Epoch[222](4/88): Loss_D: 0.2454 Loss_G: 1.3080\n===> Epoch[222](5/88): Loss_D: 0.2528 Loss_G: 1.1492\n===> Epoch[222](6/88): Loss_D: 0.2300 Loss_G: 1.2859\n===> Epoch[222](7/88): Loss_D: 0.2272 Loss_G: 1.3893\n===> Epoch[222](8/88): Loss_D: 0.2568 Loss_G: 1.1780\n===> Epoch[222](9/88): Loss_D: 0.2471 Loss_G: 0.9903\n===> Epoch[222](10/88): Loss_D: 0.2244 Loss_G: 1.3281\n===> Epoch[222](11/88): Loss_D: 0.1931 Loss_G: 1.6563\n===> Epoch[222](12/88): Loss_D: 0.2549 Loss_G: 1.2502\n===> Epoch[222](13/88): Loss_D: 0.2625 Loss_G: 1.0978\n===> Epoch[222](14/88): Loss_D: 0.2467 Loss_G: 1.1455\n===> Epoch[222](15/88): Loss_D: 0.2588 Loss_G: 1.0802\n===> Epoch[222](16/88): Loss_D: 0.2571 Loss_G: 1.1804\n===> Epoch[222](17/88): Loss_D: 0.2490 Loss_G: 1.1420\n===> Epoch[222](18/88): Loss_D: 0.2294 Loss_G: 1.2135\n===> Epoch[222](19/88): Loss_D: 0.2581 Loss_G: 0.9755\n===> Epoch[222](20/88): Loss_D: 0.2494 Loss_G: 1.1590\n===> Epoch[222](21/88): Loss_D: 0.2236 Loss_G: 1.1289\n===> Epoch[222](22/88): Loss_D: 0.2340 Loss_G: 1.0826\n===> Epoch[222](23/88): Loss_D: 0.2346 Loss_G: 1.3254\n===> Epoch[222](24/88): Loss_D: 0.2503 Loss_G: 1.2847\n===> Epoch[222](25/88): Loss_D: 0.2590 Loss_G: 1.1105\n===> Epoch[222](26/88): Loss_D: 0.2119 Loss_G: 1.3817\n===> Epoch[222](27/88): Loss_D: 0.2522 Loss_G: 1.2417\n===> Epoch[222](28/88): Loss_D: 0.2555 Loss_G: 1.1345\n===> Epoch[222](29/88): Loss_D: 0.2456 Loss_G: 1.3089\n===> Epoch[222](30/88): Loss_D: 0.2396 Loss_G: 1.2458\n===> Epoch[222](31/88): Loss_D: 0.2411 Loss_G: 1.0721\n===> Epoch[222](32/88): Loss_D: 0.1712 Loss_G: 1.5412\n===> Epoch[222](33/88): Loss_D: 0.2247 Loss_G: 1.2744\n===> Epoch[222](34/88): Loss_D: 0.2273 Loss_G: 1.3805\n===> Epoch[222](35/88): Loss_D: 0.2627 Loss_G: 1.2718\n===> Epoch[222](36/88): Loss_D: 0.1939 Loss_G: 1.8381\n===> Epoch[222](37/88): Loss_D: 0.2535 Loss_G: 1.2159\n===> Epoch[222](38/88): Loss_D: 0.2354 Loss_G: 1.3860\n===> Epoch[222](39/88): Loss_D: 0.2529 Loss_G: 1.1336\n===> Epoch[222](40/88): Loss_D: 0.2117 Loss_G: 2.1435\n===> Epoch[222](41/88): Loss_D: 0.2434 Loss_G: 1.1519\n===> Epoch[222](42/88): Loss_D: 0.2515 Loss_G: 1.3500\n===> Epoch[222](43/88): Loss_D: 0.2451 Loss_G: 1.3793\n===> Epoch[222](44/88): Loss_D: 0.2494 Loss_G: 1.2118\n===> Epoch[222](45/88): Loss_D: 0.2405 Loss_G: 1.4180\n===> Epoch[222](46/88): Loss_D: 0.2596 Loss_G: 1.0777\n===> Epoch[222](47/88): Loss_D: 0.2241 Loss_G: 1.3354\n===> Epoch[222](48/88): Loss_D: 0.2587 Loss_G: 1.0423\n===> Epoch[222](49/88): Loss_D: 0.2312 Loss_G: 1.2350\n===> Epoch[222](50/88): Loss_D: 0.2391 Loss_G: 1.3861\n===> Epoch[222](51/88): Loss_D: 0.2342 Loss_G: 1.4230\n===> Epoch[222](52/88): Loss_D: 0.2652 Loss_G: 1.3040\n===> Epoch[222](53/88): Loss_D: 0.2427 Loss_G: 1.4699\n===> Epoch[222](54/88): Loss_D: 0.2454 Loss_G: 1.1521\n===> Epoch[222](55/88): Loss_D: 0.2226 Loss_G: 1.2107\n===> Epoch[222](56/88): Loss_D: 0.2541 Loss_G: 0.9420\n===> Epoch[222](57/88): Loss_D: 0.2493 Loss_G: 1.1756\n===> Epoch[222](58/88): Loss_D: 0.2558 Loss_G: 1.1110\n===> Epoch[222](59/88): Loss_D: 0.2410 Loss_G: 1.1287\n===> Epoch[222](60/88): Loss_D: 0.2688 Loss_G: 1.1030\n===> Epoch[222](61/88): Loss_D: 0.1979 Loss_G: 1.5075\n===> Epoch[222](62/88): Loss_D: 0.2299 Loss_G: 1.3660\n===> Epoch[222](63/88): Loss_D: 0.2499 Loss_G: 1.1652\n===> Epoch[222](64/88): Loss_D: 0.2491 Loss_G: 1.2790\n===> Epoch[222](65/88): Loss_D: 0.2488 Loss_G: 1.1912\n===> Epoch[222](66/88): Loss_D: 0.2411 Loss_G: 1.0975\n===> Epoch[222](67/88): Loss_D: 0.2502 Loss_G: 1.2103\n===> Epoch[222](68/88): Loss_D: 0.2569 Loss_G: 1.2198\n===> Epoch[222](69/88): Loss_D: 0.2347 Loss_G: 1.4805\n===> Epoch[222](70/88): Loss_D: 0.2515 Loss_G: 1.1788\n===> Epoch[222](71/88): Loss_D: 0.2395 Loss_G: 1.2596\n===> Epoch[222](72/88): Loss_D: 0.2375 Loss_G: 1.1788\n===> Epoch[222](73/88): Loss_D: 0.2444 Loss_G: 1.2477\n===> Epoch[222](74/88): Loss_D: 0.2542 Loss_G: 1.2042\n===> Epoch[222](75/88): Loss_D: 0.2158 Loss_G: 1.4769\n===> Epoch[222](76/88): Loss_D: 0.2463 Loss_G: 1.1271\n===> Epoch[222](77/88): Loss_D: 0.2502 Loss_G: 1.0669\n===> Epoch[222](78/88): Loss_D: 0.2064 Loss_G: 1.4735\n===> Epoch[222](79/88): Loss_D: 0.2523 Loss_G: 1.1752\n===> Epoch[222](80/88): Loss_D: 0.2684 Loss_G: 1.2553\n===> Epoch[222](81/88): Loss_D: 0.2383 Loss_G: 1.3552\n===> Epoch[222](82/88): Loss_D: 0.2495 Loss_G: 1.0720\n===> Epoch[222](83/88): Loss_D: 0.2461 Loss_G: 1.2924\n===> Epoch[222](84/88): Loss_D: 0.1757 Loss_G: 1.6886\n===> Epoch[222](85/88): Loss_D: 0.2417 Loss_G: 1.4500\n===> Epoch[222](86/88): Loss_D: 0.2310 Loss_G: 1.2747\n===> Epoch[222](87/88): Loss_D: 0.2658 Loss_G: 1.1250\n===> Epoch[222](88/88): Loss_D: 0.2474 Loss_G: 1.1105\nlearning rate = 0.0000446\nlearning rate = 0.0000446\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0001} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[223](1/88): Loss_D: 0.2445 Loss_G: 1.2349\n===> Epoch[223](2/88): Loss_D: 0.2466 Loss_G: 1.1533\n===> Epoch[223](3/88): Loss_D: 0.2310 Loss_G: 1.3128\n===> Epoch[223](4/88): Loss_D: 0.1848 Loss_G: 1.4919\n===> Epoch[223](5/88): Loss_D: 0.2609 Loss_G: 1.2366\n===> Epoch[223](6/88): Loss_D: 0.2629 Loss_G: 1.1048\n===> Epoch[223](7/88): Loss_D: 0.2578 Loss_G: 1.2177\n===> Epoch[223](8/88): Loss_D: 0.2421 Loss_G: 1.1875\n===> Epoch[223](9/88): Loss_D: 0.2543 Loss_G: 1.2394\n===> Epoch[223](10/88): Loss_D: 0.2505 Loss_G: 1.0670\n===> Epoch[223](11/88): Loss_D: 0.2439 Loss_G: 1.1877\n===> Epoch[223](12/88): Loss_D: 0.2395 Loss_G: 1.1764\n===> Epoch[223](13/88): Loss_D: 0.2492 Loss_G: 1.1365\n===> Epoch[223](14/88): Loss_D: 0.2183 Loss_G: 1.2833\n===> Epoch[223](15/88): Loss_D: 0.2491 Loss_G: 1.1643\n===> Epoch[223](16/88): Loss_D: 0.2331 Loss_G: 1.1690\n===> Epoch[223](17/88): Loss_D: 0.2469 Loss_G: 1.2044\n===> Epoch[223](18/88): Loss_D: 0.2497 Loss_G: 1.1997\n===> Epoch[223](19/88): Loss_D: 0.2488 Loss_G: 1.0855\n===> Epoch[223](20/88): Loss_D: 0.2581 Loss_G: 1.0817\n===> Epoch[223](21/88): Loss_D: 0.2474 Loss_G: 1.2483\n===> Epoch[223](22/88): Loss_D: 0.2434 Loss_G: 1.2269\n===> Epoch[223](23/88): Loss_D: 0.2360 Loss_G: 1.1055\n===> Epoch[223](24/88): Loss_D: 0.2411 Loss_G: 1.1598\n===> Epoch[223](25/88): Loss_D: 0.2042 Loss_G: 1.5211\n===> Epoch[223](26/88): Loss_D: 0.2138 Loss_G: 1.4202\n===> Epoch[223](27/88): Loss_D: 0.2570 Loss_G: 1.2287\n===> Epoch[223](28/88): Loss_D: 0.1960 Loss_G: 1.8646\n===> Epoch[223](29/88): Loss_D: 0.2668 Loss_G: 1.1733\n===> Epoch[223](30/88): Loss_D: 0.2374 Loss_G: 1.5406\n===> Epoch[223](31/88): Loss_D: 0.2696 Loss_G: 1.1199\n===> Epoch[223](32/88): Loss_D: 0.2393 Loss_G: 1.4963\n===> Epoch[223](33/88): Loss_D: 0.2365 Loss_G: 1.3516\n===> Epoch[223](34/88): Loss_D: 0.2405 Loss_G: 1.2893\n===> Epoch[223](35/88): Loss_D: 0.2612 Loss_G: 1.0559\n===> Epoch[223](36/88): Loss_D: 0.1737 Loss_G: 1.5606\n===> Epoch[223](37/88): Loss_D: 0.2384 Loss_G: 1.3124\n===> Epoch[223](38/88): Loss_D: 0.2289 Loss_G: 1.2642\n===> Epoch[223](39/88): Loss_D: 0.2399 Loss_G: 1.2218\n===> Epoch[223](40/88): Loss_D: 0.2571 Loss_G: 1.0720\n===> Epoch[223](41/88): Loss_D: 0.2436 Loss_G: 1.2811\n===> Epoch[223](42/88): Loss_D: 0.1798 Loss_G: 1.5948\n===> Epoch[223](43/88): Loss_D: 0.2543 Loss_G: 1.0884\n===> Epoch[223](44/88): Loss_D: 0.2526 Loss_G: 1.2081\n===> Epoch[223](45/88): Loss_D: 0.2562 Loss_G: 1.0557\n===> Epoch[223](46/88): Loss_D: 0.2430 Loss_G: 1.2282\n===> Epoch[223](47/88): Loss_D: 0.2295 Loss_G: 1.3502\n===> Epoch[223](48/88): Loss_D: 0.2226 Loss_G: 1.4179\n===> Epoch[223](49/88): Loss_D: 0.2435 Loss_G: 1.2869\n===> Epoch[223](50/88): Loss_D: 0.2489 Loss_G: 1.0868\n===> Epoch[223](51/88): Loss_D: 0.1997 Loss_G: 2.2851\n===> Epoch[223](52/88): Loss_D: 0.2501 Loss_G: 1.0391\n===> Epoch[223](53/88): Loss_D: 0.2534 Loss_G: 1.0499\n===> Epoch[223](54/88): Loss_D: 0.2511 Loss_G: 1.1080\n===> Epoch[223](55/88): Loss_D: 0.2360 Loss_G: 1.2917\n===> Epoch[223](56/88): Loss_D: 0.2519 Loss_G: 1.2900\n===> Epoch[223](57/88): Loss_D: 0.2538 Loss_G: 1.1068\n===> Epoch[223](58/88): Loss_D: 0.2567 Loss_G: 1.2634\n===> Epoch[223](59/88): Loss_D: 0.2434 Loss_G: 1.3545\n===> Epoch[223](60/88): Loss_D: 0.2518 Loss_G: 1.1909\n===> Epoch[223](61/88): Loss_D: 0.2400 Loss_G: 1.1493\n===> Epoch[223](62/88): Loss_D: 0.2368 Loss_G: 1.3892\n===> Epoch[223](63/88): Loss_D: 0.2345 Loss_G: 1.3074\n===> Epoch[223](64/88): Loss_D: 0.2506 Loss_G: 1.2530\n===> Epoch[223](65/88): Loss_D: 0.2551 Loss_G: 1.2353\n===> Epoch[223](66/88): Loss_D: 0.2281 Loss_G: 1.2768\n===> Epoch[223](67/88): Loss_D: 0.2342 Loss_G: 1.3376\n===> Epoch[223](68/88): Loss_D: 0.2481 Loss_G: 1.0672\n===> Epoch[223](69/88): Loss_D: 0.2445 Loss_G: 1.2134\n===> Epoch[223](70/88): Loss_D: 0.2627 Loss_G: 1.0229\n===> Epoch[223](71/88): Loss_D: 0.2369 Loss_G: 1.3464\n===> Epoch[223](72/88): Loss_D: 0.2525 Loss_G: 1.1771\n===> Epoch[223](73/88): Loss_D: 0.2440 Loss_G: 1.2111\n===> Epoch[223](74/88): Loss_D: 0.2387 Loss_G: 1.4994\n===> Epoch[223](75/88): Loss_D: 0.2443 Loss_G: 1.1518\n===> Epoch[223](76/88): Loss_D: 0.2323 Loss_G: 1.4558\n===> Epoch[223](77/88): Loss_D: 0.2587 Loss_G: 1.0471\n===> Epoch[223](78/88): Loss_D: 0.2131 Loss_G: 1.3804\n===> Epoch[223](79/88): Loss_D: 0.2529 Loss_G: 1.1837\n===> Epoch[223](80/88): Loss_D: 0.2525 Loss_G: 0.9764\n===> Epoch[223](81/88): Loss_D: 0.2489 Loss_G: 0.9843\n===> Epoch[223](82/88): Loss_D: 0.2397 Loss_G: 1.3230\n===> Epoch[223](83/88): Loss_D: 0.2416 Loss_G: 1.1036\n===> Epoch[223](84/88): Loss_D: 0.2378 Loss_G: 1.2211\n===> Epoch[223](85/88): Loss_D: 0.2275 Loss_G: 1.1082\n===> Epoch[223](86/88): Loss_D: 0.2418 Loss_G: 1.4034\n===> Epoch[223](87/88): Loss_D: 0.2575 Loss_G: 1.1963\n===> Epoch[223](88/88): Loss_D: 0.2589 Loss_G: 1.3398\nlearning rate = 0.0000444\nlearning rate = 0.0000444\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0411} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[224](1/88): Loss_D: 0.2601 Loss_G: 1.2193\n===> Epoch[224](2/88): Loss_D: 0.2451 Loss_G: 1.2318\n===> Epoch[224](3/88): Loss_D: 0.2490 Loss_G: 1.0839\n===> Epoch[224](4/88): Loss_D: 0.2413 Loss_G: 1.2392\n===> Epoch[224](5/88): Loss_D: 0.2524 Loss_G: 1.1412\n===> Epoch[224](6/88): Loss_D: 0.2658 Loss_G: 1.0391\n===> Epoch[224](7/88): Loss_D: 0.2294 Loss_G: 1.4062\n===> Epoch[224](8/88): Loss_D: 0.2544 Loss_G: 1.0284\n===> Epoch[224](9/88): Loss_D: 0.2235 Loss_G: 1.3785\n===> Epoch[224](10/88): Loss_D: 0.1797 Loss_G: 1.5570\n===> Epoch[224](11/88): Loss_D: 0.2245 Loss_G: 1.3707\n===> Epoch[224](12/88): Loss_D: 0.2595 Loss_G: 1.1013\n===> Epoch[224](13/88): Loss_D: 0.2493 Loss_G: 1.2789\n===> Epoch[224](14/88): Loss_D: 0.2354 Loss_G: 1.1857\n===> Epoch[224](15/88): Loss_D: 0.2345 Loss_G: 1.1488\n===> Epoch[224](16/88): Loss_D: 0.2465 Loss_G: 1.2006\n===> Epoch[224](17/88): Loss_D: 0.2346 Loss_G: 1.3016\n===> Epoch[224](18/88): Loss_D: 0.2499 Loss_G: 1.2024\n===> Epoch[224](19/88): Loss_D: 0.2369 Loss_G: 1.2074\n===> Epoch[224](20/88): Loss_D: 0.2360 Loss_G: 1.3506\n===> Epoch[224](21/88): Loss_D: 0.2534 Loss_G: 1.2636\n===> Epoch[224](22/88): Loss_D: 0.2597 Loss_G: 1.1218\n===> Epoch[224](23/88): Loss_D: 0.2511 Loss_G: 1.2623\n===> Epoch[224](24/88): Loss_D: 0.2329 Loss_G: 1.3724\n===> Epoch[224](25/88): Loss_D: 0.2052 Loss_G: 1.3018\n===> Epoch[224](26/88): Loss_D: 0.2550 Loss_G: 1.3123\n===> Epoch[224](27/88): Loss_D: 0.2369 Loss_G: 1.2784\n===> Epoch[224](28/88): Loss_D: 0.2492 Loss_G: 1.1562\n===> Epoch[224](29/88): Loss_D: 0.2312 Loss_G: 1.3808\n===> Epoch[224](30/88): Loss_D: 0.2443 Loss_G: 1.2369\n===> Epoch[224](31/88): Loss_D: 0.2438 Loss_G: 1.1343\n===> Epoch[224](32/88): Loss_D: 0.2532 Loss_G: 1.1024\n===> Epoch[224](33/88): Loss_D: 0.1782 Loss_G: 1.9689\n===> Epoch[224](34/88): Loss_D: 0.2493 Loss_G: 1.1154\n===> Epoch[224](35/88): Loss_D: 0.2489 Loss_G: 1.0056\n===> Epoch[224](36/88): Loss_D: 0.2306 Loss_G: 1.3870\n===> Epoch[224](37/88): Loss_D: 0.2512 Loss_G: 1.2004\n===> Epoch[224](38/88): Loss_D: 0.2465 Loss_G: 1.1746\n===> Epoch[224](39/88): Loss_D: 0.2413 Loss_G: 1.2497\n===> Epoch[224](40/88): Loss_D: 0.2504 Loss_G: 1.1012\n===> Epoch[224](41/88): Loss_D: 0.2450 Loss_G: 1.1554\n===> Epoch[224](42/88): Loss_D: 0.2214 Loss_G: 1.3514\n===> Epoch[224](43/88): Loss_D: 0.2516 Loss_G: 0.9205\n===> Epoch[224](44/88): Loss_D: 0.2380 Loss_G: 1.4306\n===> Epoch[224](45/88): Loss_D: 0.2263 Loss_G: 1.4521\n===> Epoch[224](46/88): Loss_D: 0.2596 Loss_G: 1.2477\n===> Epoch[224](47/88): Loss_D: 0.2390 Loss_G: 1.1508\n===> Epoch[224](48/88): Loss_D: 0.2648 Loss_G: 1.0553\n===> Epoch[224](49/88): Loss_D: 0.2315 Loss_G: 1.3104\n===> Epoch[224](50/88): Loss_D: 0.2458 Loss_G: 1.1216\n===> Epoch[224](51/88): Loss_D: 0.2196 Loss_G: 1.1131\n===> Epoch[224](52/88): Loss_D: 0.2523 Loss_G: 1.1970\n===> Epoch[224](53/88): Loss_D: 0.2589 Loss_G: 1.1146\n===> Epoch[224](54/88): Loss_D: 0.2292 Loss_G: 1.3389\n===> Epoch[224](55/88): Loss_D: 0.2545 Loss_G: 1.0661\n===> Epoch[224](56/88): Loss_D: 0.2537 Loss_G: 0.9781\n===> Epoch[224](57/88): Loss_D: 0.2337 Loss_G: 1.2171\n===> Epoch[224](58/88): Loss_D: 0.2539 Loss_G: 1.2441\n===> Epoch[224](59/88): Loss_D: 0.2369 Loss_G: 1.1502\n===> Epoch[224](60/88): Loss_D: 0.2455 Loss_G: 1.1822\n===> Epoch[224](61/88): Loss_D: 0.1790 Loss_G: 1.6503\n===> Epoch[224](62/88): Loss_D: 0.2370 Loss_G: 1.3254\n===> Epoch[224](63/88): Loss_D: 0.2377 Loss_G: 1.2095\n===> Epoch[224](64/88): Loss_D: 0.2276 Loss_G: 1.3613\n===> Epoch[224](65/88): Loss_D: 0.2537 Loss_G: 1.1283\n===> Epoch[224](66/88): Loss_D: 0.2488 Loss_G: 1.0832\n===> Epoch[224](67/88): Loss_D: 0.2474 Loss_G: 1.1748\n===> Epoch[224](68/88): Loss_D: 0.2476 Loss_G: 1.1969\n===> Epoch[224](69/88): Loss_D: 0.2486 Loss_G: 1.0781\n===> Epoch[224](70/88): Loss_D: 0.2470 Loss_G: 1.0616\n===> Epoch[224](71/88): Loss_D: 0.2495 Loss_G: 1.1488\n===> Epoch[224](72/88): Loss_D: 0.1756 Loss_G: 1.5652\n===> Epoch[224](73/88): Loss_D: 0.2534 Loss_G: 1.1768\n===> Epoch[224](74/88): Loss_D: 0.2157 Loss_G: 1.3058\n===> Epoch[224](75/88): Loss_D: 0.2275 Loss_G: 2.1824\n===> Epoch[224](76/88): Loss_D: 0.2505 Loss_G: 1.2014\n===> Epoch[224](77/88): Loss_D: 0.2503 Loss_G: 1.1602\n===> Epoch[224](78/88): Loss_D: 0.2506 Loss_G: 1.0744\n===> Epoch[224](79/88): Loss_D: 0.2398 Loss_G: 1.3723\n===> Epoch[224](80/88): Loss_D: 0.2571 Loss_G: 1.2948\n===> Epoch[224](81/88): Loss_D: 0.2581 Loss_G: 1.0891\n===> Epoch[224](82/88): Loss_D: 0.2431 Loss_G: 1.3420\n===> Epoch[224](83/88): Loss_D: 0.2406 Loss_G: 1.1655\n===> Epoch[224](84/88): Loss_D: 0.2534 Loss_G: 1.2180\n===> Epoch[224](85/88): Loss_D: 0.2188 Loss_G: 1.4494\n===> Epoch[224](86/88): Loss_D: 0.2572 Loss_G: 1.2459\n===> Epoch[224](87/88): Loss_D: 0.2630 Loss_G: 0.9985\n===> Epoch[224](88/88): Loss_D: 0.2356 Loss_G: 1.2352\nlearning rate = 0.0000441\nlearning rate = 0.0000441\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0007} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[225](1/88): Loss_D: 0.2492 Loss_G: 1.3613\n===> Epoch[225](2/88): Loss_D: 0.2029 Loss_G: 1.8294\n===> Epoch[225](3/88): Loss_D: 0.2294 Loss_G: 1.4004\n===> Epoch[225](4/88): Loss_D: 0.1994 Loss_G: 2.1519\n===> Epoch[225](5/88): Loss_D: 0.2524 Loss_G: 1.2838\n===> Epoch[225](6/88): Loss_D: 0.2566 Loss_G: 1.3496\n===> Epoch[225](7/88): Loss_D: 0.2449 Loss_G: 1.0476\n===> Epoch[225](8/88): Loss_D: 0.2555 Loss_G: 1.0809\n===> Epoch[225](9/88): Loss_D: 0.2548 Loss_G: 0.9288\n===> Epoch[225](10/88): Loss_D: 0.2428 Loss_G: 1.3653\n===> Epoch[225](11/88): Loss_D: 0.2506 Loss_G: 0.9912\n===> Epoch[225](12/88): Loss_D: 0.1767 Loss_G: 1.5632\n===> Epoch[225](13/88): Loss_D: 0.2475 Loss_G: 1.2454\n===> Epoch[225](14/88): Loss_D: 0.2598 Loss_G: 1.1574\n===> Epoch[225](15/88): Loss_D: 0.2482 Loss_G: 1.1880\n===> Epoch[225](16/88): Loss_D: 0.2434 Loss_G: 1.2006\n===> Epoch[225](17/88): Loss_D: 0.2477 Loss_G: 1.4345\n===> Epoch[225](18/88): Loss_D: 0.2467 Loss_G: 1.2683\n===> Epoch[225](19/88): Loss_D: 0.2509 Loss_G: 1.2474\n===> Epoch[225](20/88): Loss_D: 0.2128 Loss_G: 1.4433\n===> Epoch[225](21/88): Loss_D: 0.2474 Loss_G: 1.3145\n===> Epoch[225](22/88): Loss_D: 0.2494 Loss_G: 1.0985\n===> Epoch[225](23/88): Loss_D: 0.2498 Loss_G: 1.2043\n===> Epoch[225](24/88): Loss_D: 0.2606 Loss_G: 1.2520\n===> Epoch[225](25/88): Loss_D: 0.2676 Loss_G: 1.0493\n===> Epoch[225](26/88): Loss_D: 0.2432 Loss_G: 1.2642\n===> Epoch[225](27/88): Loss_D: 0.2526 Loss_G: 1.1145\n===> Epoch[225](28/88): Loss_D: 0.2499 Loss_G: 1.0267\n===> Epoch[225](29/88): Loss_D: 0.2499 Loss_G: 1.1950\n===> Epoch[225](30/88): Loss_D: 0.2312 Loss_G: 1.2615\n===> Epoch[225](31/88): Loss_D: 0.2315 Loss_G: 1.1553\n===> Epoch[225](32/88): Loss_D: 0.2555 Loss_G: 1.1851\n===> Epoch[225](33/88): Loss_D: 0.1934 Loss_G: 1.4792\n===> Epoch[225](34/88): Loss_D: 0.2650 Loss_G: 1.2182\n===> Epoch[225](35/88): Loss_D: 0.2609 Loss_G: 1.2575\n===> Epoch[225](36/88): Loss_D: 0.2407 Loss_G: 1.2523\n===> Epoch[225](37/88): Loss_D: 0.2523 Loss_G: 1.2937\n===> Epoch[225](38/88): Loss_D: 0.2419 Loss_G: 1.1885\n===> Epoch[225](39/88): Loss_D: 0.2229 Loss_G: 1.2458\n===> Epoch[225](40/88): Loss_D: 0.1919 Loss_G: 1.6055\n===> Epoch[225](41/88): Loss_D: 0.2259 Loss_G: 1.4911\n===> Epoch[225](42/88): Loss_D: 0.2526 Loss_G: 1.2232\n===> Epoch[225](43/88): Loss_D: 0.2478 Loss_G: 1.1930\n===> Epoch[225](44/88): Loss_D: 0.2419 Loss_G: 1.0304\n===> Epoch[225](45/88): Loss_D: 0.2553 Loss_G: 1.0898\n===> Epoch[225](46/88): Loss_D: 0.2490 Loss_G: 1.2380\n===> Epoch[225](47/88): Loss_D: 0.2426 Loss_G: 1.1536\n===> Epoch[225](48/88): Loss_D: 0.2503 Loss_G: 1.1211\n===> Epoch[225](49/88): Loss_D: 0.2451 Loss_G: 1.2146\n===> Epoch[225](50/88): Loss_D: 0.2581 Loss_G: 1.0956\n===> Epoch[225](51/88): Loss_D: 0.2524 Loss_G: 1.0462\n===> Epoch[225](52/88): Loss_D: 0.2461 Loss_G: 1.2525\n===> Epoch[225](53/88): Loss_D: 0.2298 Loss_G: 1.3176\n===> Epoch[225](54/88): Loss_D: 0.2273 Loss_G: 1.4457\n===> Epoch[225](55/88): Loss_D: 0.2369 Loss_G: 1.2308\n===> Epoch[225](56/88): Loss_D: 0.2563 Loss_G: 1.1949\n===> Epoch[225](57/88): Loss_D: 0.2289 Loss_G: 1.4423\n===> Epoch[225](58/88): Loss_D: 0.2506 Loss_G: 1.2862\n===> Epoch[225](59/88): Loss_D: 0.2500 Loss_G: 1.1307\n===> Epoch[225](60/88): Loss_D: 0.2489 Loss_G: 1.1339\n===> Epoch[225](61/88): Loss_D: 0.2496 Loss_G: 1.1662\n===> Epoch[225](62/88): Loss_D: 0.2353 Loss_G: 1.3018\n===> Epoch[225](63/88): Loss_D: 0.2496 Loss_G: 1.1595\n===> Epoch[225](64/88): Loss_D: 0.2338 Loss_G: 1.3302\n===> Epoch[225](65/88): Loss_D: 0.2253 Loss_G: 1.3643\n===> Epoch[225](66/88): Loss_D: 0.2294 Loss_G: 1.0794\n===> Epoch[225](67/88): Loss_D: 0.2294 Loss_G: 1.4084\n===> Epoch[225](68/88): Loss_D: 0.2744 Loss_G: 1.0949\n===> Epoch[225](69/88): Loss_D: 0.2150 Loss_G: 1.4612\n===> Epoch[225](70/88): Loss_D: 0.2533 Loss_G: 1.2374\n===> Epoch[225](71/88): Loss_D: 0.2700 Loss_G: 1.1981\n===> Epoch[225](72/88): Loss_D: 0.2370 Loss_G: 1.2039\n===> Epoch[225](73/88): Loss_D: 0.2653 Loss_G: 1.0921\n===> Epoch[225](74/88): Loss_D: 0.2352 Loss_G: 1.4491\n===> Epoch[225](75/88): Loss_D: 0.2527 Loss_G: 1.0457\n===> Epoch[225](76/88): Loss_D: 0.2426 Loss_G: 1.1795\n===> Epoch[225](77/88): Loss_D: 0.2491 Loss_G: 1.1800\n===> Epoch[225](78/88): Loss_D: 0.2390 Loss_G: 1.1050\n===> Epoch[225](79/88): Loss_D: 0.2450 Loss_G: 1.2114\n===> Epoch[225](80/88): Loss_D: 0.2434 Loss_G: 1.1276\n===> Epoch[225](81/88): Loss_D: 0.2424 Loss_G: 1.3913\n===> Epoch[225](82/88): Loss_D: 0.2628 Loss_G: 1.2395\n===> Epoch[225](83/88): Loss_D: 0.2441 Loss_G: 1.1950\n===> Epoch[225](84/88): Loss_D: 0.2514 Loss_G: 1.2424\n===> Epoch[225](85/88): Loss_D: 0.2421 Loss_G: 1.2955\n===> Epoch[225](86/88): Loss_D: 0.2445 Loss_G: 1.2691\n===> Epoch[225](87/88): Loss_D: 0.2367 Loss_G: 1.5038\n===> Epoch[225](88/88): Loss_D: 0.2494 Loss_G: 1.1737\nlearning rate = 0.0000439\nlearning rate = 0.0000439\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0015} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[226](1/88): Loss_D: 0.2212 Loss_G: 1.1744\n===> Epoch[226](2/88): Loss_D: 0.2425 Loss_G: 1.1872\n===> Epoch[226](3/88): Loss_D: 0.2542 Loss_G: 1.1510\n===> Epoch[226](4/88): Loss_D: 0.2460 Loss_G: 0.9752\n===> Epoch[226](5/88): Loss_D: 0.2538 Loss_G: 1.2318\n===> Epoch[226](6/88): Loss_D: 0.2454 Loss_G: 1.1838\n===> Epoch[226](7/88): Loss_D: 0.2450 Loss_G: 1.1130\n===> Epoch[226](8/88): Loss_D: 0.1815 Loss_G: 1.8455\n===> Epoch[226](9/88): Loss_D: 0.2588 Loss_G: 1.2113\n===> Epoch[226](10/88): Loss_D: 0.2227 Loss_G: 1.3782\n===> Epoch[226](11/88): Loss_D: 0.2522 Loss_G: 1.1321\n===> Epoch[226](12/88): Loss_D: 0.2341 Loss_G: 1.3582\n===> Epoch[226](13/88): Loss_D: 0.2325 Loss_G: 1.1563\n===> Epoch[226](14/88): Loss_D: 0.2384 Loss_G: 1.2461\n===> Epoch[226](15/88): Loss_D: 0.2441 Loss_G: 1.3780\n===> Epoch[226](16/88): Loss_D: 0.2415 Loss_G: 1.2190\n===> Epoch[226](17/88): Loss_D: 0.2407 Loss_G: 1.1689\n===> Epoch[226](18/88): Loss_D: 0.2377 Loss_G: 1.0856\n===> Epoch[226](19/88): Loss_D: 0.2514 Loss_G: 1.1653\n===> Epoch[226](20/88): Loss_D: 0.2461 Loss_G: 1.1675\n===> Epoch[226](21/88): Loss_D: 0.2387 Loss_G: 1.4131\n===> Epoch[226](22/88): Loss_D: 0.2545 Loss_G: 1.0978\n===> Epoch[226](23/88): Loss_D: 0.1668 Loss_G: 1.5376\n===> Epoch[226](24/88): Loss_D: 0.2499 Loss_G: 1.1061\n===> Epoch[226](25/88): Loss_D: 0.2452 Loss_G: 1.2105\n===> Epoch[226](26/88): Loss_D: 0.2397 Loss_G: 1.2705\n===> Epoch[226](27/88): Loss_D: 0.2273 Loss_G: 1.4256\n===> Epoch[226](28/88): Loss_D: 0.2368 Loss_G: 1.3725\n===> Epoch[226](29/88): Loss_D: 0.2236 Loss_G: 1.3590\n===> Epoch[226](30/88): Loss_D: 0.2358 Loss_G: 1.0996\n===> Epoch[226](31/88): Loss_D: 0.2639 Loss_G: 1.1653\n===> Epoch[226](32/88): Loss_D: 0.2215 Loss_G: 1.3751\n===> Epoch[226](33/88): Loss_D: 0.2415 Loss_G: 1.3812\n===> Epoch[226](34/88): Loss_D: 0.2646 Loss_G: 1.1026\n===> Epoch[226](35/88): Loss_D: 0.2462 Loss_G: 1.2107\n===> Epoch[226](36/88): Loss_D: 0.2582 Loss_G: 1.0489\n===> Epoch[226](37/88): Loss_D: 0.2416 Loss_G: 1.2346\n===> Epoch[226](38/88): Loss_D: 0.2246 Loss_G: 1.3705\n===> Epoch[226](39/88): Loss_D: 0.1764 Loss_G: 1.5811\n===> Epoch[226](40/88): Loss_D: 0.2251 Loss_G: 1.2315\n===> Epoch[226](41/88): Loss_D: 0.2563 Loss_G: 1.1151\n===> Epoch[226](42/88): Loss_D: 0.2336 Loss_G: 1.4517\n===> Epoch[226](43/88): Loss_D: 0.2438 Loss_G: 1.2732\n===> Epoch[226](44/88): Loss_D: 0.2739 Loss_G: 1.0886\n===> Epoch[226](45/88): Loss_D: 0.2321 Loss_G: 1.2732\n===> Epoch[226](46/88): Loss_D: 0.2455 Loss_G: 1.2339\n===> Epoch[226](47/88): Loss_D: 0.2351 Loss_G: 1.4976\n===> Epoch[226](48/88): Loss_D: 0.2500 Loss_G: 1.1688\n===> Epoch[226](49/88): Loss_D: 0.2558 Loss_G: 1.0270\n===> Epoch[226](50/88): Loss_D: 0.2464 Loss_G: 1.0796\n===> Epoch[226](51/88): Loss_D: 0.2459 Loss_G: 1.1404\n===> Epoch[226](52/88): Loss_D: 0.2582 Loss_G: 1.1624\n===> Epoch[226](53/88): Loss_D: 0.2518 Loss_G: 1.2549\n===> Epoch[226](54/88): Loss_D: 0.2440 Loss_G: 1.2388\n===> Epoch[226](55/88): Loss_D: 0.2442 Loss_G: 1.0600\n===> Epoch[226](56/88): Loss_D: 0.2498 Loss_G: 1.2989\n===> Epoch[226](57/88): Loss_D: 0.2583 Loss_G: 1.0683\n===> Epoch[226](58/88): Loss_D: 0.2529 Loss_G: 1.1048\n===> Epoch[226](59/88): Loss_D: 0.2502 Loss_G: 1.2774\n===> Epoch[226](60/88): Loss_D: 0.2250 Loss_G: 1.4149\n===> Epoch[226](61/88): Loss_D: 0.2472 Loss_G: 1.0962\n===> Epoch[226](62/88): Loss_D: 0.2292 Loss_G: 1.4152\n===> Epoch[226](63/88): Loss_D: 0.2355 Loss_G: 1.1337\n===> Epoch[226](64/88): Loss_D: 0.2454 Loss_G: 1.2987\n===> Epoch[226](65/88): Loss_D: 0.2457 Loss_G: 1.1648\n===> Epoch[226](66/88): Loss_D: 0.2513 Loss_G: 1.2040\n===> Epoch[226](67/88): Loss_D: 0.2398 Loss_G: 1.2631\n===> Epoch[226](68/88): Loss_D: 0.2364 Loss_G: 1.1182\n===> Epoch[226](69/88): Loss_D: 0.2426 Loss_G: 1.1572\n===> Epoch[226](70/88): Loss_D: 0.2095 Loss_G: 1.4984\n===> Epoch[226](71/88): Loss_D: 0.2072 Loss_G: 2.2703\n===> Epoch[226](72/88): Loss_D: 0.2523 Loss_G: 1.3576\n===> Epoch[226](73/88): Loss_D: 0.2420 Loss_G: 1.2555\n===> Epoch[226](74/88): Loss_D: 0.2569 Loss_G: 1.1864\n===> Epoch[226](75/88): Loss_D: 0.2304 Loss_G: 1.3908\n===> Epoch[226](76/88): Loss_D: 0.2389 Loss_G: 1.4196\n===> Epoch[226](77/88): Loss_D: 0.2576 Loss_G: 1.0993\n===> Epoch[226](78/88): Loss_D: 0.2590 Loss_G: 1.2295\n===> Epoch[226](79/88): Loss_D: 0.2394 Loss_G: 1.1148\n===> Epoch[226](80/88): Loss_D: 0.2473 Loss_G: 1.1668\n===> Epoch[226](81/88): Loss_D: 0.2579 Loss_G: 0.9680\n===> Epoch[226](82/88): Loss_D: 0.2549 Loss_G: 1.2406\n===> Epoch[226](83/88): Loss_D: 0.2411 Loss_G: 1.4007\n===> Epoch[226](84/88): Loss_D: 0.1810 Loss_G: 1.4974\n===> Epoch[226](85/88): Loss_D: 0.2314 Loss_G: 1.2194\n===> Epoch[226](86/88): Loss_D: 0.2547 Loss_G: 1.1981\n===> Epoch[226](87/88): Loss_D: 0.2540 Loss_G: 1.2884\n===> Epoch[226](88/88): Loss_D: 0.2583 Loss_G: 1.0899\nlearning rate = 0.0000436\nlearning rate = 0.0000436\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0006} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[227](1/88): Loss_D: 0.2617 Loss_G: 1.0343\n===> Epoch[227](2/88): Loss_D: 0.2531 Loss_G: 0.9824\n===> Epoch[227](3/88): Loss_D: 0.1978 Loss_G: 1.5646\n===> Epoch[227](4/88): Loss_D: 0.2461 Loss_G: 1.3006\n===> Epoch[227](5/88): Loss_D: 0.2455 Loss_G: 1.1902\n===> Epoch[227](6/88): Loss_D: 0.2477 Loss_G: 1.0898\n===> Epoch[227](7/88): Loss_D: 0.2481 Loss_G: 1.0185\n===> Epoch[227](8/88): Loss_D: 0.2486 Loss_G: 1.1910\n===> Epoch[227](9/88): Loss_D: 0.2428 Loss_G: 1.3340\n===> Epoch[227](10/88): Loss_D: 0.2356 Loss_G: 1.3662\n===> Epoch[227](11/88): Loss_D: 0.2376 Loss_G: 1.2459\n===> Epoch[227](12/88): Loss_D: 0.2327 Loss_G: 1.3357\n===> Epoch[227](13/88): Loss_D: 0.2340 Loss_G: 1.2752\n===> Epoch[227](14/88): Loss_D: 0.2357 Loss_G: 1.2548\n===> Epoch[227](15/88): Loss_D: 0.2374 Loss_G: 1.2510\n===> Epoch[227](16/88): Loss_D: 0.2612 Loss_G: 1.2290\n===> Epoch[227](17/88): Loss_D: 0.2384 Loss_G: 1.3245\n===> Epoch[227](18/88): Loss_D: 0.2384 Loss_G: 1.5034\n===> Epoch[227](19/88): Loss_D: 0.2293 Loss_G: 1.1355\n===> Epoch[227](20/88): Loss_D: 0.2469 Loss_G: 1.2190\n===> Epoch[227](21/88): Loss_D: 0.2408 Loss_G: 1.3746\n===> Epoch[227](22/88): Loss_D: 0.2482 Loss_G: 1.1151\n===> Epoch[227](23/88): Loss_D: 0.2267 Loss_G: 1.2253\n===> Epoch[227](24/88): Loss_D: 0.2656 Loss_G: 1.0467\n===> Epoch[227](25/88): Loss_D: 0.2376 Loss_G: 1.2219\n===> Epoch[227](26/88): Loss_D: 0.2534 Loss_G: 1.0681\n===> Epoch[227](27/88): Loss_D: 0.2469 Loss_G: 1.2351\n===> Epoch[227](28/88): Loss_D: 0.2154 Loss_G: 1.3453\n===> Epoch[227](29/88): Loss_D: 0.2081 Loss_G: 2.2539\n===> Epoch[227](30/88): Loss_D: 0.2484 Loss_G: 1.4296\n===> Epoch[227](31/88): Loss_D: 0.2272 Loss_G: 1.5000\n===> Epoch[227](32/88): Loss_D: 0.2200 Loss_G: 1.2748\n===> Epoch[227](33/88): Loss_D: 0.2521 Loss_G: 1.2690\n===> Epoch[227](34/88): Loss_D: 0.2550 Loss_G: 1.1535\n===> Epoch[227](35/88): Loss_D: 0.2310 Loss_G: 1.2852\n===> Epoch[227](36/88): Loss_D: 0.2557 Loss_G: 1.0667\n===> Epoch[227](37/88): Loss_D: 0.2324 Loss_G: 1.4133\n===> Epoch[227](38/88): Loss_D: 0.2492 Loss_G: 1.2046\n===> Epoch[227](39/88): Loss_D: 0.2415 Loss_G: 1.1985\n===> Epoch[227](40/88): Loss_D: 0.2580 Loss_G: 1.1325\n===> Epoch[227](41/88): Loss_D: 0.2476 Loss_G: 1.1371\n===> Epoch[227](42/88): Loss_D: 0.2602 Loss_G: 1.0982\n===> Epoch[227](43/88): Loss_D: 0.2549 Loss_G: 1.0246\n===> Epoch[227](44/88): Loss_D: 0.2452 Loss_G: 1.1366\n===> Epoch[227](45/88): Loss_D: 0.2513 Loss_G: 1.1897\n===> Epoch[227](46/88): Loss_D: 0.2490 Loss_G: 1.1522\n===> Epoch[227](47/88): Loss_D: 0.2288 Loss_G: 1.3598\n===> Epoch[227](48/88): Loss_D: 0.2482 Loss_G: 0.9304\n===> Epoch[227](49/88): Loss_D: 0.2434 Loss_G: 1.1229\n===> Epoch[227](50/88): Loss_D: 0.2360 Loss_G: 1.3988\n===> Epoch[227](51/88): Loss_D: 0.2503 Loss_G: 1.1448\n===> Epoch[227](52/88): Loss_D: 0.2520 Loss_G: 1.2019\n===> Epoch[227](53/88): Loss_D: 0.1651 Loss_G: 1.4892\n===> Epoch[227](54/88): Loss_D: 0.2440 Loss_G: 1.2009\n===> Epoch[227](55/88): Loss_D: 0.2360 Loss_G: 1.3888\n===> Epoch[227](56/88): Loss_D: 0.2527 Loss_G: 1.0766\n===> Epoch[227](57/88): Loss_D: 0.2571 Loss_G: 1.1042\n===> Epoch[227](58/88): Loss_D: 0.2519 Loss_G: 1.1605\n===> Epoch[227](59/88): Loss_D: 0.2000 Loss_G: 1.7881\n===> Epoch[227](60/88): Loss_D: 0.2483 Loss_G: 1.1182\n===> Epoch[227](61/88): Loss_D: 0.2506 Loss_G: 1.2490\n===> Epoch[227](62/88): Loss_D: 0.2554 Loss_G: 1.2861\n===> Epoch[227](63/88): Loss_D: 0.2559 Loss_G: 1.1778\n===> Epoch[227](64/88): Loss_D: 0.1773 Loss_G: 1.5403\n===> Epoch[227](65/88): Loss_D: 0.2105 Loss_G: 1.5280\n===> Epoch[227](66/88): Loss_D: 0.2543 Loss_G: 1.0763\n===> Epoch[227](67/88): Loss_D: 0.2554 Loss_G: 1.2222\n===> Epoch[227](68/88): Loss_D: 0.2408 Loss_G: 1.2596\n===> Epoch[227](69/88): Loss_D: 0.2568 Loss_G: 1.1800\n===> Epoch[227](70/88): Loss_D: 0.2558 Loss_G: 1.0528\n===> Epoch[227](71/88): Loss_D: 0.2540 Loss_G: 1.2000\n===> Epoch[227](72/88): Loss_D: 0.2474 Loss_G: 1.0987\n===> Epoch[227](73/88): Loss_D: 0.2388 Loss_G: 1.1928\n===> Epoch[227](74/88): Loss_D: 0.2692 Loss_G: 1.1960\n===> Epoch[227](75/88): Loss_D: 0.2275 Loss_G: 1.4461\n===> Epoch[227](76/88): Loss_D: 0.2166 Loss_G: 1.3718\n===> Epoch[227](77/88): Loss_D: 0.2483 Loss_G: 1.2312\n===> Epoch[227](78/88): Loss_D: 0.2355 Loss_G: 1.5267\n===> Epoch[227](79/88): Loss_D: 0.2585 Loss_G: 1.1077\n===> Epoch[227](80/88): Loss_D: 0.2472 Loss_G: 1.2650\n===> Epoch[227](81/88): Loss_D: 0.2526 Loss_G: 1.2102\n===> Epoch[227](82/88): Loss_D: 0.2310 Loss_G: 1.2540\n===> Epoch[227](83/88): Loss_D: 0.2406 Loss_G: 1.1359\n===> Epoch[227](84/88): Loss_D: 0.2310 Loss_G: 1.1663\n===> Epoch[227](85/88): Loss_D: 0.2436 Loss_G: 1.1659\n===> Epoch[227](86/88): Loss_D: 0.2563 Loss_G: 1.2752\n===> Epoch[227](87/88): Loss_D: 0.2531 Loss_G: 1.2175\n===> Epoch[227](88/88): Loss_D: 0.2301 Loss_G: 1.5304\nlearning rate = 0.0000434\nlearning rate = 0.0000434\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0002} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[228](1/88): Loss_D: 0.2035 Loss_G: 1.7497\n===> Epoch[228](2/88): Loss_D: 0.2174 Loss_G: 1.4765\n===> Epoch[228](3/88): Loss_D: 0.2409 Loss_G: 1.1555\n===> Epoch[228](4/88): Loss_D: 0.2551 Loss_G: 1.0543\n===> Epoch[228](5/88): Loss_D: 0.2576 Loss_G: 1.1857\n===> Epoch[228](6/88): Loss_D: 0.2343 Loss_G: 1.4162\n===> Epoch[228](7/88): Loss_D: 0.2485 Loss_G: 1.1445\n===> Epoch[228](8/88): Loss_D: 0.2370 Loss_G: 1.4157\n===> Epoch[228](9/88): Loss_D: 0.2492 Loss_G: 1.1733\n===> Epoch[228](10/88): Loss_D: 0.2375 Loss_G: 1.3154\n===> Epoch[228](11/88): Loss_D: 0.2534 Loss_G: 1.0410\n===> Epoch[228](12/88): Loss_D: 0.2469 Loss_G: 1.2056\n===> Epoch[228](13/88): Loss_D: 0.2482 Loss_G: 1.1743\n===> Epoch[228](14/88): Loss_D: 0.2440 Loss_G: 1.2320\n===> Epoch[228](15/88): Loss_D: 0.2411 Loss_G: 1.2652\n===> Epoch[228](16/88): Loss_D: 0.2543 Loss_G: 1.2885\n===> Epoch[228](17/88): Loss_D: 0.1669 Loss_G: 1.5602\n===> Epoch[228](18/88): Loss_D: 0.2108 Loss_G: 2.1714\n===> Epoch[228](19/88): Loss_D: 0.2529 Loss_G: 1.0857\n===> Epoch[228](20/88): Loss_D: 0.2432 Loss_G: 1.3646\n===> Epoch[228](21/88): Loss_D: 0.2513 Loss_G: 1.1797\n===> Epoch[228](22/88): Loss_D: 0.2273 Loss_G: 1.3211\n===> Epoch[228](23/88): Loss_D: 0.2424 Loss_G: 1.2293\n===> Epoch[228](24/88): Loss_D: 0.2182 Loss_G: 1.4021\n===> Epoch[228](25/88): Loss_D: 0.2589 Loss_G: 1.2131\n===> Epoch[228](26/88): Loss_D: 0.2649 Loss_G: 1.0825\n===> Epoch[228](27/88): Loss_D: 0.2497 Loss_G: 1.3551\n===> Epoch[228](28/88): Loss_D: 0.2557 Loss_G: 1.1089\n===> Epoch[228](29/88): Loss_D: 0.2387 Loss_G: 1.0830\n===> Epoch[228](30/88): Loss_D: 0.2456 Loss_G: 1.1678\n===> Epoch[228](31/88): Loss_D: 0.2546 Loss_G: 1.1455\n===> Epoch[228](32/88): Loss_D: 0.2503 Loss_G: 1.1791\n===> Epoch[228](33/88): Loss_D: 0.2403 Loss_G: 1.1661\n===> Epoch[228](34/88): Loss_D: 0.2570 Loss_G: 1.1072\n===> Epoch[228](35/88): Loss_D: 0.2295 Loss_G: 1.4769\n===> Epoch[228](36/88): Loss_D: 0.2680 Loss_G: 1.0428\n===> Epoch[228](37/88): Loss_D: 0.2311 Loss_G: 1.2026\n===> Epoch[228](38/88): Loss_D: 0.2492 Loss_G: 1.0631\n===> Epoch[228](39/88): Loss_D: 0.2489 Loss_G: 1.2880\n===> Epoch[228](40/88): Loss_D: 0.2513 Loss_G: 1.2537\n===> Epoch[228](41/88): Loss_D: 0.2510 Loss_G: 1.0097\n===> Epoch[228](42/88): Loss_D: 0.2397 Loss_G: 1.2571\n===> Epoch[228](43/88): Loss_D: 0.2459 Loss_G: 1.2384\n===> Epoch[228](44/88): Loss_D: 0.2543 Loss_G: 1.1046\n===> Epoch[228](45/88): Loss_D: 0.2570 Loss_G: 1.0777\n===> Epoch[228](46/88): Loss_D: 0.2308 Loss_G: 1.1610\n===> Epoch[228](47/88): Loss_D: 0.2361 Loss_G: 1.3050\n===> Epoch[228](48/88): Loss_D: 0.2519 Loss_G: 1.0958\n===> Epoch[228](49/88): Loss_D: 0.2623 Loss_G: 0.9543\n===> Epoch[228](50/88): Loss_D: 0.2520 Loss_G: 1.1746\n===> Epoch[228](51/88): Loss_D: 0.2500 Loss_G: 1.0910\n===> Epoch[228](52/88): Loss_D: 0.2413 Loss_G: 1.2502\n===> Epoch[228](53/88): Loss_D: 0.2344 Loss_G: 1.3065\n===> Epoch[228](54/88): Loss_D: 0.2541 Loss_G: 1.2115\n===> Epoch[228](55/88): Loss_D: 0.2445 Loss_G: 1.3674\n===> Epoch[228](56/88): Loss_D: 0.2428 Loss_G: 1.2568\n===> Epoch[228](57/88): Loss_D: 0.2061 Loss_G: 1.4824\n===> Epoch[228](58/88): Loss_D: 0.2426 Loss_G: 1.1983\n===> Epoch[228](59/88): Loss_D: 0.2618 Loss_G: 1.2237\n===> Epoch[228](60/88): Loss_D: 0.2491 Loss_G: 1.1744\n===> Epoch[228](61/88): Loss_D: 0.2555 Loss_G: 1.0993\n===> Epoch[228](62/88): Loss_D: 0.2462 Loss_G: 1.1822\n===> Epoch[228](63/88): Loss_D: 0.2355 Loss_G: 1.1743\n===> Epoch[228](64/88): Loss_D: 0.2026 Loss_G: 1.4275\n===> Epoch[228](65/88): Loss_D: 0.2365 Loss_G: 1.3842\n===> Epoch[228](66/88): Loss_D: 0.2288 Loss_G: 1.3372\n===> Epoch[228](67/88): Loss_D: 0.2503 Loss_G: 1.2519\n===> Epoch[228](68/88): Loss_D: 0.2273 Loss_G: 1.2706\n===> Epoch[228](69/88): Loss_D: 0.2420 Loss_G: 1.2943\n===> Epoch[228](70/88): Loss_D: 0.2534 Loss_G: 1.0747\n===> Epoch[228](71/88): Loss_D: 0.2518 Loss_G: 1.2757\n===> Epoch[228](72/88): Loss_D: 0.2476 Loss_G: 1.1794\n===> Epoch[228](73/88): Loss_D: 0.2455 Loss_G: 1.2822\n===> Epoch[228](74/88): Loss_D: 0.2427 Loss_G: 1.0718\n===> Epoch[228](75/88): Loss_D: 0.2436 Loss_G: 1.1653\n===> Epoch[228](76/88): Loss_D: 0.2085 Loss_G: 1.3983\n===> Epoch[228](77/88): Loss_D: 0.2537 Loss_G: 1.1652\n===> Epoch[228](78/88): Loss_D: 0.2423 Loss_G: 1.4080\n===> Epoch[228](79/88): Loss_D: 0.1688 Loss_G: 1.5252\n===> Epoch[228](80/88): Loss_D: 0.2517 Loss_G: 1.0290\n===> Epoch[228](81/88): Loss_D: 0.1997 Loss_G: 1.3807\n===> Epoch[228](82/88): Loss_D: 0.1842 Loss_G: 1.5633\n===> Epoch[228](83/88): Loss_D: 0.2601 Loss_G: 1.1887\n===> Epoch[228](84/88): Loss_D: 0.2452 Loss_G: 1.3570\n===> Epoch[228](85/88): Loss_D: 0.2582 Loss_G: 1.2252\n===> Epoch[228](86/88): Loss_D: 0.2321 Loss_G: 1.2500\n===> Epoch[228](87/88): Loss_D: 0.2513 Loss_G: 1.0177\n===> Epoch[228](88/88): Loss_D: 0.2257 Loss_G: 1.2700\nlearning rate = 0.0000431\nlearning rate = 0.0000431\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0024} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[229](1/88): Loss_D: 0.2322 Loss_G: 1.3148\n===> Epoch[229](2/88): Loss_D: 0.2541 Loss_G: 1.1198\n===> Epoch[229](3/88): Loss_D: 0.2296 Loss_G: 1.2865\n===> Epoch[229](4/88): Loss_D: 0.2422 Loss_G: 1.1119\n===> Epoch[229](5/88): Loss_D: 0.2695 Loss_G: 1.0980\n===> Epoch[229](6/88): Loss_D: 0.2460 Loss_G: 1.1799\n===> Epoch[229](7/88): Loss_D: 0.2457 Loss_G: 1.2603\n===> Epoch[229](8/88): Loss_D: 0.1719 Loss_G: 1.5430\n===> Epoch[229](9/88): Loss_D: 0.2362 Loss_G: 1.2178\n===> Epoch[229](10/88): Loss_D: 0.2440 Loss_G: 1.1136\n===> Epoch[229](11/88): Loss_D: 0.2695 Loss_G: 1.0940\n===> Epoch[229](12/88): Loss_D: 0.2635 Loss_G: 1.1191\n===> Epoch[229](13/88): Loss_D: 0.2009 Loss_G: 1.4025\n===> Epoch[229](14/88): Loss_D: 0.2152 Loss_G: 2.2178\n===> Epoch[229](15/88): Loss_D: 0.2617 Loss_G: 1.1796\n===> Epoch[229](16/88): Loss_D: 0.2482 Loss_G: 1.3491\n===> Epoch[229](17/88): Loss_D: 0.1937 Loss_G: 1.5767\n===> Epoch[229](18/88): Loss_D: 0.2283 Loss_G: 1.3373\n===> Epoch[229](19/88): Loss_D: 0.2565 Loss_G: 1.2242\n===> Epoch[229](20/88): Loss_D: 0.2398 Loss_G: 1.4525\n===> Epoch[229](21/88): Loss_D: 0.2449 Loss_G: 1.2832\n===> Epoch[229](22/88): Loss_D: 0.2417 Loss_G: 1.0755\n===> Epoch[229](23/88): Loss_D: 0.2415 Loss_G: 1.2440\n===> Epoch[229](24/88): Loss_D: 0.2560 Loss_G: 1.2323\n===> Epoch[229](25/88): Loss_D: 0.2410 Loss_G: 1.3187\n===> Epoch[229](26/88): Loss_D: 0.2464 Loss_G: 1.1581\n===> Epoch[229](27/88): Loss_D: 0.2533 Loss_G: 1.3770\n===> Epoch[229](28/88): Loss_D: 0.2453 Loss_G: 1.1715\n===> Epoch[229](29/88): Loss_D: 0.2355 Loss_G: 1.1122\n===> Epoch[229](30/88): Loss_D: 0.2518 Loss_G: 1.2442\n===> Epoch[229](31/88): Loss_D: 0.2392 Loss_G: 1.1652\n===> Epoch[229](32/88): Loss_D: 0.2508 Loss_G: 1.1495\n===> Epoch[229](33/88): Loss_D: 0.1939 Loss_G: 1.8101\n===> Epoch[229](34/88): Loss_D: 0.2450 Loss_G: 1.2449\n===> Epoch[229](35/88): Loss_D: 0.2553 Loss_G: 0.9926\n===> Epoch[229](36/88): Loss_D: 0.2628 Loss_G: 0.9563\n===> Epoch[229](37/88): Loss_D: 0.2570 Loss_G: 0.8990\n===> Epoch[229](38/88): Loss_D: 0.2324 Loss_G: 1.0726\n===> Epoch[229](39/88): Loss_D: 0.2516 Loss_G: 1.0772\n===> Epoch[229](40/88): Loss_D: 0.2731 Loss_G: 1.0430\n===> Epoch[229](41/88): Loss_D: 0.2425 Loss_G: 1.2533\n===> Epoch[229](42/88): Loss_D: 0.2500 Loss_G: 1.3992\n===> Epoch[229](43/88): Loss_D: 0.2559 Loss_G: 1.1864\n===> Epoch[229](44/88): Loss_D: 0.2510 Loss_G: 1.2418\n===> Epoch[229](45/88): Loss_D: 0.2485 Loss_G: 1.1373\n===> Epoch[229](46/88): Loss_D: 0.2538 Loss_G: 1.2028\n===> Epoch[229](47/88): Loss_D: 0.2622 Loss_G: 1.0891\n===> Epoch[229](48/88): Loss_D: 0.2424 Loss_G: 1.3401\n===> Epoch[229](49/88): Loss_D: 0.2494 Loss_G: 1.1389\n===> Epoch[229](50/88): Loss_D: 0.2466 Loss_G: 1.1917\n===> Epoch[229](51/88): Loss_D: 0.2573 Loss_G: 1.1968\n===> Epoch[229](52/88): Loss_D: 0.2286 Loss_G: 1.3945\n===> Epoch[229](53/88): Loss_D: 0.2492 Loss_G: 1.1336\n===> Epoch[229](54/88): Loss_D: 0.2417 Loss_G: 1.1918\n===> Epoch[229](55/88): Loss_D: 0.2493 Loss_G: 1.0975\n===> Epoch[229](56/88): Loss_D: 0.2445 Loss_G: 1.1722\n===> Epoch[229](57/88): Loss_D: 0.1731 Loss_G: 1.5732\n===> Epoch[229](58/88): Loss_D: 0.2416 Loss_G: 1.1171\n===> Epoch[229](59/88): Loss_D: 0.2368 Loss_G: 1.4020\n===> Epoch[229](60/88): Loss_D: 0.2521 Loss_G: 1.1465\n===> Epoch[229](61/88): Loss_D: 0.2505 Loss_G: 1.2195\n===> Epoch[229](62/88): Loss_D: 0.2596 Loss_G: 1.2117\n===> Epoch[229](63/88): Loss_D: 0.2366 Loss_G: 1.2847\n===> Epoch[229](64/88): Loss_D: 0.2411 Loss_G: 1.1365\n===> Epoch[229](65/88): Loss_D: 0.2249 Loss_G: 1.2733\n===> Epoch[229](66/88): Loss_D: 0.2440 Loss_G: 1.2226\n===> Epoch[229](67/88): Loss_D: 0.2283 Loss_G: 1.2857\n===> Epoch[229](68/88): Loss_D: 0.2399 Loss_G: 1.3136\n===> Epoch[229](69/88): Loss_D: 0.2580 Loss_G: 1.0229\n===> Epoch[229](70/88): Loss_D: 0.2109 Loss_G: 1.4599\n===> Epoch[229](71/88): Loss_D: 0.2197 Loss_G: 1.4256\n===> Epoch[229](72/88): Loss_D: 0.2187 Loss_G: 1.3700\n===> Epoch[229](73/88): Loss_D: 0.2605 Loss_G: 1.1840\n===> Epoch[229](74/88): Loss_D: 0.2414 Loss_G: 1.4845\n===> Epoch[229](75/88): Loss_D: 0.2422 Loss_G: 1.2592\n===> Epoch[229](76/88): Loss_D: 0.2514 Loss_G: 1.4326\n===> Epoch[229](77/88): Loss_D: 0.2655 Loss_G: 1.0809\n===> Epoch[229](78/88): Loss_D: 0.2329 Loss_G: 1.2172\n===> Epoch[229](79/88): Loss_D: 0.2475 Loss_G: 1.2115\n===> Epoch[229](80/88): Loss_D: 0.2503 Loss_G: 1.2728\n===> Epoch[229](81/88): Loss_D: 0.2397 Loss_G: 1.2535\n===> Epoch[229](82/88): Loss_D: 0.2544 Loss_G: 1.2465\n===> Epoch[229](83/88): Loss_D: 0.2369 Loss_G: 1.3120\n===> Epoch[229](84/88): Loss_D: 0.2418 Loss_G: 1.1148\n===> Epoch[229](85/88): Loss_D: 0.2527 Loss_G: 1.0250\n===> Epoch[229](86/88): Loss_D: 0.2360 Loss_G: 1.2741\n===> Epoch[229](87/88): Loss_D: 0.2498 Loss_G: 1.2809\n===> Epoch[229](88/88): Loss_D: 0.2453 Loss_G: 1.2070\nlearning rate = 0.0000429\nlearning rate = 0.0000429\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0001} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[230](1/88): Loss_D: 0.2448 Loss_G: 1.0416\n===> Epoch[230](2/88): Loss_D: 0.1844 Loss_G: 1.5608\n===> Epoch[230](3/88): Loss_D: 0.2567 Loss_G: 1.1487\n===> Epoch[230](4/88): Loss_D: 0.2530 Loss_G: 1.0657\n===> Epoch[230](5/88): Loss_D: 0.2474 Loss_G: 1.1774\n===> Epoch[230](6/88): Loss_D: 0.2540 Loss_G: 1.2564\n===> Epoch[230](7/88): Loss_D: 0.2288 Loss_G: 1.1154\n===> Epoch[230](8/88): Loss_D: 0.2490 Loss_G: 1.3668\n===> Epoch[230](9/88): Loss_D: 0.2516 Loss_G: 1.2318\n===> Epoch[230](10/88): Loss_D: 0.2545 Loss_G: 1.1134\n===> Epoch[230](11/88): Loss_D: 0.2377 Loss_G: 1.3296\n===> Epoch[230](12/88): Loss_D: 0.2369 Loss_G: 1.2233\n===> Epoch[230](13/88): Loss_D: 0.2570 Loss_G: 1.2500\n===> Epoch[230](14/88): Loss_D: 0.2527 Loss_G: 1.1964\n===> Epoch[230](15/88): Loss_D: 0.2371 Loss_G: 1.3094\n===> Epoch[230](16/88): Loss_D: 0.2619 Loss_G: 1.0704\n===> Epoch[230](17/88): Loss_D: 0.2272 Loss_G: 1.3370\n===> Epoch[230](18/88): Loss_D: 0.2364 Loss_G: 1.4152\n===> Epoch[230](19/88): Loss_D: 0.2467 Loss_G: 0.8934\n===> Epoch[230](20/88): Loss_D: 0.2568 Loss_G: 1.1503\n===> Epoch[230](21/88): Loss_D: 0.2294 Loss_G: 1.1588\n===> Epoch[230](22/88): Loss_D: 0.2429 Loss_G: 1.1142\n===> Epoch[230](23/88): Loss_D: 0.2513 Loss_G: 1.0393\n===> Epoch[230](24/88): Loss_D: 0.2061 Loss_G: 2.2013\n===> Epoch[230](25/88): Loss_D: 0.2523 Loss_G: 1.1708\n===> Epoch[230](26/88): Loss_D: 0.2507 Loss_G: 1.1739\n===> Epoch[230](27/88): Loss_D: 0.2405 Loss_G: 1.2239\n===> Epoch[230](28/88): Loss_D: 0.2530 Loss_G: 1.2076\n===> Epoch[230](29/88): Loss_D: 0.2481 Loss_G: 1.1729\n===> Epoch[230](30/88): Loss_D: 0.2137 Loss_G: 1.5361\n===> Epoch[230](31/88): Loss_D: 0.2377 Loss_G: 1.1639\n===> Epoch[230](32/88): Loss_D: 0.2610 Loss_G: 1.1574\n===> Epoch[230](33/88): Loss_D: 0.2602 Loss_G: 1.0356\n===> Epoch[230](34/88): Loss_D: 0.2274 Loss_G: 1.5387\n===> Epoch[230](35/88): Loss_D: 0.2541 Loss_G: 1.0854\n===> Epoch[230](36/88): Loss_D: 0.2483 Loss_G: 1.1538\n===> Epoch[230](37/88): Loss_D: 0.2443 Loss_G: 1.2504\n===> Epoch[230](38/88): Loss_D: 0.2433 Loss_G: 1.4009\n===> Epoch[230](39/88): Loss_D: 0.2449 Loss_G: 1.1309\n===> Epoch[230](40/88): Loss_D: 0.2398 Loss_G: 1.1582\n===> Epoch[230](41/88): Loss_D: 0.1822 Loss_G: 1.6209\n===> Epoch[230](42/88): Loss_D: 0.2331 Loss_G: 1.3411\n===> Epoch[230](43/88): Loss_D: 0.2415 Loss_G: 1.0967\n===> Epoch[230](44/88): Loss_D: 0.2479 Loss_G: 1.1077\n===> Epoch[230](45/88): Loss_D: 0.2545 Loss_G: 0.9760\n===> Epoch[230](46/88): Loss_D: 0.2160 Loss_G: 1.3318\n===> Epoch[230](47/88): Loss_D: 0.2344 Loss_G: 1.3377\n===> Epoch[230](48/88): Loss_D: 0.2576 Loss_G: 1.1792\n===> Epoch[230](49/88): Loss_D: 0.2633 Loss_G: 1.1270\n===> Epoch[230](50/88): Loss_D: 0.2469 Loss_G: 1.2576\n===> Epoch[230](51/88): Loss_D: 0.2497 Loss_G: 1.1393\n===> Epoch[230](52/88): Loss_D: 0.2436 Loss_G: 1.2500\n===> Epoch[230](53/88): Loss_D: 0.2346 Loss_G: 1.2957\n===> Epoch[230](54/88): Loss_D: 0.2455 Loss_G: 1.2732\n===> Epoch[230](55/88): Loss_D: 0.2485 Loss_G: 1.0639\n===> Epoch[230](56/88): Loss_D: 0.2327 Loss_G: 1.1723\n===> Epoch[230](57/88): Loss_D: 0.2469 Loss_G: 1.1034\n===> Epoch[230](58/88): Loss_D: 0.2445 Loss_G: 1.2766\n===> Epoch[230](59/88): Loss_D: 0.2391 Loss_G: 1.1924\n===> Epoch[230](60/88): Loss_D: 0.2351 Loss_G: 1.3824\n===> Epoch[230](61/88): Loss_D: 0.2398 Loss_G: 1.3448\n===> Epoch[230](62/88): Loss_D: 0.2259 Loss_G: 1.3114\n===> Epoch[230](63/88): Loss_D: 0.2509 Loss_G: 1.2052\n===> Epoch[230](64/88): Loss_D: 0.2538 Loss_G: 1.1216\n===> Epoch[230](65/88): Loss_D: 0.2166 Loss_G: 1.3658\n===> Epoch[230](66/88): Loss_D: 0.2549 Loss_G: 1.2206\n===> Epoch[230](67/88): Loss_D: 0.2523 Loss_G: 1.0062\n===> Epoch[230](68/88): Loss_D: 0.2549 Loss_G: 1.1524\n===> Epoch[230](69/88): Loss_D: 0.2254 Loss_G: 1.2395\n===> Epoch[230](70/88): Loss_D: 0.2001 Loss_G: 1.7893\n===> Epoch[230](71/88): Loss_D: 0.2519 Loss_G: 1.2196\n===> Epoch[230](72/88): Loss_D: 0.2319 Loss_G: 1.4248\n===> Epoch[230](73/88): Loss_D: 0.2488 Loss_G: 1.2022\n===> Epoch[230](74/88): Loss_D: 0.2563 Loss_G: 1.1635\n===> Epoch[230](75/88): Loss_D: 0.2562 Loss_G: 1.2067\n===> Epoch[230](76/88): Loss_D: 0.1759 Loss_G: 1.5289\n===> Epoch[230](77/88): Loss_D: 0.1916 Loss_G: 1.6033\n===> Epoch[230](78/88): Loss_D: 0.2533 Loss_G: 1.4164\n===> Epoch[230](79/88): Loss_D: 0.2461 Loss_G: 1.3399\n===> Epoch[230](80/88): Loss_D: 0.2521 Loss_G: 1.2500\n===> Epoch[230](81/88): Loss_D: 0.2532 Loss_G: 1.0926\n===> Epoch[230](82/88): Loss_D: 0.2654 Loss_G: 1.0243\n===> Epoch[230](83/88): Loss_D: 0.2537 Loss_G: 1.2273\n===> Epoch[230](84/88): Loss_D: 0.2284 Loss_G: 1.2681\n===> Epoch[230](85/88): Loss_D: 0.2386 Loss_G: 1.2839\n===> Epoch[230](86/88): Loss_D: 0.2411 Loss_G: 1.2852\n===> Epoch[230](87/88): Loss_D: 0.2518 Loss_G: 1.0634\n===> Epoch[230](88/88): Loss_D: 0.2366 Loss_G: 1.1096\nlearning rate = 0.0000426\nlearning rate = 0.0000426\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0075} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[231](1/88): Loss_D: 0.2325 Loss_G: 1.4390\n===> Epoch[231](2/88): Loss_D: 0.2118 Loss_G: 1.3297\n===> Epoch[231](3/88): Loss_D: 0.2318 Loss_G: 1.1086\n===> Epoch[231](4/88): Loss_D: 0.2368 Loss_G: 1.1212\n===> Epoch[231](5/88): Loss_D: 0.2585 Loss_G: 1.2407\n===> Epoch[231](6/88): Loss_D: 0.1824 Loss_G: 1.4699\n===> Epoch[231](7/88): Loss_D: 0.2522 Loss_G: 1.0792\n===> Epoch[231](8/88): Loss_D: 0.2556 Loss_G: 1.1819\n===> Epoch[231](9/88): Loss_D: 0.2464 Loss_G: 1.1028\n===> Epoch[231](10/88): Loss_D: 0.2501 Loss_G: 1.0798\n===> Epoch[231](11/88): Loss_D: 0.2456 Loss_G: 1.1906\n===> Epoch[231](12/88): Loss_D: 0.2590 Loss_G: 1.2961\n===> Epoch[231](13/88): Loss_D: 0.2294 Loss_G: 1.3178\n===> Epoch[231](14/88): Loss_D: 0.2500 Loss_G: 1.1449\n===> Epoch[231](15/88): Loss_D: 0.2560 Loss_G: 1.1228\n===> Epoch[231](16/88): Loss_D: 0.2525 Loss_G: 1.0747\n===> Epoch[231](17/88): Loss_D: 0.2416 Loss_G: 1.2767\n===> Epoch[231](18/88): Loss_D: 0.2289 Loss_G: 1.2601\n===> Epoch[231](19/88): Loss_D: 0.2529 Loss_G: 1.0134\n===> Epoch[231](20/88): Loss_D: 0.2354 Loss_G: 1.4102\n===> Epoch[231](21/88): Loss_D: 0.2638 Loss_G: 1.0744\n===> Epoch[231](22/88): Loss_D: 0.2460 Loss_G: 1.1390\n===> Epoch[231](23/88): Loss_D: 0.2417 Loss_G: 1.2682\n===> Epoch[231](24/88): Loss_D: 0.2449 Loss_G: 1.3249\n===> Epoch[231](25/88): Loss_D: 0.2394 Loss_G: 1.4507\n===> Epoch[231](26/88): Loss_D: 0.2498 Loss_G: 1.0885\n===> Epoch[231](27/88): Loss_D: 0.1881 Loss_G: 1.6366\n===> Epoch[231](28/88): Loss_D: 0.2450 Loss_G: 1.0922\n===> Epoch[231](29/88): Loss_D: 0.2523 Loss_G: 1.1294\n===> Epoch[231](30/88): Loss_D: 0.1856 Loss_G: 1.7647\n===> Epoch[231](31/88): Loss_D: 0.2413 Loss_G: 1.2373\n===> Epoch[231](32/88): Loss_D: 0.2416 Loss_G: 1.2277\n===> Epoch[231](33/88): Loss_D: 0.2110 Loss_G: 1.4087\n===> Epoch[231](34/88): Loss_D: 0.2522 Loss_G: 1.1509\n===> Epoch[231](35/88): Loss_D: 0.2547 Loss_G: 1.1383\n===> Epoch[231](36/88): Loss_D: 0.2387 Loss_G: 1.4009\n===> Epoch[231](37/88): Loss_D: 0.2597 Loss_G: 1.2690\n===> Epoch[231](38/88): Loss_D: 0.2458 Loss_G: 1.1881\n===> Epoch[231](39/88): Loss_D: 0.2538 Loss_G: 1.0531\n===> Epoch[231](40/88): Loss_D: 0.2419 Loss_G: 1.1311\n===> Epoch[231](41/88): Loss_D: 0.2336 Loss_G: 1.4397\n===> Epoch[231](42/88): Loss_D: 0.2571 Loss_G: 1.2055\n===> Epoch[231](43/88): Loss_D: 0.2386 Loss_G: 1.2526\n===> Epoch[231](44/88): Loss_D: 0.2472 Loss_G: 1.2806\n===> Epoch[231](45/88): Loss_D: 0.2352 Loss_G: 1.1267\n===> Epoch[231](46/88): Loss_D: 0.2517 Loss_G: 1.1985\n===> Epoch[231](47/88): Loss_D: 0.2523 Loss_G: 1.2331\n===> Epoch[231](48/88): Loss_D: 0.2671 Loss_G: 1.1723\n===> Epoch[231](49/88): Loss_D: 0.2405 Loss_G: 1.1906\n===> Epoch[231](50/88): Loss_D: 0.2388 Loss_G: 1.2636\n===> Epoch[231](51/88): Loss_D: 0.2119 Loss_G: 1.5032\n===> Epoch[231](52/88): Loss_D: 0.2510 Loss_G: 1.2889\n===> Epoch[231](53/88): Loss_D: 0.2644 Loss_G: 1.1381\n===> Epoch[231](54/88): Loss_D: 0.2474 Loss_G: 0.9423\n===> Epoch[231](55/88): Loss_D: 0.2566 Loss_G: 1.0701\n===> Epoch[231](56/88): Loss_D: 0.2583 Loss_G: 1.1087\n===> Epoch[231](57/88): Loss_D: 0.2384 Loss_G: 1.3624\n===> Epoch[231](58/88): Loss_D: 0.2561 Loss_G: 1.1647\n===> Epoch[231](59/88): Loss_D: 0.2353 Loss_G: 1.4096\n===> Epoch[231](60/88): Loss_D: 0.2419 Loss_G: 1.2731\n===> Epoch[231](61/88): Loss_D: 0.2464 Loss_G: 1.2184\n===> Epoch[231](62/88): Loss_D: 0.2359 Loss_G: 1.3950\n===> Epoch[231](63/88): Loss_D: 0.2426 Loss_G: 1.2109\n===> Epoch[231](64/88): Loss_D: 0.2252 Loss_G: 1.3887\n===> Epoch[231](65/88): Loss_D: 0.2443 Loss_G: 1.2077\n===> Epoch[231](66/88): Loss_D: 0.2565 Loss_G: 1.1398\n===> Epoch[231](67/88): Loss_D: 0.2495 Loss_G: 1.2713\n===> Epoch[231](68/88): Loss_D: 0.2503 Loss_G: 1.1529\n===> Epoch[231](69/88): Loss_D: 0.2203 Loss_G: 1.4039\n===> Epoch[231](70/88): Loss_D: 0.2469 Loss_G: 1.1734\n===> Epoch[231](71/88): Loss_D: 0.2464 Loss_G: 1.0291\n===> Epoch[231](72/88): Loss_D: 0.1664 Loss_G: 1.5007\n===> Epoch[231](73/88): Loss_D: 0.2482 Loss_G: 1.2686\n===> Epoch[231](74/88): Loss_D: 0.2277 Loss_G: 1.1298\n===> Epoch[231](75/88): Loss_D: 0.2579 Loss_G: 1.1296\n===> Epoch[231](76/88): Loss_D: 0.2545 Loss_G: 1.2238\n===> Epoch[231](77/88): Loss_D: 0.2429 Loss_G: 1.1525\n===> Epoch[231](78/88): Loss_D: 0.2193 Loss_G: 1.3983\n===> Epoch[231](79/88): Loss_D: 0.2350 Loss_G: 1.3590\n===> Epoch[231](80/88): Loss_D: 0.2494 Loss_G: 1.2466\n===> Epoch[231](81/88): Loss_D: 0.2077 Loss_G: 2.1100\n===> Epoch[231](82/88): Loss_D: 0.2394 Loss_G: 1.2682\n===> Epoch[231](83/88): Loss_D: 0.2477 Loss_G: 1.3076\n===> Epoch[231](84/88): Loss_D: 0.2469 Loss_G: 1.2157\n===> Epoch[231](85/88): Loss_D: 0.2444 Loss_G: 1.2039\n===> Epoch[231](86/88): Loss_D: 0.2673 Loss_G: 1.0670\n===> Epoch[231](87/88): Loss_D: 0.2518 Loss_G: 1.1740\n===> Epoch[231](88/88): Loss_D: 0.2485 Loss_G: 0.9673\nlearning rate = 0.0000424\nlearning rate = 0.0000424\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0037} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[232](1/88): Loss_D: 0.2525 Loss_G: 1.0639\n===> Epoch[232](2/88): Loss_D: 0.2161 Loss_G: 1.4155\n===> Epoch[232](3/88): Loss_D: 0.1764 Loss_G: 1.5421\n===> Epoch[232](4/88): Loss_D: 0.2538 Loss_G: 1.1549\n===> Epoch[232](5/88): Loss_D: 0.2341 Loss_G: 1.2229\n===> Epoch[232](6/88): Loss_D: 0.2508 Loss_G: 1.1494\n===> Epoch[232](7/88): Loss_D: 0.2390 Loss_G: 1.3995\n===> Epoch[232](8/88): Loss_D: 0.2468 Loss_G: 1.4356\n===> Epoch[232](9/88): Loss_D: 0.2476 Loss_G: 1.0754\n===> Epoch[232](10/88): Loss_D: 0.2615 Loss_G: 1.0069\n===> Epoch[232](11/88): Loss_D: 0.2432 Loss_G: 1.0456\n===> Epoch[232](12/88): Loss_D: 0.2374 Loss_G: 1.0620\n===> Epoch[232](13/88): Loss_D: 0.2356 Loss_G: 1.4989\n===> Epoch[232](14/88): Loss_D: 0.2530 Loss_G: 1.2732\n===> Epoch[232](15/88): Loss_D: 0.2326 Loss_G: 1.2839\n===> Epoch[232](16/88): Loss_D: 0.2496 Loss_G: 1.1836\n===> Epoch[232](17/88): Loss_D: 0.2584 Loss_G: 0.9642\n===> Epoch[232](18/88): Loss_D: 0.2484 Loss_G: 1.1482\n===> Epoch[232](19/88): Loss_D: 0.2050 Loss_G: 1.5247\n===> Epoch[232](20/88): Loss_D: 0.2374 Loss_G: 1.1690\n===> Epoch[232](21/88): Loss_D: 0.2514 Loss_G: 1.1841\n===> Epoch[232](22/88): Loss_D: 0.2531 Loss_G: 1.2760\n===> Epoch[232](23/88): Loss_D: 0.2580 Loss_G: 1.0960\n===> Epoch[232](24/88): Loss_D: 0.2479 Loss_G: 1.1637\n===> Epoch[232](25/88): Loss_D: 0.1904 Loss_G: 1.6593\n===> Epoch[232](26/88): Loss_D: 0.2286 Loss_G: 1.1126\n===> Epoch[232](27/88): Loss_D: 0.2540 Loss_G: 1.1841\n===> Epoch[232](28/88): Loss_D: 0.2270 Loss_G: 1.2285\n===> Epoch[232](29/88): Loss_D: 0.2515 Loss_G: 1.2819\n===> Epoch[232](30/88): Loss_D: 0.2558 Loss_G: 0.9843\n===> Epoch[232](31/88): Loss_D: 0.2432 Loss_G: 1.2291\n===> Epoch[232](32/88): Loss_D: 0.2378 Loss_G: 1.1623\n===> Epoch[232](33/88): Loss_D: 0.2025 Loss_G: 2.0966\n===> Epoch[232](34/88): Loss_D: 0.2476 Loss_G: 1.1949\n===> Epoch[232](35/88): Loss_D: 0.2433 Loss_G: 1.3416\n===> Epoch[232](36/88): Loss_D: 0.2586 Loss_G: 1.2243\n===> Epoch[232](37/88): Loss_D: 0.2408 Loss_G: 1.2194\n===> Epoch[232](38/88): Loss_D: 0.2381 Loss_G: 1.3293\n===> Epoch[232](39/88): Loss_D: 0.2593 Loss_G: 1.2151\n===> Epoch[232](40/88): Loss_D: 0.2374 Loss_G: 1.3515\n===> Epoch[232](41/88): Loss_D: 0.2418 Loss_G: 1.1048\n===> Epoch[232](42/88): Loss_D: 0.2531 Loss_G: 1.2137\n===> Epoch[232](43/88): Loss_D: 0.2397 Loss_G: 1.2674\n===> Epoch[232](44/88): Loss_D: 0.2401 Loss_G: 1.3937\n===> Epoch[232](45/88): Loss_D: 0.2347 Loss_G: 1.1290\n===> Epoch[232](46/88): Loss_D: 0.1806 Loss_G: 1.4664\n===> Epoch[232](47/88): Loss_D: 0.2490 Loss_G: 1.2474\n===> Epoch[232](48/88): Loss_D: 0.2750 Loss_G: 1.1483\n===> Epoch[232](49/88): Loss_D: 0.2480 Loss_G: 1.2370\n===> Epoch[232](50/88): Loss_D: 0.2491 Loss_G: 1.1709\n===> Epoch[232](51/88): Loss_D: 0.2384 Loss_G: 1.1637\n===> Epoch[232](52/88): Loss_D: 0.2512 Loss_G: 1.1997\n===> Epoch[232](53/88): Loss_D: 0.2483 Loss_G: 1.2156\n===> Epoch[232](54/88): Loss_D: 0.2495 Loss_G: 1.0476\n===> Epoch[232](55/88): Loss_D: 0.2552 Loss_G: 1.0818\n===> Epoch[232](56/88): Loss_D: 0.2420 Loss_G: 1.1542\n===> Epoch[232](57/88): Loss_D: 0.2346 Loss_G: 1.3878\n===> Epoch[232](58/88): Loss_D: 0.2392 Loss_G: 1.3807\n===> Epoch[232](59/88): Loss_D: 0.2444 Loss_G: 1.2825\n===> Epoch[232](60/88): Loss_D: 0.2515 Loss_G: 1.2113\n===> Epoch[232](61/88): Loss_D: 0.2439 Loss_G: 0.9837\n===> Epoch[232](62/88): Loss_D: 0.2415 Loss_G: 1.2066\n===> Epoch[232](63/88): Loss_D: 0.2555 Loss_G: 1.0727\n===> Epoch[232](64/88): Loss_D: 0.2457 Loss_G: 1.2174\n===> Epoch[232](65/88): Loss_D: 0.2267 Loss_G: 1.3753\n===> Epoch[232](66/88): Loss_D: 0.2417 Loss_G: 1.2665\n===> Epoch[232](67/88): Loss_D: 0.2607 Loss_G: 0.9899\n===> Epoch[232](68/88): Loss_D: 0.2421 Loss_G: 1.2004\n===> Epoch[232](69/88): Loss_D: 0.2400 Loss_G: 1.2193\n===> Epoch[232](70/88): Loss_D: 0.2406 Loss_G: 1.3736\n===> Epoch[232](71/88): Loss_D: 0.2298 Loss_G: 1.3191\n===> Epoch[232](72/88): Loss_D: 0.2567 Loss_G: 1.0677\n===> Epoch[232](73/88): Loss_D: 0.2377 Loss_G: 1.1792\n===> Epoch[232](74/88): Loss_D: 0.2478 Loss_G: 1.0397\n===> Epoch[232](75/88): Loss_D: 0.2513 Loss_G: 1.2155\n===> Epoch[232](76/88): Loss_D: 0.2062 Loss_G: 1.7185\n===> Epoch[232](77/88): Loss_D: 0.2244 Loss_G: 1.3736\n===> Epoch[232](78/88): Loss_D: 0.2552 Loss_G: 1.1770\n===> Epoch[232](79/88): Loss_D: 0.2392 Loss_G: 1.1190\n===> Epoch[232](80/88): Loss_D: 0.2467 Loss_G: 1.0770\n===> Epoch[232](81/88): Loss_D: 0.2509 Loss_G: 1.1782\n===> Epoch[232](82/88): Loss_D: 0.2380 Loss_G: 1.3132\n===> Epoch[232](83/88): Loss_D: 0.2440 Loss_G: 1.2996\n===> Epoch[232](84/88): Loss_D: 0.2494 Loss_G: 1.1028\n===> Epoch[232](85/88): Loss_D: 0.2481 Loss_G: 1.1557\n===> Epoch[232](86/88): Loss_D: 0.2118 Loss_G: 1.4224\n===> Epoch[232](87/88): Loss_D: 0.2565 Loss_G: 1.1329\n===> Epoch[232](88/88): Loss_D: 0.2389 Loss_G: 1.2657\nlearning rate = 0.0000421\nlearning rate = 0.0000421\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0012} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[233](1/88): Loss_D: 0.2330 Loss_G: 1.1475\n===> Epoch[233](2/88): Loss_D: 0.2609 Loss_G: 1.0781\n===> Epoch[233](3/88): Loss_D: 0.2523 Loss_G: 1.1505\n===> Epoch[233](4/88): Loss_D: 0.2316 Loss_G: 1.1937\n===> Epoch[233](5/88): Loss_D: 0.2592 Loss_G: 1.1958\n===> Epoch[233](6/88): Loss_D: 0.2600 Loss_G: 1.1238\n===> Epoch[233](7/88): Loss_D: 0.1789 Loss_G: 1.5718\n===> Epoch[233](8/88): Loss_D: 0.2358 Loss_G: 1.2202\n===> Epoch[233](9/88): Loss_D: 0.2576 Loss_G: 1.1859\n===> Epoch[233](10/88): Loss_D: 0.2502 Loss_G: 1.0468\n===> Epoch[233](11/88): Loss_D: 0.2607 Loss_G: 1.1578\n===> Epoch[233](12/88): Loss_D: 0.2356 Loss_G: 1.3473\n===> Epoch[233](13/88): Loss_D: 0.2325 Loss_G: 1.2427\n===> Epoch[233](14/88): Loss_D: 0.2501 Loss_G: 1.1305\n===> Epoch[233](15/88): Loss_D: 0.2547 Loss_G: 1.1513\n===> Epoch[233](16/88): Loss_D: 0.2318 Loss_G: 1.3374\n===> Epoch[233](17/88): Loss_D: 0.2356 Loss_G: 1.6010\n===> Epoch[233](18/88): Loss_D: 0.2347 Loss_G: 1.2062\n===> Epoch[233](19/88): Loss_D: 0.2476 Loss_G: 1.1033\n===> Epoch[233](20/88): Loss_D: 0.2426 Loss_G: 1.2148\n===> Epoch[233](21/88): Loss_D: 0.2526 Loss_G: 1.2570\n===> Epoch[233](22/88): Loss_D: 0.2285 Loss_G: 1.4239\n===> Epoch[233](23/88): Loss_D: 0.2554 Loss_G: 1.1245\n===> Epoch[233](24/88): Loss_D: 0.2217 Loss_G: 1.3726\n===> Epoch[233](25/88): Loss_D: 0.1855 Loss_G: 1.4958\n===> Epoch[233](26/88): Loss_D: 0.2519 Loss_G: 1.0979\n===> Epoch[233](27/88): Loss_D: 0.2559 Loss_G: 1.1249\n===> Epoch[233](28/88): Loss_D: 0.2151 Loss_G: 1.3347\n===> Epoch[233](29/88): Loss_D: 0.2417 Loss_G: 1.2171\n===> Epoch[233](30/88): Loss_D: 0.1679 Loss_G: 1.5730\n===> Epoch[233](31/88): Loss_D: 0.2517 Loss_G: 1.1207\n===> Epoch[233](32/88): Loss_D: 0.2586 Loss_G: 1.2766\n===> Epoch[233](33/88): Loss_D: 0.2264 Loss_G: 1.3933\n===> Epoch[233](34/88): Loss_D: 0.2561 Loss_G: 1.1787\n===> Epoch[233](35/88): Loss_D: 0.2605 Loss_G: 0.9289\n===> Epoch[233](36/88): Loss_D: 0.2431 Loss_G: 1.1363\n===> Epoch[233](37/88): Loss_D: 0.2426 Loss_G: 1.1224\n===> Epoch[233](38/88): Loss_D: 0.2241 Loss_G: 1.3411\n===> Epoch[233](39/88): Loss_D: 0.2106 Loss_G: 2.2269\n===> Epoch[233](40/88): Loss_D: 0.2536 Loss_G: 1.1826\n===> Epoch[233](41/88): Loss_D: 0.2548 Loss_G: 1.1274\n===> Epoch[233](42/88): Loss_D: 0.2384 Loss_G: 1.3064\n===> Epoch[233](43/88): Loss_D: 0.2646 Loss_G: 0.9543\n===> Epoch[233](44/88): Loss_D: 0.2590 Loss_G: 1.2555\n===> Epoch[233](45/88): Loss_D: 0.2716 Loss_G: 1.1216\n===> Epoch[233](46/88): Loss_D: 0.2494 Loss_G: 1.2747\n===> Epoch[233](47/88): Loss_D: 0.2576 Loss_G: 1.0518\n===> Epoch[233](48/88): Loss_D: 0.2437 Loss_G: 1.1853\n===> Epoch[233](49/88): Loss_D: 0.2464 Loss_G: 1.1799\n===> Epoch[233](50/88): Loss_D: 0.2313 Loss_G: 1.0534\n===> Epoch[233](51/88): Loss_D: 0.2523 Loss_G: 1.2442\n===> Epoch[233](52/88): Loss_D: 0.2670 Loss_G: 1.0994\n===> Epoch[233](53/88): Loss_D: 0.2457 Loss_G: 1.2906\n===> Epoch[233](54/88): Loss_D: 0.1898 Loss_G: 1.7062\n===> Epoch[233](55/88): Loss_D: 0.2311 Loss_G: 1.3178\n===> Epoch[233](56/88): Loss_D: 0.2540 Loss_G: 1.2356\n===> Epoch[233](57/88): Loss_D: 0.2327 Loss_G: 1.3152\n===> Epoch[233](58/88): Loss_D: 0.2404 Loss_G: 1.4142\n===> Epoch[233](59/88): Loss_D: 0.2534 Loss_G: 1.1318\n===> Epoch[233](60/88): Loss_D: 0.2514 Loss_G: 1.2024\n===> Epoch[233](61/88): Loss_D: 0.1737 Loss_G: 1.5369\n===> Epoch[233](62/88): Loss_D: 0.2496 Loss_G: 1.4481\n===> Epoch[233](63/88): Loss_D: 0.2448 Loss_G: 1.3327\n===> Epoch[233](64/88): Loss_D: 0.2481 Loss_G: 1.4246\n===> Epoch[233](65/88): Loss_D: 0.2435 Loss_G: 1.2117\n===> Epoch[233](66/88): Loss_D: 0.2607 Loss_G: 0.9915\n===> Epoch[233](67/88): Loss_D: 0.2504 Loss_G: 1.1603\n===> Epoch[233](68/88): Loss_D: 0.2161 Loss_G: 1.4049\n===> Epoch[233](69/88): Loss_D: 0.2525 Loss_G: 1.1504\n===> Epoch[233](70/88): Loss_D: 0.2310 Loss_G: 1.3096\n===> Epoch[233](71/88): Loss_D: 0.2394 Loss_G: 1.2302\n===> Epoch[233](72/88): Loss_D: 0.2432 Loss_G: 1.1579\n===> Epoch[233](73/88): Loss_D: 0.2490 Loss_G: 1.1223\n===> Epoch[233](74/88): Loss_D: 0.2411 Loss_G: 1.1904\n===> Epoch[233](75/88): Loss_D: 0.2563 Loss_G: 1.2195\n===> Epoch[233](76/88): Loss_D: 0.2553 Loss_G: 1.1710\n===> Epoch[233](77/88): Loss_D: 0.2460 Loss_G: 0.9993\n===> Epoch[233](78/88): Loss_D: 0.2094 Loss_G: 1.4258\n===> Epoch[233](79/88): Loss_D: 0.2474 Loss_G: 1.2116\n===> Epoch[233](80/88): Loss_D: 0.2438 Loss_G: 1.1767\n===> Epoch[233](81/88): Loss_D: 0.2528 Loss_G: 1.0199\n===> Epoch[233](82/88): Loss_D: 0.2389 Loss_G: 1.1405\n===> Epoch[233](83/88): Loss_D: 0.2603 Loss_G: 0.9985\n===> Epoch[233](84/88): Loss_D: 0.2429 Loss_G: 1.1695\n===> Epoch[233](85/88): Loss_D: 0.2591 Loss_G: 1.0761\n===> Epoch[233](86/88): Loss_D: 0.1987 Loss_G: 1.4865\n===> Epoch[233](87/88): Loss_D: 0.2571 Loss_G: 1.1707\n===> Epoch[233](88/88): Loss_D: 0.2454 Loss_G: 1.3076\nlearning rate = 0.0000419\nlearning rate = 0.0000419\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0010} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[234](1/88): Loss_D: 0.2688 Loss_G: 1.0626\n===> Epoch[234](2/88): Loss_D: 0.2276 Loss_G: 1.0671\n===> Epoch[234](3/88): Loss_D: 0.2494 Loss_G: 1.1714\n===> Epoch[234](4/88): Loss_D: 0.2641 Loss_G: 1.1044\n===> Epoch[234](5/88): Loss_D: 0.2621 Loss_G: 1.0214\n===> Epoch[234](6/88): Loss_D: 0.2478 Loss_G: 1.1674\n===> Epoch[234](7/88): Loss_D: 0.2396 Loss_G: 1.3098\n===> Epoch[234](8/88): Loss_D: 0.2476 Loss_G: 1.1045\n===> Epoch[234](9/88): Loss_D: 0.2437 Loss_G: 1.1483\n===> Epoch[234](10/88): Loss_D: 0.2458 Loss_G: 1.2789\n===> Epoch[234](11/88): Loss_D: 0.2434 Loss_G: 1.2104\n===> Epoch[234](12/88): Loss_D: 0.2530 Loss_G: 0.9295\n===> Epoch[234](13/88): Loss_D: 0.1803 Loss_G: 1.4724\n===> Epoch[234](14/88): Loss_D: 0.2308 Loss_G: 1.1839\n===> Epoch[234](15/88): Loss_D: 0.2288 Loss_G: 1.1114\n===> Epoch[234](16/88): Loss_D: 0.2538 Loss_G: 1.1649\n===> Epoch[234](17/88): Loss_D: 0.2471 Loss_G: 1.0788\n===> Epoch[234](18/88): Loss_D: 0.2190 Loss_G: 1.3711\n===> Epoch[234](19/88): Loss_D: 0.2484 Loss_G: 1.1763\n===> Epoch[234](20/88): Loss_D: 0.2479 Loss_G: 1.2536\n===> Epoch[234](21/88): Loss_D: 0.1963 Loss_G: 1.7576\n===> Epoch[234](22/88): Loss_D: 0.2617 Loss_G: 1.0550\n===> Epoch[234](23/88): Loss_D: 0.2266 Loss_G: 1.4051\n===> Epoch[234](24/88): Loss_D: 0.2521 Loss_G: 1.1774\n===> Epoch[234](25/88): Loss_D: 0.2177 Loss_G: 1.4031\n===> Epoch[234](26/88): Loss_D: 0.2545 Loss_G: 1.2144\n===> Epoch[234](27/88): Loss_D: 0.2582 Loss_G: 1.2221\n===> Epoch[234](28/88): Loss_D: 0.2523 Loss_G: 1.1627\n===> Epoch[234](29/88): Loss_D: 0.2573 Loss_G: 1.2388\n===> Epoch[234](30/88): Loss_D: 0.2419 Loss_G: 1.0527\n===> Epoch[234](31/88): Loss_D: 0.2301 Loss_G: 1.3890\n===> Epoch[234](32/88): Loss_D: 0.2332 Loss_G: 1.3203\n===> Epoch[234](33/88): Loss_D: 0.2619 Loss_G: 1.2161\n===> Epoch[234](34/88): Loss_D: 0.2560 Loss_G: 1.1359\n===> Epoch[234](35/88): Loss_D: 0.2540 Loss_G: 1.1256\n===> Epoch[234](36/88): Loss_D: 0.2301 Loss_G: 1.3919\n===> Epoch[234](37/88): Loss_D: 0.2057 Loss_G: 1.5148\n===> Epoch[234](38/88): Loss_D: 0.2596 Loss_G: 1.0125\n===> Epoch[234](39/88): Loss_D: 0.2335 Loss_G: 1.1541\n===> Epoch[234](40/88): Loss_D: 0.2288 Loss_G: 1.3411\n===> Epoch[234](41/88): Loss_D: 0.2601 Loss_G: 1.1407\n===> Epoch[234](42/88): Loss_D: 0.2414 Loss_G: 1.2777\n===> Epoch[234](43/88): Loss_D: 0.2486 Loss_G: 1.1661\n===> Epoch[234](44/88): Loss_D: 0.2511 Loss_G: 1.0780\n===> Epoch[234](45/88): Loss_D: 0.2481 Loss_G: 1.2779\n===> Epoch[234](46/88): Loss_D: 0.2447 Loss_G: 1.2166\n===> Epoch[234](47/88): Loss_D: 0.2667 Loss_G: 1.0669\n===> Epoch[234](48/88): Loss_D: 0.2528 Loss_G: 0.9079\n===> Epoch[234](49/88): Loss_D: 0.2561 Loss_G: 1.0651\n===> Epoch[234](50/88): Loss_D: 0.2453 Loss_G: 1.1491\n===> Epoch[234](51/88): Loss_D: 0.2387 Loss_G: 1.1353\n===> Epoch[234](52/88): Loss_D: 0.2573 Loss_G: 1.0568\n===> Epoch[234](53/88): Loss_D: 0.2431 Loss_G: 1.2368\n===> Epoch[234](54/88): Loss_D: 0.2247 Loss_G: 1.3167\n===> Epoch[234](55/88): Loss_D: 0.2297 Loss_G: 2.0369\n===> Epoch[234](56/88): Loss_D: 0.2436 Loss_G: 1.1756\n===> Epoch[234](57/88): Loss_D: 0.2449 Loss_G: 1.4251\n===> Epoch[234](58/88): Loss_D: 0.2463 Loss_G: 1.2899\n===> Epoch[234](59/88): Loss_D: 0.2479 Loss_G: 1.2551\n===> Epoch[234](60/88): Loss_D: 0.2409 Loss_G: 1.0964\n===> Epoch[234](61/88): Loss_D: 0.2350 Loss_G: 1.2217\n===> Epoch[234](62/88): Loss_D: 0.2655 Loss_G: 1.2439\n===> Epoch[234](63/88): Loss_D: 0.2401 Loss_G: 1.2868\n===> Epoch[234](64/88): Loss_D: 0.2432 Loss_G: 1.2497\n===> Epoch[234](65/88): Loss_D: 0.2349 Loss_G: 1.3535\n===> Epoch[234](66/88): Loss_D: 0.2500 Loss_G: 1.1120\n===> Epoch[234](67/88): Loss_D: 0.2544 Loss_G: 1.2153\n===> Epoch[234](68/88): Loss_D: 0.2317 Loss_G: 1.2470\n===> Epoch[234](69/88): Loss_D: 0.2525 Loss_G: 1.1047\n===> Epoch[234](70/88): Loss_D: 0.2614 Loss_G: 1.0932\n===> Epoch[234](71/88): Loss_D: 0.2470 Loss_G: 1.1579\n===> Epoch[234](72/88): Loss_D: 0.2373 Loss_G: 1.2856\n===> Epoch[234](73/88): Loss_D: 0.2418 Loss_G: 1.1838\n===> Epoch[234](74/88): Loss_D: 0.2438 Loss_G: 1.1617\n===> Epoch[234](75/88): Loss_D: 0.2320 Loss_G: 1.1574\n===> Epoch[234](76/88): Loss_D: 0.1705 Loss_G: 1.6341\n===> Epoch[234](77/88): Loss_D: 0.2119 Loss_G: 1.3818\n===> Epoch[234](78/88): Loss_D: 0.2344 Loss_G: 1.3782\n===> Epoch[234](79/88): Loss_D: 0.2372 Loss_G: 1.3337\n===> Epoch[234](80/88): Loss_D: 0.2572 Loss_G: 1.2283\n===> Epoch[234](81/88): Loss_D: 0.2406 Loss_G: 1.1858\n===> Epoch[234](82/88): Loss_D: 0.2537 Loss_G: 1.1318\n===> Epoch[234](83/88): Loss_D: 0.2391 Loss_G: 1.1925\n===> Epoch[234](84/88): Loss_D: 0.1818 Loss_G: 1.6131\n===> Epoch[234](85/88): Loss_D: 0.2213 Loss_G: 1.4098\n===> Epoch[234](86/88): Loss_D: 0.2563 Loss_G: 1.0659\n===> Epoch[234](87/88): Loss_D: 0.2244 Loss_G: 1.2455\n===> Epoch[234](88/88): Loss_D: 0.2381 Loss_G: 1.1988\nlearning rate = 0.0000416\nlearning rate = 0.0000416\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0108} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[235](1/88): Loss_D: 0.2592 Loss_G: 1.1108\n===> Epoch[235](2/88): Loss_D: 0.2516 Loss_G: 0.9488\n===> Epoch[235](3/88): Loss_D: 0.2527 Loss_G: 1.1905\n===> Epoch[235](4/88): Loss_D: 0.2096 Loss_G: 1.4060\n===> Epoch[235](5/88): Loss_D: 0.2425 Loss_G: 1.1452\n===> Epoch[235](6/88): Loss_D: 0.2518 Loss_G: 1.2456\n===> Epoch[235](7/88): Loss_D: 0.2571 Loss_G: 1.1581\n===> Epoch[235](8/88): Loss_D: 0.2399 Loss_G: 1.0817\n===> Epoch[235](9/88): Loss_D: 0.2526 Loss_G: 1.2105\n===> Epoch[235](10/88): Loss_D: 0.2549 Loss_G: 1.1090\n===> Epoch[235](11/88): Loss_D: 0.2409 Loss_G: 1.2913\n===> Epoch[235](12/88): Loss_D: 0.2394 Loss_G: 1.3117\n===> Epoch[235](13/88): Loss_D: 0.2416 Loss_G: 1.2823\n===> Epoch[235](14/88): Loss_D: 0.2570 Loss_G: 1.0205\n===> Epoch[235](15/88): Loss_D: 0.2394 Loss_G: 1.4199\n===> Epoch[235](16/88): Loss_D: 0.2333 Loss_G: 1.2802\n===> Epoch[235](17/88): Loss_D: 0.2565 Loss_G: 1.0571\n===> Epoch[235](18/88): Loss_D: 0.2402 Loss_G: 1.1579\n===> Epoch[235](19/88): Loss_D: 0.2508 Loss_G: 1.1505\n===> Epoch[235](20/88): Loss_D: 0.2492 Loss_G: 1.2757\n===> Epoch[235](21/88): Loss_D: 0.2560 Loss_G: 1.1196\n===> Epoch[235](22/88): Loss_D: 0.2534 Loss_G: 1.2432\n===> Epoch[235](23/88): Loss_D: 0.2416 Loss_G: 1.0899\n===> Epoch[235](24/88): Loss_D: 0.2480 Loss_G: 1.0475\n===> Epoch[235](25/88): Loss_D: 0.2442 Loss_G: 1.1479\n===> Epoch[235](26/88): Loss_D: 0.2531 Loss_G: 1.0933\n===> Epoch[235](27/88): Loss_D: 0.2423 Loss_G: 1.2382\n===> Epoch[235](28/88): Loss_D: 0.2409 Loss_G: 1.1183\n===> Epoch[235](29/88): Loss_D: 0.2301 Loss_G: 1.2624\n===> Epoch[235](30/88): Loss_D: 0.2497 Loss_G: 1.2665\n===> Epoch[235](31/88): Loss_D: 0.2458 Loss_G: 1.0751\n===> Epoch[235](32/88): Loss_D: 0.2463 Loss_G: 1.1042\n===> Epoch[235](33/88): Loss_D: 0.2287 Loss_G: 1.2920\n===> Epoch[235](34/88): Loss_D: 0.2452 Loss_G: 1.2096\n===> Epoch[235](35/88): Loss_D: 0.2357 Loss_G: 1.1642\n===> Epoch[235](36/88): Loss_D: 0.1938 Loss_G: 1.6022\n===> Epoch[235](37/88): Loss_D: 0.2406 Loss_G: 1.3608\n===> Epoch[235](38/88): Loss_D: 0.2452 Loss_G: 1.3734\n===> Epoch[235](39/88): Loss_D: 0.2400 Loss_G: 1.3542\n===> Epoch[235](40/88): Loss_D: 0.2673 Loss_G: 1.2063\n===> Epoch[235](41/88): Loss_D: 0.2386 Loss_G: 1.3285\n===> Epoch[235](42/88): Loss_D: 0.2465 Loss_G: 1.2084\n===> Epoch[235](43/88): Loss_D: 0.2434 Loss_G: 1.1117\n===> Epoch[235](44/88): Loss_D: 0.2503 Loss_G: 1.1099\n===> Epoch[235](45/88): Loss_D: 0.2453 Loss_G: 1.1822\n===> Epoch[235](46/88): Loss_D: 0.1962 Loss_G: 1.8584\n===> Epoch[235](47/88): Loss_D: 0.2110 Loss_G: 1.2980\n===> Epoch[235](48/88): Loss_D: 0.2353 Loss_G: 1.3136\n===> Epoch[235](49/88): Loss_D: 0.2433 Loss_G: 1.2885\n===> Epoch[235](50/88): Loss_D: 0.2307 Loss_G: 1.3361\n===> Epoch[235](51/88): Loss_D: 0.2489 Loss_G: 1.2189\n===> Epoch[235](52/88): Loss_D: 0.2502 Loss_G: 1.2022\n===> Epoch[235](53/88): Loss_D: 0.2486 Loss_G: 1.0267\n===> Epoch[235](54/88): Loss_D: 0.2451 Loss_G: 1.0733\n===> Epoch[235](55/88): Loss_D: 0.2253 Loss_G: 2.0615\n===> Epoch[235](56/88): Loss_D: 0.2479 Loss_G: 1.1654\n===> Epoch[235](57/88): Loss_D: 0.2463 Loss_G: 1.0784\n===> Epoch[235](58/88): Loss_D: 0.2425 Loss_G: 1.1890\n===> Epoch[235](59/88): Loss_D: 0.2570 Loss_G: 1.1345\n===> Epoch[235](60/88): Loss_D: 0.2676 Loss_G: 1.1029\n===> Epoch[235](61/88): Loss_D: 0.2675 Loss_G: 1.0783\n===> Epoch[235](62/88): Loss_D: 0.2468 Loss_G: 1.1854\n===> Epoch[235](63/88): Loss_D: 0.2106 Loss_G: 1.4043\n===> Epoch[235](64/88): Loss_D: 0.2400 Loss_G: 1.1411\n===> Epoch[235](65/88): Loss_D: 0.2632 Loss_G: 1.1056\n===> Epoch[235](66/88): Loss_D: 0.2485 Loss_G: 1.3837\n===> Epoch[235](67/88): Loss_D: 0.1742 Loss_G: 1.6098\n===> Epoch[235](68/88): Loss_D: 0.2549 Loss_G: 1.1301\n===> Epoch[235](69/88): Loss_D: 0.2471 Loss_G: 1.4069\n===> Epoch[235](70/88): Loss_D: 0.2412 Loss_G: 1.2572\n===> Epoch[235](71/88): Loss_D: 0.2588 Loss_G: 1.0227\n===> Epoch[235](72/88): Loss_D: 0.2388 Loss_G: 1.2046\n===> Epoch[235](73/88): Loss_D: 0.2272 Loss_G: 1.2742\n===> Epoch[235](74/88): Loss_D: 0.2412 Loss_G: 1.1652\n===> Epoch[235](75/88): Loss_D: 0.2560 Loss_G: 1.2324\n===> Epoch[235](76/88): Loss_D: 0.2438 Loss_G: 1.1013\n===> Epoch[235](77/88): Loss_D: 0.2404 Loss_G: 1.0683\n===> Epoch[235](78/88): Loss_D: 0.2508 Loss_G: 1.1870\n===> Epoch[235](79/88): Loss_D: 0.2313 Loss_G: 1.3720\n===> Epoch[235](80/88): Loss_D: 0.2463 Loss_G: 1.2982\n===> Epoch[235](81/88): Loss_D: 0.2570 Loss_G: 1.0152\n===> Epoch[235](82/88): Loss_D: 0.2503 Loss_G: 1.1850\n===> Epoch[235](83/88): Loss_D: 0.2558 Loss_G: 1.0463\n===> Epoch[235](84/88): Loss_D: 0.2383 Loss_G: 1.2193\n===> Epoch[235](85/88): Loss_D: 0.2505 Loss_G: 1.2025\n===> Epoch[235](86/88): Loss_D: 0.1674 Loss_G: 1.5193\n===> Epoch[235](87/88): Loss_D: 0.2576 Loss_G: 1.2469\n===> Epoch[235](88/88): Loss_D: 0.2171 Loss_G: 1.2906\nlearning rate = 0.0000414\nlearning rate = 0.0000414\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0001} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[236](1/88): Loss_D: 0.2434 Loss_G: 1.3367\n===> Epoch[236](2/88): Loss_D: 0.2575 Loss_G: 1.2286\n===> Epoch[236](3/88): Loss_D: 0.2479 Loss_G: 1.3459\n===> Epoch[236](4/88): Loss_D: 0.2417 Loss_G: 1.1549\n===> Epoch[236](5/88): Loss_D: 0.2456 Loss_G: 1.1750\n===> Epoch[236](6/88): Loss_D: 0.2468 Loss_G: 1.1104\n===> Epoch[236](7/88): Loss_D: 0.2367 Loss_G: 1.0993\n===> Epoch[236](8/88): Loss_D: 0.2540 Loss_G: 1.0660\n===> Epoch[236](9/88): Loss_D: 0.2401 Loss_G: 1.2445\n===> Epoch[236](10/88): Loss_D: 0.2494 Loss_G: 1.2119\n===> Epoch[236](11/88): Loss_D: 0.2434 Loss_G: 1.1613\n===> Epoch[236](12/88): Loss_D: 0.2457 Loss_G: 1.0232\n===> Epoch[236](13/88): Loss_D: 0.2269 Loss_G: 1.1054\n===> Epoch[236](14/88): Loss_D: 0.2479 Loss_G: 1.3921\n===> Epoch[236](15/88): Loss_D: 0.2487 Loss_G: 1.0503\n===> Epoch[236](16/88): Loss_D: 0.2620 Loss_G: 1.1051\n===> Epoch[236](17/88): Loss_D: 0.2429 Loss_G: 1.2955\n===> Epoch[236](18/88): Loss_D: 0.1854 Loss_G: 1.4993\n===> Epoch[236](19/88): Loss_D: 0.2557 Loss_G: 1.1334\n===> Epoch[236](20/88): Loss_D: 0.2558 Loss_G: 1.1369\n===> Epoch[236](21/88): Loss_D: 0.2388 Loss_G: 1.2299\n===> Epoch[236](22/88): Loss_D: 0.2477 Loss_G: 1.1893\n===> Epoch[236](23/88): Loss_D: 0.2433 Loss_G: 1.2406\n===> Epoch[236](24/88): Loss_D: 0.2537 Loss_G: 1.1426\n===> Epoch[236](25/88): Loss_D: 0.2370 Loss_G: 1.1715\n===> Epoch[236](26/88): Loss_D: 0.2296 Loss_G: 1.3684\n===> Epoch[236](27/88): Loss_D: 0.2505 Loss_G: 1.2793\n===> Epoch[236](28/88): Loss_D: 0.2401 Loss_G: 1.2318\n===> Epoch[236](29/88): Loss_D: 0.2514 Loss_G: 1.0164\n===> Epoch[236](30/88): Loss_D: 0.2365 Loss_G: 1.2224\n===> Epoch[236](31/88): Loss_D: 0.2391 Loss_G: 1.3004\n===> Epoch[236](32/88): Loss_D: 0.2444 Loss_G: 1.2011\n===> Epoch[236](33/88): Loss_D: 0.2415 Loss_G: 1.2688\n===> Epoch[236](34/88): Loss_D: 0.2410 Loss_G: 1.2355\n===> Epoch[236](35/88): Loss_D: 0.2441 Loss_G: 1.0651\n===> Epoch[236](36/88): Loss_D: 0.2576 Loss_G: 1.1786\n===> Epoch[236](37/88): Loss_D: 0.2555 Loss_G: 1.1486\n===> Epoch[236](38/88): Loss_D: 0.2441 Loss_G: 1.1882\n===> Epoch[236](39/88): Loss_D: 0.2417 Loss_G: 1.1740\n===> Epoch[236](40/88): Loss_D: 0.1860 Loss_G: 1.5636\n===> Epoch[236](41/88): Loss_D: 0.2504 Loss_G: 1.0896\n===> Epoch[236](42/88): Loss_D: 0.2550 Loss_G: 1.1657\n===> Epoch[236](43/88): Loss_D: 0.2486 Loss_G: 1.0964\n===> Epoch[236](44/88): Loss_D: 0.2480 Loss_G: 1.0312\n===> Epoch[236](45/88): Loss_D: 0.2488 Loss_G: 1.1212\n===> Epoch[236](46/88): Loss_D: 0.1904 Loss_G: 2.2796\n===> Epoch[236](47/88): Loss_D: 0.2581 Loss_G: 1.0784\n===> Epoch[236](48/88): Loss_D: 0.2516 Loss_G: 1.2065\n===> Epoch[236](49/88): Loss_D: 0.2356 Loss_G: 1.4010\n===> Epoch[236](50/88): Loss_D: 0.2329 Loss_G: 1.2435\n===> Epoch[236](51/88): Loss_D: 0.2443 Loss_G: 1.0393\n===> Epoch[236](52/88): Loss_D: 0.2463 Loss_G: 1.1730\n===> Epoch[236](53/88): Loss_D: 0.2591 Loss_G: 0.9963\n===> Epoch[236](54/88): Loss_D: 0.2489 Loss_G: 1.1824\n===> Epoch[236](55/88): Loss_D: 0.2499 Loss_G: 1.2347\n===> Epoch[236](56/88): Loss_D: 0.2290 Loss_G: 1.2773\n===> Epoch[236](57/88): Loss_D: 0.2548 Loss_G: 0.9949\n===> Epoch[236](58/88): Loss_D: 0.2351 Loss_G: 1.3432\n===> Epoch[236](59/88): Loss_D: 0.2476 Loss_G: 1.2108\n===> Epoch[236](60/88): Loss_D: 0.2495 Loss_G: 0.9149\n===> Epoch[236](61/88): Loss_D: 0.2406 Loss_G: 1.0129\n===> Epoch[236](62/88): Loss_D: 0.2457 Loss_G: 1.2023\n===> Epoch[236](63/88): Loss_D: 0.2048 Loss_G: 1.3217\n===> Epoch[236](64/88): Loss_D: 0.2541 Loss_G: 1.0700\n===> Epoch[236](65/88): Loss_D: 0.2457 Loss_G: 1.3763\n===> Epoch[236](66/88): Loss_D: 0.2503 Loss_G: 1.2097\n===> Epoch[236](67/88): Loss_D: 0.1602 Loss_G: 1.5142\n===> Epoch[236](68/88): Loss_D: 0.2231 Loss_G: 1.3185\n===> Epoch[236](69/88): Loss_D: 0.1878 Loss_G: 1.5328\n===> Epoch[236](70/88): Loss_D: 0.2472 Loss_G: 1.3074\n===> Epoch[236](71/88): Loss_D: 0.2377 Loss_G: 1.3584\n===> Epoch[236](72/88): Loss_D: 0.2472 Loss_G: 1.2558\n===> Epoch[236](73/88): Loss_D: 0.2435 Loss_G: 1.1396\n===> Epoch[236](74/88): Loss_D: 0.2519 Loss_G: 1.1896\n===> Epoch[236](75/88): Loss_D: 0.2382 Loss_G: 1.0808\n===> Epoch[236](76/88): Loss_D: 0.2448 Loss_G: 1.4646\n===> Epoch[236](77/88): Loss_D: 0.2333 Loss_G: 1.1043\n===> Epoch[236](78/88): Loss_D: 0.1873 Loss_G: 1.7883\n===> Epoch[236](79/88): Loss_D: 0.1995 Loss_G: 1.4474\n===> Epoch[236](80/88): Loss_D: 0.2578 Loss_G: 1.1524\n===> Epoch[236](81/88): Loss_D: 0.2370 Loss_G: 1.3754\n===> Epoch[236](82/88): Loss_D: 0.2695 Loss_G: 1.0170\n===> Epoch[236](83/88): Loss_D: 0.2455 Loss_G: 1.3686\n===> Epoch[236](84/88): Loss_D: 0.2596 Loss_G: 1.1804\n===> Epoch[236](85/88): Loss_D: 0.2472 Loss_G: 1.2685\n===> Epoch[236](86/88): Loss_D: 0.2544 Loss_G: 1.2466\n===> Epoch[236](87/88): Loss_D: 0.2367 Loss_G: 1.2139\n===> Epoch[236](88/88): Loss_D: 0.2597 Loss_G: 1.0046\nlearning rate = 0.0000411\nlearning rate = 0.0000411\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0039} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[237](1/88): Loss_D: 0.2425 Loss_G: 1.0490\n===> Epoch[237](2/88): Loss_D: 0.2526 Loss_G: 1.1458\n===> Epoch[237](3/88): Loss_D: 0.2244 Loss_G: 1.3564\n===> Epoch[237](4/88): Loss_D: 0.2546 Loss_G: 1.0639\n===> Epoch[237](5/88): Loss_D: 0.2632 Loss_G: 1.1107\n===> Epoch[237](6/88): Loss_D: 0.2357 Loss_G: 1.0774\n===> Epoch[237](7/88): Loss_D: 0.2381 Loss_G: 1.2330\n===> Epoch[237](8/88): Loss_D: 0.2555 Loss_G: 1.0291\n===> Epoch[237](9/88): Loss_D: 0.2516 Loss_G: 1.1564\n===> Epoch[237](10/88): Loss_D: 0.2173 Loss_G: 1.3741\n===> Epoch[237](11/88): Loss_D: 0.2554 Loss_G: 1.0618\n===> Epoch[237](12/88): Loss_D: 0.2439 Loss_G: 1.2090\n===> Epoch[237](13/88): Loss_D: 0.2537 Loss_G: 1.0683\n===> Epoch[237](14/88): Loss_D: 0.2219 Loss_G: 1.1701\n===> Epoch[237](15/88): Loss_D: 0.2534 Loss_G: 1.1209\n===> Epoch[237](16/88): Loss_D: 0.2391 Loss_G: 1.3805\n===> Epoch[237](17/88): Loss_D: 0.2572 Loss_G: 1.1882\n===> Epoch[237](18/88): Loss_D: 0.2584 Loss_G: 1.1122\n===> Epoch[237](19/88): Loss_D: 0.2419 Loss_G: 1.2390\n===> Epoch[237](20/88): Loss_D: 0.2419 Loss_G: 1.4027\n===> Epoch[237](21/88): Loss_D: 0.2412 Loss_G: 1.4025\n===> Epoch[237](22/88): Loss_D: 0.2527 Loss_G: 1.1146\n===> Epoch[237](23/88): Loss_D: 0.2521 Loss_G: 1.2009\n===> Epoch[237](24/88): Loss_D: 0.2548 Loss_G: 1.0694\n===> Epoch[237](25/88): Loss_D: 0.1868 Loss_G: 1.5865\n===> Epoch[237](26/88): Loss_D: 0.2415 Loss_G: 1.3398\n===> Epoch[237](27/88): Loss_D: 0.2524 Loss_G: 1.0307\n===> Epoch[237](28/88): Loss_D: 0.2494 Loss_G: 1.1153\n===> Epoch[237](29/88): Loss_D: 0.2374 Loss_G: 1.3607\n===> Epoch[237](30/88): Loss_D: 0.2489 Loss_G: 1.1709\n===> Epoch[237](31/88): Loss_D: 0.2475 Loss_G: 1.1170\n===> Epoch[237](32/88): Loss_D: 0.2390 Loss_G: 1.1995\n===> Epoch[237](33/88): Loss_D: 0.2462 Loss_G: 1.1541\n===> Epoch[237](34/88): Loss_D: 0.2410 Loss_G: 1.1394\n===> Epoch[237](35/88): Loss_D: 0.2294 Loss_G: 1.2454\n===> Epoch[237](36/88): Loss_D: 0.2541 Loss_G: 0.9987\n===> Epoch[237](37/88): Loss_D: 0.2469 Loss_G: 1.2119\n===> Epoch[237](38/88): Loss_D: 0.2426 Loss_G: 1.2733\n===> Epoch[237](39/88): Loss_D: 0.2513 Loss_G: 1.1731\n===> Epoch[237](40/88): Loss_D: 0.2616 Loss_G: 1.0828\n===> Epoch[237](41/88): Loss_D: 0.2367 Loss_G: 1.3064\n===> Epoch[237](42/88): Loss_D: 0.2260 Loss_G: 1.3663\n===> Epoch[237](43/88): Loss_D: 0.2485 Loss_G: 1.2445\n===> Epoch[237](44/88): Loss_D: 0.2426 Loss_G: 1.2195\n===> Epoch[237](45/88): Loss_D: 0.2356 Loss_G: 1.1714\n===> Epoch[237](46/88): Loss_D: 0.2281 Loss_G: 1.2585\n===> Epoch[237](47/88): Loss_D: 0.1842 Loss_G: 1.5313\n===> Epoch[237](48/88): Loss_D: 0.2483 Loss_G: 1.1490\n===> Epoch[237](49/88): Loss_D: 0.2538 Loss_G: 1.0576\n===> Epoch[237](50/88): Loss_D: 0.2367 Loss_G: 1.1968\n===> Epoch[237](51/88): Loss_D: 0.2412 Loss_G: 1.1709\n===> Epoch[237](52/88): Loss_D: 0.1777 Loss_G: 1.4982\n===> Epoch[237](53/88): Loss_D: 0.2668 Loss_G: 1.2705\n===> Epoch[237](54/88): Loss_D: 0.2490 Loss_G: 1.2995\n===> Epoch[237](55/88): Loss_D: 0.2252 Loss_G: 1.1905\n===> Epoch[237](56/88): Loss_D: 0.2478 Loss_G: 1.1093\n===> Epoch[237](57/88): Loss_D: 0.2611 Loss_G: 0.9530\n===> Epoch[237](58/88): Loss_D: 0.2489 Loss_G: 1.2095\n===> Epoch[237](59/88): Loss_D: 0.2100 Loss_G: 1.3475\n===> Epoch[237](60/88): Loss_D: 0.2160 Loss_G: 1.7513\n===> Epoch[237](61/88): Loss_D: 0.2488 Loss_G: 1.1439\n===> Epoch[237](62/88): Loss_D: 0.2526 Loss_G: 1.2183\n===> Epoch[237](63/88): Loss_D: 0.2434 Loss_G: 1.2137\n===> Epoch[237](64/88): Loss_D: 0.2569 Loss_G: 1.2354\n===> Epoch[237](65/88): Loss_D: 0.2451 Loss_G: 1.1085\n===> Epoch[237](66/88): Loss_D: 0.2411 Loss_G: 1.3893\n===> Epoch[237](67/88): Loss_D: 0.2421 Loss_G: 1.0903\n===> Epoch[237](68/88): Loss_D: 0.2509 Loss_G: 0.9502\n===> Epoch[237](69/88): Loss_D: 0.2363 Loss_G: 1.3084\n===> Epoch[237](70/88): Loss_D: 0.2657 Loss_G: 1.0392\n===> Epoch[237](71/88): Loss_D: 0.2154 Loss_G: 2.0862\n===> Epoch[237](72/88): Loss_D: 0.2356 Loss_G: 1.4122\n===> Epoch[237](73/88): Loss_D: 0.2588 Loss_G: 1.1122\n===> Epoch[237](74/88): Loss_D: 0.2373 Loss_G: 1.3169\n===> Epoch[237](75/88): Loss_D: 0.2264 Loss_G: 1.3927\n===> Epoch[237](76/88): Loss_D: 0.2425 Loss_G: 1.2767\n===> Epoch[237](77/88): Loss_D: 0.2586 Loss_G: 1.0336\n===> Epoch[237](78/88): Loss_D: 0.2669 Loss_G: 1.2837\n===> Epoch[237](79/88): Loss_D: 0.2448 Loss_G: 1.2991\n===> Epoch[237](80/88): Loss_D: 0.2573 Loss_G: 1.0349\n===> Epoch[237](81/88): Loss_D: 0.2479 Loss_G: 1.2083\n===> Epoch[237](82/88): Loss_D: 0.2345 Loss_G: 1.1724\n===> Epoch[237](83/88): Loss_D: 0.2205 Loss_G: 1.5009\n===> Epoch[237](84/88): Loss_D: 0.2512 Loss_G: 1.2142\n===> Epoch[237](85/88): Loss_D: 0.2464 Loss_G: 1.2297\n===> Epoch[237](86/88): Loss_D: 0.2299 Loss_G: 1.3199\n===> Epoch[237](87/88): Loss_D: 0.2566 Loss_G: 1.2983\n===> Epoch[237](88/88): Loss_D: 0.2359 Loss_G: 1.2387\nlearning rate = 0.0000409\nlearning rate = 0.0000409\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0008} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[238](1/88): Loss_D: 0.2373 Loss_G: 1.3301\n===> Epoch[238](2/88): Loss_D: 0.2449 Loss_G: 1.1696\n===> Epoch[238](3/88): Loss_D: 0.2450 Loss_G: 1.1699\n===> Epoch[238](4/88): Loss_D: 0.2527 Loss_G: 1.1757\n===> Epoch[238](5/88): Loss_D: 0.2420 Loss_G: 1.0000\n===> Epoch[238](6/88): Loss_D: 0.2319 Loss_G: 1.2160\n===> Epoch[238](7/88): Loss_D: 0.2511 Loss_G: 1.1712\n===> Epoch[238](8/88): Loss_D: 0.2069 Loss_G: 1.4116\n===> Epoch[238](9/88): Loss_D: 0.2249 Loss_G: 1.4832\n===> Epoch[238](10/88): Loss_D: 0.2501 Loss_G: 1.2194\n===> Epoch[238](11/88): Loss_D: 0.2536 Loss_G: 1.0841\n===> Epoch[238](12/88): Loss_D: 0.2446 Loss_G: 1.0272\n===> Epoch[238](13/88): Loss_D: 0.2482 Loss_G: 1.1684\n===> Epoch[238](14/88): Loss_D: 0.2239 Loss_G: 1.4294\n===> Epoch[238](15/88): Loss_D: 0.2482 Loss_G: 1.2150\n===> Epoch[238](16/88): Loss_D: 0.2229 Loss_G: 1.1447\n===> Epoch[238](17/88): Loss_D: 0.2622 Loss_G: 0.9088\n===> Epoch[238](18/88): Loss_D: 0.1835 Loss_G: 1.5667\n===> Epoch[238](19/88): Loss_D: 0.2538 Loss_G: 1.1880\n===> Epoch[238](20/88): Loss_D: 0.2625 Loss_G: 1.2376\n===> Epoch[238](21/88): Loss_D: 0.2438 Loss_G: 1.2921\n===> Epoch[238](22/88): Loss_D: 0.2521 Loss_G: 1.2045\n===> Epoch[238](23/88): Loss_D: 0.2310 Loss_G: 1.2348\n===> Epoch[238](24/88): Loss_D: 0.2505 Loss_G: 1.1376\n===> Epoch[238](25/88): Loss_D: 0.2424 Loss_G: 1.2437\n===> Epoch[238](26/88): Loss_D: 0.2307 Loss_G: 1.1460\n===> Epoch[238](27/88): Loss_D: 0.2434 Loss_G: 1.1790\n===> Epoch[238](28/88): Loss_D: 0.2537 Loss_G: 1.0837\n===> Epoch[238](29/88): Loss_D: 0.2478 Loss_G: 1.0619\n===> Epoch[238](30/88): Loss_D: 0.2501 Loss_G: 1.0532\n===> Epoch[238](31/88): Loss_D: 0.2303 Loss_G: 1.0835\n===> Epoch[238](32/88): Loss_D: 0.2532 Loss_G: 1.0422\n===> Epoch[238](33/88): Loss_D: 0.2321 Loss_G: 1.3378\n===> Epoch[238](34/88): Loss_D: 0.2278 Loss_G: 1.3498\n===> Epoch[238](35/88): Loss_D: 0.2471 Loss_G: 1.4278\n===> Epoch[238](36/88): Loss_D: 0.1899 Loss_G: 1.8600\n===> Epoch[238](37/88): Loss_D: 0.2579 Loss_G: 1.0222\n===> Epoch[238](38/88): Loss_D: 0.2321 Loss_G: 1.3432\n===> Epoch[238](39/88): Loss_D: 0.1785 Loss_G: 1.5351\n===> Epoch[238](40/88): Loss_D: 0.2618 Loss_G: 1.3107\n===> Epoch[238](41/88): Loss_D: 0.2690 Loss_G: 1.2425\n===> Epoch[238](42/88): Loss_D: 0.2443 Loss_G: 1.0909\n===> Epoch[238](43/88): Loss_D: 0.2342 Loss_G: 1.3373\n===> Epoch[238](44/88): Loss_D: 0.2342 Loss_G: 1.3215\n===> Epoch[238](45/88): Loss_D: 0.2438 Loss_G: 1.2375\n===> Epoch[238](46/88): Loss_D: 0.2412 Loss_G: 1.1661\n===> Epoch[238](47/88): Loss_D: 0.2456 Loss_G: 1.2220\n===> Epoch[238](48/88): Loss_D: 0.2260 Loss_G: 1.2380\n===> Epoch[238](49/88): Loss_D: 0.2550 Loss_G: 1.1412\n===> Epoch[238](50/88): Loss_D: 0.2439 Loss_G: 1.1560\n===> Epoch[238](51/88): Loss_D: 0.1720 Loss_G: 1.5160\n===> Epoch[238](52/88): Loss_D: 0.2444 Loss_G: 1.1793\n===> Epoch[238](53/88): Loss_D: 0.2535 Loss_G: 0.9154\n===> Epoch[238](54/88): Loss_D: 0.2385 Loss_G: 1.3607\n===> Epoch[238](55/88): Loss_D: 0.2494 Loss_G: 1.2234\n===> Epoch[238](56/88): Loss_D: 0.2286 Loss_G: 1.2306\n===> Epoch[238](57/88): Loss_D: 0.2325 Loss_G: 1.2099\n===> Epoch[238](58/88): Loss_D: 0.2079 Loss_G: 1.5196\n===> Epoch[238](59/88): Loss_D: 0.2460 Loss_G: 1.2296\n===> Epoch[238](60/88): Loss_D: 0.2607 Loss_G: 1.0037\n===> Epoch[238](61/88): Loss_D: 0.2469 Loss_G: 1.1730\n===> Epoch[238](62/88): Loss_D: 0.2366 Loss_G: 1.3800\n===> Epoch[238](63/88): Loss_D: 0.2432 Loss_G: 1.2724\n===> Epoch[238](64/88): Loss_D: 0.2518 Loss_G: 1.0570\n===> Epoch[238](65/88): Loss_D: 0.2548 Loss_G: 1.2198\n===> Epoch[238](66/88): Loss_D: 0.2458 Loss_G: 1.1170\n===> Epoch[238](67/88): Loss_D: 0.2445 Loss_G: 1.0526\n===> Epoch[238](68/88): Loss_D: 0.2416 Loss_G: 1.2844\n===> Epoch[238](69/88): Loss_D: 0.2456 Loss_G: 1.1765\n===> Epoch[238](70/88): Loss_D: 0.2431 Loss_G: 1.3852\n===> Epoch[238](71/88): Loss_D: 0.2359 Loss_G: 1.1602\n===> Epoch[238](72/88): Loss_D: 0.2623 Loss_G: 1.0349\n===> Epoch[238](73/88): Loss_D: 0.2042 Loss_G: 2.1927\n===> Epoch[238](74/88): Loss_D: 0.2542 Loss_G: 1.2070\n===> Epoch[238](75/88): Loss_D: 0.2549 Loss_G: 1.0722\n===> Epoch[238](76/88): Loss_D: 0.2478 Loss_G: 1.2783\n===> Epoch[238](77/88): Loss_D: 0.2453 Loss_G: 1.2816\n===> Epoch[238](78/88): Loss_D: 0.2463 Loss_G: 1.1017\n===> Epoch[238](79/88): Loss_D: 0.2314 Loss_G: 1.2765\n===> Epoch[238](80/88): Loss_D: 0.2551 Loss_G: 1.1657\n===> Epoch[238](81/88): Loss_D: 0.2535 Loss_G: 1.1480\n===> Epoch[238](82/88): Loss_D: 0.2491 Loss_G: 1.1859\n===> Epoch[238](83/88): Loss_D: 0.2326 Loss_G: 1.3705\n===> Epoch[238](84/88): Loss_D: 0.2340 Loss_G: 1.1717\n===> Epoch[238](85/88): Loss_D: 0.2363 Loss_G: 1.2239\n===> Epoch[238](86/88): Loss_D: 0.2265 Loss_G: 1.3180\n===> Epoch[238](87/88): Loss_D: 0.2251 Loss_G: 1.3450\n===> Epoch[238](88/88): Loss_D: 0.2410 Loss_G: 1.2711\nlearning rate = 0.0000406\nlearning rate = 0.0000406\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0006} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[239](1/88): Loss_D: 0.2115 Loss_G: 1.3841\n===> Epoch[239](2/88): Loss_D: 0.2575 Loss_G: 1.2534\n===> Epoch[239](3/88): Loss_D: 0.2407 Loss_G: 1.2819\n===> Epoch[239](4/88): Loss_D: 0.2511 Loss_G: 0.8918\n===> Epoch[239](5/88): Loss_D: 0.2682 Loss_G: 1.1904\n===> Epoch[239](6/88): Loss_D: 0.2380 Loss_G: 1.0871\n===> Epoch[239](7/88): Loss_D: 0.2479 Loss_G: 1.2605\n===> Epoch[239](8/88): Loss_D: 0.2458 Loss_G: 1.2139\n===> Epoch[239](9/88): Loss_D: 0.2472 Loss_G: 1.1789\n===> Epoch[239](10/88): Loss_D: 0.2473 Loss_G: 1.1276\n===> Epoch[239](11/88): Loss_D: 0.2411 Loss_G: 1.4849\n===> Epoch[239](12/88): Loss_D: 0.2482 Loss_G: 1.0792\n===> Epoch[239](13/88): Loss_D: 0.2583 Loss_G: 0.9939\n===> Epoch[239](14/88): Loss_D: 0.2390 Loss_G: 1.1542\n===> Epoch[239](15/88): Loss_D: 0.2472 Loss_G: 1.1609\n===> Epoch[239](16/88): Loss_D: 0.2361 Loss_G: 1.1727\n===> Epoch[239](17/88): Loss_D: 0.2514 Loss_G: 1.0961\n===> Epoch[239](18/88): Loss_D: 0.2518 Loss_G: 1.2398\n===> Epoch[239](19/88): Loss_D: 0.2405 Loss_G: 1.0973\n===> Epoch[239](20/88): Loss_D: 0.2568 Loss_G: 0.9387\n===> Epoch[239](21/88): Loss_D: 0.1634 Loss_G: 1.4935\n===> Epoch[239](22/88): Loss_D: 0.2601 Loss_G: 1.2176\n===> Epoch[239](23/88): Loss_D: 0.2398 Loss_G: 1.2567\n===> Epoch[239](24/88): Loss_D: 0.2209 Loss_G: 1.2718\n===> Epoch[239](25/88): Loss_D: 0.2513 Loss_G: 1.3096\n===> Epoch[239](26/88): Loss_D: 0.2613 Loss_G: 1.2245\n===> Epoch[239](27/88): Loss_D: 0.2438 Loss_G: 1.2509\n===> Epoch[239](28/88): Loss_D: 0.2335 Loss_G: 1.2309\n===> Epoch[239](29/88): Loss_D: 0.2584 Loss_G: 1.0862\n===> Epoch[239](30/88): Loss_D: 0.2511 Loss_G: 1.1131\n===> Epoch[239](31/88): Loss_D: 0.2692 Loss_G: 1.0200\n===> Epoch[239](32/88): Loss_D: 0.2418 Loss_G: 1.1845\n===> Epoch[239](33/88): Loss_D: 0.2356 Loss_G: 1.2623\n===> Epoch[239](34/88): Loss_D: 0.2480 Loss_G: 1.2117\n===> Epoch[239](35/88): Loss_D: 0.2096 Loss_G: 1.3435\n===> Epoch[239](36/88): Loss_D: 0.2486 Loss_G: 1.3524\n===> Epoch[239](37/88): Loss_D: 0.2363 Loss_G: 1.2585\n===> Epoch[239](38/88): Loss_D: 0.2511 Loss_G: 1.1413\n===> Epoch[239](39/88): Loss_D: 0.2493 Loss_G: 1.1382\n===> Epoch[239](40/88): Loss_D: 0.2436 Loss_G: 1.1338\n===> Epoch[239](41/88): Loss_D: 0.2477 Loss_G: 1.0864\n===> Epoch[239](42/88): Loss_D: 0.2450 Loss_G: 1.2397\n===> Epoch[239](43/88): Loss_D: 0.1907 Loss_G: 1.4701\n===> Epoch[239](44/88): Loss_D: 0.2656 Loss_G: 1.2598\n===> Epoch[239](45/88): Loss_D: 0.2474 Loss_G: 1.3184\n===> Epoch[239](46/88): Loss_D: 0.2592 Loss_G: 1.0312\n===> Epoch[239](47/88): Loss_D: 0.2528 Loss_G: 1.1747\n===> Epoch[239](48/88): Loss_D: 0.2509 Loss_G: 1.0723\n===> Epoch[239](49/88): Loss_D: 0.2641 Loss_G: 1.0418\n===> Epoch[239](50/88): Loss_D: 0.1978 Loss_G: 1.7425\n===> Epoch[239](51/88): Loss_D: 0.2494 Loss_G: 1.1143\n===> Epoch[239](52/88): Loss_D: 0.2305 Loss_G: 1.3745\n===> Epoch[239](53/88): Loss_D: 0.2575 Loss_G: 1.0698\n===> Epoch[239](54/88): Loss_D: 0.2443 Loss_G: 1.1739\n===> Epoch[239](55/88): Loss_D: 0.2497 Loss_G: 1.0809\n===> Epoch[239](56/88): Loss_D: 0.2079 Loss_G: 1.4365\n===> Epoch[239](57/88): Loss_D: 0.2335 Loss_G: 1.4156\n===> Epoch[239](58/88): Loss_D: 0.2477 Loss_G: 1.1013\n===> Epoch[239](59/88): Loss_D: 0.2496 Loss_G: 1.0200\n===> Epoch[239](60/88): Loss_D: 0.2513 Loss_G: 1.1068\n===> Epoch[239](61/88): Loss_D: 0.2268 Loss_G: 1.3208\n===> Epoch[239](62/88): Loss_D: 0.2501 Loss_G: 1.1666\n===> Epoch[239](63/88): Loss_D: 0.2494 Loss_G: 1.1452\n===> Epoch[239](64/88): Loss_D: 0.2420 Loss_G: 1.1717\n===> Epoch[239](65/88): Loss_D: 0.1891 Loss_G: 1.6136\n===> Epoch[239](66/88): Loss_D: 0.2440 Loss_G: 1.2167\n===> Epoch[239](67/88): Loss_D: 0.2385 Loss_G: 1.2887\n===> Epoch[239](68/88): Loss_D: 0.2435 Loss_G: 1.1440\n===> Epoch[239](69/88): Loss_D: 0.2383 Loss_G: 1.4268\n===> Epoch[239](70/88): Loss_D: 0.2375 Loss_G: 1.2347\n===> Epoch[239](71/88): Loss_D: 0.2411 Loss_G: 1.3501\n===> Epoch[239](72/88): Loss_D: 0.2533 Loss_G: 1.1853\n===> Epoch[239](73/88): Loss_D: 0.2544 Loss_G: 1.1652\n===> Epoch[239](74/88): Loss_D: 0.2521 Loss_G: 1.1861\n===> Epoch[239](75/88): Loss_D: 0.2507 Loss_G: 1.1379\n===> Epoch[239](76/88): Loss_D: 0.2411 Loss_G: 1.2722\n===> Epoch[239](77/88): Loss_D: 0.2466 Loss_G: 1.1165\n===> Epoch[239](78/88): Loss_D: 0.2396 Loss_G: 1.2971\n===> Epoch[239](79/88): Loss_D: 0.2496 Loss_G: 1.0134\n===> Epoch[239](80/88): Loss_D: 0.2386 Loss_G: 1.3068\n===> Epoch[239](81/88): Loss_D: 0.2402 Loss_G: 1.1674\n===> Epoch[239](82/88): Loss_D: 0.2416 Loss_G: 1.2144\n===> Epoch[239](83/88): Loss_D: 0.2433 Loss_G: 1.1714\n===> Epoch[239](84/88): Loss_D: 0.2454 Loss_G: 1.2148\n===> Epoch[239](85/88): Loss_D: 0.2528 Loss_G: 0.9797\n===> Epoch[239](86/88): Loss_D: 0.2565 Loss_G: 1.2612\n===> Epoch[239](87/88): Loss_D: 0.2326 Loss_G: 1.9045\n===> Epoch[239](88/88): Loss_D: 0.2480 Loss_G: 1.0545\nlearning rate = 0.0000404\nlearning rate = 0.0000404\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0232} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[240](1/88): Loss_D: 0.2513 Loss_G: 1.0482\n===> Epoch[240](2/88): Loss_D: 0.2343 Loss_G: 1.3155\n===> Epoch[240](3/88): Loss_D: 0.2499 Loss_G: 1.1884\n===> Epoch[240](4/88): Loss_D: 0.1880 Loss_G: 1.7248\n===> Epoch[240](5/88): Loss_D: 0.2538 Loss_G: 1.2633\n===> Epoch[240](6/88): Loss_D: 0.2332 Loss_G: 1.2813\n===> Epoch[240](7/88): Loss_D: 0.2546 Loss_G: 1.2264\n===> Epoch[240](8/88): Loss_D: 0.2382 Loss_G: 1.2095\n===> Epoch[240](9/88): Loss_D: 0.2545 Loss_G: 0.8955\n===> Epoch[240](10/88): Loss_D: 0.2236 Loss_G: 1.1561\n===> Epoch[240](11/88): Loss_D: 0.2624 Loss_G: 1.0234\n===> Epoch[240](12/88): Loss_D: 0.2457 Loss_G: 1.1396\n===> Epoch[240](13/88): Loss_D: 0.2567 Loss_G: 1.0645\n===> Epoch[240](14/88): Loss_D: 0.2481 Loss_G: 1.2225\n===> Epoch[240](15/88): Loss_D: 0.2551 Loss_G: 1.0274\n===> Epoch[240](16/88): Loss_D: 0.2383 Loss_G: 1.1081\n===> Epoch[240](17/88): Loss_D: 0.2384 Loss_G: 1.4526\n===> Epoch[240](18/88): Loss_D: 0.2520 Loss_G: 1.0034\n===> Epoch[240](19/88): Loss_D: 0.2209 Loss_G: 1.1883\n===> Epoch[240](20/88): Loss_D: 0.2349 Loss_G: 1.2747\n===> Epoch[240](21/88): Loss_D: 0.2562 Loss_G: 1.0244\n===> Epoch[240](22/88): Loss_D: 0.2631 Loss_G: 1.1319\n===> Epoch[240](23/88): Loss_D: 0.2304 Loss_G: 1.9177\n===> Epoch[240](24/88): Loss_D: 0.2540 Loss_G: 1.2049\n===> Epoch[240](25/88): Loss_D: 0.2538 Loss_G: 1.2051\n===> Epoch[240](26/88): Loss_D: 0.2326 Loss_G: 1.1789\n===> Epoch[240](27/88): Loss_D: 0.2451 Loss_G: 1.3687\n===> Epoch[240](28/88): Loss_D: 0.2466 Loss_G: 1.2149\n===> Epoch[240](29/88): Loss_D: 0.2346 Loss_G: 1.4175\n===> Epoch[240](30/88): Loss_D: 0.2333 Loss_G: 1.1475\n===> Epoch[240](31/88): Loss_D: 0.1804 Loss_G: 1.4848\n===> Epoch[240](32/88): Loss_D: 0.2329 Loss_G: 1.4455\n===> Epoch[240](33/88): Loss_D: 0.2493 Loss_G: 1.1383\n===> Epoch[240](34/88): Loss_D: 0.2505 Loss_G: 1.2097\n===> Epoch[240](35/88): Loss_D: 0.2538 Loss_G: 1.0496\n===> Epoch[240](36/88): Loss_D: 0.2483 Loss_G: 1.1906\n===> Epoch[240](37/88): Loss_D: 0.2468 Loss_G: 1.3197\n===> Epoch[240](38/88): Loss_D: 0.2301 Loss_G: 1.3293\n===> Epoch[240](39/88): Loss_D: 0.2477 Loss_G: 1.1647\n===> Epoch[240](40/88): Loss_D: 0.2380 Loss_G: 1.3557\n===> Epoch[240](41/88): Loss_D: 0.2415 Loss_G: 1.2843\n===> Epoch[240](42/88): Loss_D: 0.2625 Loss_G: 1.2105\n===> Epoch[240](43/88): Loss_D: 0.2510 Loss_G: 1.1185\n===> Epoch[240](44/88): Loss_D: 0.1820 Loss_G: 1.5170\n===> Epoch[240](45/88): Loss_D: 0.2481 Loss_G: 1.2600\n===> Epoch[240](46/88): Loss_D: 0.2461 Loss_G: 1.0811\n===> Epoch[240](47/88): Loss_D: 0.2140 Loss_G: 1.4174\n===> Epoch[240](48/88): Loss_D: 0.2449 Loss_G: 1.1510\n===> Epoch[240](49/88): Loss_D: 0.2488 Loss_G: 1.0945\n===> Epoch[240](50/88): Loss_D: 0.2334 Loss_G: 1.0982\n===> Epoch[240](51/88): Loss_D: 0.2367 Loss_G: 1.3098\n===> Epoch[240](52/88): Loss_D: 0.2430 Loss_G: 1.1433\n===> Epoch[240](53/88): Loss_D: 0.2540 Loss_G: 1.1549\n===> Epoch[240](54/88): Loss_D: 0.2420 Loss_G: 1.1264\n===> Epoch[240](55/88): Loss_D: 0.2530 Loss_G: 1.2085\n===> Epoch[240](56/88): Loss_D: 0.2573 Loss_G: 1.1524\n===> Epoch[240](57/88): Loss_D: 0.2455 Loss_G: 1.2210\n===> Epoch[240](58/88): Loss_D: 0.2449 Loss_G: 1.2097\n===> Epoch[240](59/88): Loss_D: 0.2372 Loss_G: 1.3464\n===> Epoch[240](60/88): Loss_D: 0.2370 Loss_G: 1.3803\n===> Epoch[240](61/88): Loss_D: 0.2505 Loss_G: 1.0873\n===> Epoch[240](62/88): Loss_D: 0.2635 Loss_G: 0.9986\n===> Epoch[240](63/88): Loss_D: 0.2470 Loss_G: 1.2277\n===> Epoch[240](64/88): Loss_D: 0.2678 Loss_G: 1.1008\n===> Epoch[240](65/88): Loss_D: 0.2513 Loss_G: 1.0445\n===> Epoch[240](66/88): Loss_D: 0.2404 Loss_G: 1.2870\n===> Epoch[240](67/88): Loss_D: 0.2485 Loss_G: 1.2125\n===> Epoch[240](68/88): Loss_D: 0.2237 Loss_G: 1.3595\n===> Epoch[240](69/88): Loss_D: 0.2280 Loss_G: 1.3715\n===> Epoch[240](70/88): Loss_D: 0.2476 Loss_G: 1.0717\n===> Epoch[240](71/88): Loss_D: 0.2394 Loss_G: 1.2898\n===> Epoch[240](72/88): Loss_D: 0.2420 Loss_G: 1.2063\n===> Epoch[240](73/88): Loss_D: 0.2595 Loss_G: 1.2144\n===> Epoch[240](74/88): Loss_D: 0.2527 Loss_G: 1.2649\n===> Epoch[240](75/88): Loss_D: 0.2442 Loss_G: 1.1992\n===> Epoch[240](76/88): Loss_D: 0.2435 Loss_G: 1.1485\n===> Epoch[240](77/88): Loss_D: 0.2397 Loss_G: 1.2972\n===> Epoch[240](78/88): Loss_D: 0.2530 Loss_G: 1.1380\n===> Epoch[240](79/88): Loss_D: 0.2522 Loss_G: 1.0968\n===> Epoch[240](80/88): Loss_D: 0.2373 Loss_G: 1.2621\n===> Epoch[240](81/88): Loss_D: 0.2380 Loss_G: 1.1849\n===> Epoch[240](82/88): Loss_D: 0.2468 Loss_G: 1.3069\n===> Epoch[240](83/88): Loss_D: 0.2476 Loss_G: 1.0134\n===> Epoch[240](84/88): Loss_D: 0.2527 Loss_G: 1.1693\n===> Epoch[240](85/88): Loss_D: 0.2512 Loss_G: 0.9393\n===> Epoch[240](86/88): Loss_D: 0.1651 Loss_G: 1.5424\n===> Epoch[240](87/88): Loss_D: 0.2525 Loss_G: 1.0666\n===> Epoch[240](88/88): Loss_D: 0.2523 Loss_G: 1.0569\nlearning rate = 0.0000401\nlearning rate = 0.0000401\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0011} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[241](1/88): Loss_D: 0.2565 Loss_G: 0.9933\n===> Epoch[241](2/88): Loss_D: 0.2508 Loss_G: 1.1259\n===> Epoch[241](3/88): Loss_D: 0.2508 Loss_G: 1.0428\n===> Epoch[241](4/88): Loss_D: 0.2340 Loss_G: 1.3382\n===> Epoch[241](5/88): Loss_D: 0.2433 Loss_G: 1.2254\n===> Epoch[241](6/88): Loss_D: 0.2562 Loss_G: 1.2321\n===> Epoch[241](7/88): Loss_D: 0.2455 Loss_G: 1.0630\n===> Epoch[241](8/88): Loss_D: 0.2461 Loss_G: 1.1111\n===> Epoch[241](9/88): Loss_D: 0.2442 Loss_G: 1.1306\n===> Epoch[241](10/88): Loss_D: 0.2341 Loss_G: 1.0707\n===> Epoch[241](11/88): Loss_D: 0.2408 Loss_G: 1.1056\n===> Epoch[241](12/88): Loss_D: 0.2363 Loss_G: 1.3845\n===> Epoch[241](13/88): Loss_D: 0.2382 Loss_G: 1.2121\n===> Epoch[241](14/88): Loss_D: 0.2226 Loss_G: 1.2239\n===> Epoch[241](15/88): Loss_D: 0.2400 Loss_G: 1.1174\n===> Epoch[241](16/88): Loss_D: 0.2348 Loss_G: 1.2502\n===> Epoch[241](17/88): Loss_D: 0.2465 Loss_G: 1.1559\n===> Epoch[241](18/88): Loss_D: 0.2433 Loss_G: 1.0401\n===> Epoch[241](19/88): Loss_D: 0.2409 Loss_G: 1.0954\n===> Epoch[241](20/88): Loss_D: 0.2333 Loss_G: 1.2268\n===> Epoch[241](21/88): Loss_D: 0.2470 Loss_G: 1.1449\n===> Epoch[241](22/88): Loss_D: 0.2010 Loss_G: 1.4786\n===> Epoch[241](23/88): Loss_D: 0.2571 Loss_G: 1.0960\n===> Epoch[241](24/88): Loss_D: 0.2574 Loss_G: 1.0950\n===> Epoch[241](25/88): Loss_D: 0.2383 Loss_G: 1.2861\n===> Epoch[241](26/88): Loss_D: 0.2112 Loss_G: 1.3637\n===> Epoch[241](27/88): Loss_D: 0.2344 Loss_G: 1.3733\n===> Epoch[241](28/88): Loss_D: 0.2417 Loss_G: 1.2510\n===> Epoch[241](29/88): Loss_D: 0.2435 Loss_G: 1.2344\n===> Epoch[241](30/88): Loss_D: 0.2330 Loss_G: 1.3600\n===> Epoch[241](31/88): Loss_D: 0.2467 Loss_G: 1.1569\n===> Epoch[241](32/88): Loss_D: 0.2443 Loss_G: 1.1433\n===> Epoch[241](33/88): Loss_D: 0.2421 Loss_G: 1.1697\n===> Epoch[241](34/88): Loss_D: 0.2286 Loss_G: 1.3217\n===> Epoch[241](35/88): Loss_D: 0.2555 Loss_G: 0.9968\n===> Epoch[241](36/88): Loss_D: 0.1716 Loss_G: 1.4572\n===> Epoch[241](37/88): Loss_D: 0.2533 Loss_G: 1.2769\n===> Epoch[241](38/88): Loss_D: 0.1783 Loss_G: 1.4783\n===> Epoch[241](39/88): Loss_D: 0.2478 Loss_G: 1.0461\n===> Epoch[241](40/88): Loss_D: 0.2663 Loss_G: 1.0669\n===> Epoch[241](41/88): Loss_D: 0.2375 Loss_G: 1.2574\n===> Epoch[241](42/88): Loss_D: 0.2433 Loss_G: 1.5078\n===> Epoch[241](43/88): Loss_D: 0.2460 Loss_G: 1.2432\n===> Epoch[241](44/88): Loss_D: 0.2515 Loss_G: 1.1330\n===> Epoch[241](45/88): Loss_D: 0.2464 Loss_G: 1.1265\n===> Epoch[241](46/88): Loss_D: 0.2482 Loss_G: 1.1383\n===> Epoch[241](47/88): Loss_D: 0.2443 Loss_G: 1.2274\n===> Epoch[241](48/88): Loss_D: 0.2386 Loss_G: 1.0689\n===> Epoch[241](49/88): Loss_D: 0.2598 Loss_G: 1.0085\n===> Epoch[241](50/88): Loss_D: 0.2542 Loss_G: 1.1961\n===> Epoch[241](51/88): Loss_D: 0.2388 Loss_G: 1.1011\n===> Epoch[241](52/88): Loss_D: 0.1865 Loss_G: 1.5793\n===> Epoch[241](53/88): Loss_D: 0.2474 Loss_G: 1.1624\n===> Epoch[241](54/88): Loss_D: 0.2557 Loss_G: 1.2869\n===> Epoch[241](55/88): Loss_D: 0.1986 Loss_G: 1.6473\n===> Epoch[241](56/88): Loss_D: 0.2451 Loss_G: 1.3145\n===> Epoch[241](57/88): Loss_D: 0.2550 Loss_G: 1.2329\n===> Epoch[241](58/88): Loss_D: 0.2560 Loss_G: 1.1494\n===> Epoch[241](59/88): Loss_D: 0.2507 Loss_G: 1.2001\n===> Epoch[241](60/88): Loss_D: 0.2319 Loss_G: 1.1932\n===> Epoch[241](61/88): Loss_D: 0.2585 Loss_G: 1.0996\n===> Epoch[241](62/88): Loss_D: 0.2017 Loss_G: 2.1128\n===> Epoch[241](63/88): Loss_D: 0.2555 Loss_G: 1.1907\n===> Epoch[241](64/88): Loss_D: 0.2440 Loss_G: 1.1831\n===> Epoch[241](65/88): Loss_D: 0.2392 Loss_G: 1.2609\n===> Epoch[241](66/88): Loss_D: 0.2559 Loss_G: 0.9349\n===> Epoch[241](67/88): Loss_D: 0.2510 Loss_G: 1.1434\n===> Epoch[241](68/88): Loss_D: 0.2631 Loss_G: 1.0374\n===> Epoch[241](69/88): Loss_D: 0.2538 Loss_G: 1.2073\n===> Epoch[241](70/88): Loss_D: 0.2512 Loss_G: 1.1731\n===> Epoch[241](71/88): Loss_D: 0.2371 Loss_G: 1.2722\n===> Epoch[241](72/88): Loss_D: 0.2578 Loss_G: 0.9630\n===> Epoch[241](73/88): Loss_D: 0.2301 Loss_G: 1.3707\n===> Epoch[241](74/88): Loss_D: 0.2605 Loss_G: 1.1207\n===> Epoch[241](75/88): Loss_D: 0.2564 Loss_G: 1.2524\n===> Epoch[241](76/88): Loss_D: 0.2485 Loss_G: 1.0984\n===> Epoch[241](77/88): Loss_D: 0.2430 Loss_G: 1.1450\n===> Epoch[241](78/88): Loss_D: 0.2489 Loss_G: 1.1290\n===> Epoch[241](79/88): Loss_D: 0.2451 Loss_G: 1.2242\n===> Epoch[241](80/88): Loss_D: 0.2506 Loss_G: 1.2128\n===> Epoch[241](81/88): Loss_D: 0.2382 Loss_G: 1.2483\n===> Epoch[241](82/88): Loss_D: 0.2561 Loss_G: 1.0385\n===> Epoch[241](83/88): Loss_D: 0.2394 Loss_G: 1.3167\n===> Epoch[241](84/88): Loss_D: 0.2645 Loss_G: 1.1353\n===> Epoch[241](85/88): Loss_D: 0.2509 Loss_G: 1.1110\n===> Epoch[241](86/88): Loss_D: 0.2281 Loss_G: 1.4758\n===> Epoch[241](87/88): Loss_D: 0.1980 Loss_G: 1.5029\n===> Epoch[241](88/88): Loss_D: 0.2405 Loss_G: 1.4120\nlearning rate = 0.0000399\nlearning rate = 0.0000399\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0000} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[242](1/88): Loss_D: 0.2385 Loss_G: 1.4045\n===> Epoch[242](2/88): Loss_D: 0.2401 Loss_G: 1.3144\n===> Epoch[242](3/88): Loss_D: 0.2488 Loss_G: 1.1522\n===> Epoch[242](4/88): Loss_D: 0.2509 Loss_G: 1.0793\n===> Epoch[242](5/88): Loss_D: 0.2459 Loss_G: 1.4806\n===> Epoch[242](6/88): Loss_D: 0.2568 Loss_G: 1.1963\n===> Epoch[242](7/88): Loss_D: 0.2379 Loss_G: 1.1214\n===> Epoch[242](8/88): Loss_D: 0.2527 Loss_G: 1.2372\n===> Epoch[242](9/88): Loss_D: 0.1691 Loss_G: 1.5037\n===> Epoch[242](10/88): Loss_D: 0.2595 Loss_G: 1.1316\n===> Epoch[242](11/88): Loss_D: 0.2493 Loss_G: 1.2256\n===> Epoch[242](12/88): Loss_D: 0.2018 Loss_G: 1.7038\n===> Epoch[242](13/88): Loss_D: 0.1910 Loss_G: 1.5978\n===> Epoch[242](14/88): Loss_D: 0.2587 Loss_G: 1.0145\n===> Epoch[242](15/88): Loss_D: 0.2362 Loss_G: 1.2185\n===> Epoch[242](16/88): Loss_D: 0.2547 Loss_G: 1.2511\n===> Epoch[242](17/88): Loss_D: 0.2643 Loss_G: 1.2244\n===> Epoch[242](18/88): Loss_D: 0.2204 Loss_G: 1.3557\n===> Epoch[242](19/88): Loss_D: 0.2342 Loss_G: 1.0712\n===> Epoch[242](20/88): Loss_D: 0.2459 Loss_G: 1.1622\n===> Epoch[242](21/88): Loss_D: 0.2559 Loss_G: 1.0603\n===> Epoch[242](22/88): Loss_D: 0.2500 Loss_G: 1.1718\n===> Epoch[242](23/88): Loss_D: 0.2391 Loss_G: 1.0402\n===> Epoch[242](24/88): Loss_D: 0.2472 Loss_G: 1.1174\n===> Epoch[242](25/88): Loss_D: 0.2484 Loss_G: 1.1707\n===> Epoch[242](26/88): Loss_D: 0.2515 Loss_G: 1.1453\n===> Epoch[242](27/88): Loss_D: 0.2376 Loss_G: 1.2786\n===> Epoch[242](28/88): Loss_D: 0.2608 Loss_G: 0.9794\n===> Epoch[242](29/88): Loss_D: 0.2270 Loss_G: 1.4446\n===> Epoch[242](30/88): Loss_D: 0.2455 Loss_G: 1.3063\n===> Epoch[242](31/88): Loss_D: 0.2459 Loss_G: 1.1394\n===> Epoch[242](32/88): Loss_D: 0.2332 Loss_G: 1.2725\n===> Epoch[242](33/88): Loss_D: 0.2413 Loss_G: 1.1198\n===> Epoch[242](34/88): Loss_D: 0.2393 Loss_G: 1.2548\n===> Epoch[242](35/88): Loss_D: 0.2509 Loss_G: 1.1838\n===> Epoch[242](36/88): Loss_D: 0.2579 Loss_G: 1.1353\n===> Epoch[242](37/88): Loss_D: 0.2404 Loss_G: 1.1551\n===> Epoch[242](38/88): Loss_D: 0.2403 Loss_G: 1.1924\n===> Epoch[242](39/88): Loss_D: 0.2450 Loss_G: 1.3570\n===> Epoch[242](40/88): Loss_D: 0.2553 Loss_G: 1.1095\n===> Epoch[242](41/88): Loss_D: 0.2417 Loss_G: 1.1963\n===> Epoch[242](42/88): Loss_D: 0.2313 Loss_G: 1.3723\n===> Epoch[242](43/88): Loss_D: 0.2535 Loss_G: 0.9920\n===> Epoch[242](44/88): Loss_D: 0.2521 Loss_G: 1.0861\n===> Epoch[242](45/88): Loss_D: 0.2389 Loss_G: 1.2307\n===> Epoch[242](46/88): Loss_D: 0.2509 Loss_G: 1.0515\n===> Epoch[242](47/88): Loss_D: 0.2371 Loss_G: 1.2791\n===> Epoch[242](48/88): Loss_D: 0.2400 Loss_G: 1.2059\n===> Epoch[242](49/88): Loss_D: 0.2479 Loss_G: 1.1498\n===> Epoch[242](50/88): Loss_D: 0.2063 Loss_G: 1.5099\n===> Epoch[242](51/88): Loss_D: 0.2373 Loss_G: 1.2058\n===> Epoch[242](52/88): Loss_D: 0.2480 Loss_G: 1.2518\n===> Epoch[242](53/88): Loss_D: 0.2509 Loss_G: 1.2221\n===> Epoch[242](54/88): Loss_D: 0.2615 Loss_G: 1.0253\n===> Epoch[242](55/88): Loss_D: 0.2441 Loss_G: 1.0122\n===> Epoch[242](56/88): Loss_D: 0.2495 Loss_G: 1.0216\n===> Epoch[242](57/88): Loss_D: 0.2594 Loss_G: 1.0519\n===> Epoch[242](58/88): Loss_D: 0.2508 Loss_G: 1.2129\n===> Epoch[242](59/88): Loss_D: 0.2449 Loss_G: 1.1322\n===> Epoch[242](60/88): Loss_D: 0.2014 Loss_G: 2.1066\n===> Epoch[242](61/88): Loss_D: 0.2346 Loss_G: 1.2169\n===> Epoch[242](62/88): Loss_D: 0.2542 Loss_G: 1.1311\n===> Epoch[242](63/88): Loss_D: 0.2355 Loss_G: 1.1635\n===> Epoch[242](64/88): Loss_D: 0.2317 Loss_G: 1.4102\n===> Epoch[242](65/88): Loss_D: 0.2553 Loss_G: 1.2325\n===> Epoch[242](66/88): Loss_D: 0.2566 Loss_G: 1.0574\n===> Epoch[242](67/88): Loss_D: 0.2517 Loss_G: 1.3512\n===> Epoch[242](68/88): Loss_D: 0.2390 Loss_G: 1.2243\n===> Epoch[242](69/88): Loss_D: 0.2297 Loss_G: 1.3238\n===> Epoch[242](70/88): Loss_D: 0.2321 Loss_G: 1.3283\n===> Epoch[242](71/88): Loss_D: 0.2594 Loss_G: 1.0560\n===> Epoch[242](72/88): Loss_D: 0.2121 Loss_G: 1.3919\n===> Epoch[242](73/88): Loss_D: 0.2495 Loss_G: 1.1653\n===> Epoch[242](74/88): Loss_D: 0.2578 Loss_G: 1.0483\n===> Epoch[242](75/88): Loss_D: 0.2441 Loss_G: 1.1405\n===> Epoch[242](76/88): Loss_D: 0.2498 Loss_G: 1.1268\n===> Epoch[242](77/88): Loss_D: 0.2347 Loss_G: 1.1588\n===> Epoch[242](78/88): Loss_D: 0.2492 Loss_G: 1.1567\n===> Epoch[242](79/88): Loss_D: 0.2384 Loss_G: 1.1554\n===> Epoch[242](80/88): Loss_D: 0.2505 Loss_G: 1.2260\n===> Epoch[242](81/88): Loss_D: 0.2498 Loss_G: 1.1941\n===> Epoch[242](82/88): Loss_D: 0.2378 Loss_G: 1.1823\n===> Epoch[242](83/88): Loss_D: 0.2515 Loss_G: 0.8966\n===> Epoch[242](84/88): Loss_D: 0.2378 Loss_G: 1.2080\n===> Epoch[242](85/88): Loss_D: 0.1716 Loss_G: 1.5182\n===> Epoch[242](86/88): Loss_D: 0.2376 Loss_G: 1.1660\n===> Epoch[242](87/88): Loss_D: 0.2435 Loss_G: 1.3456\n===> Epoch[242](88/88): Loss_D: 0.2429 Loss_G: 1.0969\nlearning rate = 0.0000397\nlearning rate = 0.0000397\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0101} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[243](1/88): Loss_D: 0.2002 Loss_G: 1.4642\n===> Epoch[243](2/88): Loss_D: 0.2459 Loss_G: 1.1062\n===> Epoch[243](3/88): Loss_D: 0.2486 Loss_G: 1.1582\n===> Epoch[243](4/88): Loss_D: 0.2382 Loss_G: 1.3061\n===> Epoch[243](5/88): Loss_D: 0.1803 Loss_G: 1.5316\n===> Epoch[243](6/88): Loss_D: 0.2358 Loss_G: 1.2924\n===> Epoch[243](7/88): Loss_D: 0.2384 Loss_G: 1.3170\n===> Epoch[243](8/88): Loss_D: 0.2593 Loss_G: 1.1784\n===> Epoch[243](9/88): Loss_D: 0.2503 Loss_G: 1.2481\n===> Epoch[243](10/88): Loss_D: 0.2423 Loss_G: 1.0646\n===> Epoch[243](11/88): Loss_D: 0.2330 Loss_G: 1.3881\n===> Epoch[243](12/88): Loss_D: 0.2588 Loss_G: 1.1635\n===> Epoch[243](13/88): Loss_D: 0.2321 Loss_G: 1.2781\n===> Epoch[243](14/88): Loss_D: 0.2504 Loss_G: 1.2064\n===> Epoch[243](15/88): Loss_D: 0.1711 Loss_G: 1.5303\n===> Epoch[243](16/88): Loss_D: 0.2614 Loss_G: 1.2460\n===> Epoch[243](17/88): Loss_D: 0.2151 Loss_G: 1.3945\n===> Epoch[243](18/88): Loss_D: 0.2571 Loss_G: 1.2823\n===> Epoch[243](19/88): Loss_D: 0.2343 Loss_G: 1.3622\n===> Epoch[243](20/88): Loss_D: 0.2489 Loss_G: 1.1424\n===> Epoch[243](21/88): Loss_D: 0.2386 Loss_G: 1.4435\n===> Epoch[243](22/88): Loss_D: 0.2489 Loss_G: 1.1965\n===> Epoch[243](23/88): Loss_D: 0.2473 Loss_G: 1.2289\n===> Epoch[243](24/88): Loss_D: 0.2578 Loss_G: 1.1498\n===> Epoch[243](25/88): Loss_D: 0.2579 Loss_G: 0.9601\n===> Epoch[243](26/88): Loss_D: 0.2609 Loss_G: 1.0666\n===> Epoch[243](27/88): Loss_D: 0.2502 Loss_G: 1.1080\n===> Epoch[243](28/88): Loss_D: 0.2470 Loss_G: 1.1887\n===> Epoch[243](29/88): Loss_D: 0.2507 Loss_G: 1.0111\n===> Epoch[243](30/88): Loss_D: 0.2516 Loss_G: 1.0962\n===> Epoch[243](31/88): Loss_D: 0.2416 Loss_G: 1.1576\n===> Epoch[243](32/88): Loss_D: 0.1883 Loss_G: 1.5992\n===> Epoch[243](33/88): Loss_D: 0.2478 Loss_G: 1.2738\n===> Epoch[243](34/88): Loss_D: 0.2432 Loss_G: 1.1250\n===> Epoch[243](35/88): Loss_D: 0.2574 Loss_G: 0.8707\n===> Epoch[243](36/88): Loss_D: 0.2545 Loss_G: 1.1464\n===> Epoch[243](37/88): Loss_D: 0.2403 Loss_G: 1.2321\n===> Epoch[243](38/88): Loss_D: 0.2582 Loss_G: 0.9691\n===> Epoch[243](39/88): Loss_D: 0.2479 Loss_G: 1.1401\n===> Epoch[243](40/88): Loss_D: 0.2479 Loss_G: 1.2030\n===> Epoch[243](41/88): Loss_D: 0.2642 Loss_G: 1.0530\n===> Epoch[243](42/88): Loss_D: 0.2379 Loss_G: 1.2173\n===> Epoch[243](43/88): Loss_D: 0.2294 Loss_G: 1.4197\n===> Epoch[243](44/88): Loss_D: 0.2460 Loss_G: 1.1502\n===> Epoch[243](45/88): Loss_D: 0.2244 Loss_G: 1.9061\n===> Epoch[243](46/88): Loss_D: 0.2395 Loss_G: 1.2945\n===> Epoch[243](47/88): Loss_D: 0.2444 Loss_G: 1.2473\n===> Epoch[243](48/88): Loss_D: 0.2345 Loss_G: 1.3455\n===> Epoch[243](49/88): Loss_D: 0.2279 Loss_G: 1.4093\n===> Epoch[243](50/88): Loss_D: 0.2542 Loss_G: 1.1944\n===> Epoch[243](51/88): Loss_D: 0.2353 Loss_G: 1.1475\n===> Epoch[243](52/88): Loss_D: 0.2349 Loss_G: 1.1743\n===> Epoch[243](53/88): Loss_D: 0.2348 Loss_G: 1.2912\n===> Epoch[243](54/88): Loss_D: 0.2476 Loss_G: 1.1936\n===> Epoch[243](55/88): Loss_D: 0.2383 Loss_G: 1.2396\n===> Epoch[243](56/88): Loss_D: 0.2511 Loss_G: 1.0495\n===> Epoch[243](57/88): Loss_D: 0.2397 Loss_G: 1.3164\n===> Epoch[243](58/88): Loss_D: 0.2554 Loss_G: 1.0385\n===> Epoch[243](59/88): Loss_D: 0.2476 Loss_G: 1.1009\n===> Epoch[243](60/88): Loss_D: 0.2424 Loss_G: 1.1197\n===> Epoch[243](61/88): Loss_D: 0.2473 Loss_G: 1.1656\n===> Epoch[243](62/88): Loss_D: 0.2358 Loss_G: 1.1529\n===> Epoch[243](63/88): Loss_D: 0.2519 Loss_G: 1.0541\n===> Epoch[243](64/88): Loss_D: 0.2520 Loss_G: 1.1150\n===> Epoch[243](65/88): Loss_D: 0.2516 Loss_G: 1.1539\n===> Epoch[243](66/88): Loss_D: 0.2333 Loss_G: 1.2816\n===> Epoch[243](67/88): Loss_D: 0.2355 Loss_G: 1.4365\n===> Epoch[243](68/88): Loss_D: 0.2496 Loss_G: 1.0573\n===> Epoch[243](69/88): Loss_D: 0.2131 Loss_G: 1.3750\n===> Epoch[243](70/88): Loss_D: 0.2525 Loss_G: 0.9662\n===> Epoch[243](71/88): Loss_D: 0.2468 Loss_G: 1.1909\n===> Epoch[243](72/88): Loss_D: 0.2513 Loss_G: 1.1558\n===> Epoch[243](73/88): Loss_D: 0.2368 Loss_G: 1.3030\n===> Epoch[243](74/88): Loss_D: 0.2504 Loss_G: 1.2215\n===> Epoch[243](75/88): Loss_D: 0.2551 Loss_G: 1.0179\n===> Epoch[243](76/88): Loss_D: 0.2506 Loss_G: 1.2107\n===> Epoch[243](77/88): Loss_D: 0.2527 Loss_G: 1.2967\n===> Epoch[243](78/88): Loss_D: 0.2417 Loss_G: 1.0445\n===> Epoch[243](79/88): Loss_D: 0.2483 Loss_G: 1.1091\n===> Epoch[243](80/88): Loss_D: 0.2274 Loss_G: 1.0293\n===> Epoch[243](81/88): Loss_D: 0.2412 Loss_G: 1.3271\n===> Epoch[243](82/88): Loss_D: 0.2340 Loss_G: 1.3042\n===> Epoch[243](83/88): Loss_D: 0.2398 Loss_G: 1.1824\n===> Epoch[243](84/88): Loss_D: 0.2487 Loss_G: 1.1618\n===> Epoch[243](85/88): Loss_D: 0.1951 Loss_G: 1.7657\n===> Epoch[243](86/88): Loss_D: 0.2538 Loss_G: 1.0912\n===> Epoch[243](87/88): Loss_D: 0.2517 Loss_G: 1.1216\n===> Epoch[243](88/88): Loss_D: 0.2560 Loss_G: 1.1007\nlearning rate = 0.0000394\nlearning rate = 0.0000394\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0003} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[244](1/88): Loss_D: 0.2324 Loss_G: 1.1093\n===> Epoch[244](2/88): Loss_D: 0.2485 Loss_G: 1.1182\n===> Epoch[244](3/88): Loss_D: 0.2416 Loss_G: 1.1067\n===> Epoch[244](4/88): Loss_D: 0.2655 Loss_G: 0.9017\n===> Epoch[244](5/88): Loss_D: 0.1922 Loss_G: 1.4638\n===> Epoch[244](6/88): Loss_D: 0.2546 Loss_G: 1.1921\n===> Epoch[244](7/88): Loss_D: 0.2409 Loss_G: 1.2419\n===> Epoch[244](8/88): Loss_D: 0.2527 Loss_G: 1.1219\n===> Epoch[244](9/88): Loss_D: 0.2564 Loss_G: 0.9733\n===> Epoch[244](10/88): Loss_D: 0.2431 Loss_G: 1.2202\n===> Epoch[244](11/88): Loss_D: 0.2589 Loss_G: 1.0169\n===> Epoch[244](12/88): Loss_D: 0.2486 Loss_G: 1.2168\n===> Epoch[244](13/88): Loss_D: 0.2508 Loss_G: 1.0487\n===> Epoch[244](14/88): Loss_D: 0.2480 Loss_G: 1.0108\n===> Epoch[244](15/88): Loss_D: 0.2474 Loss_G: 1.1645\n===> Epoch[244](16/88): Loss_D: 0.2541 Loss_G: 1.1766\n===> Epoch[244](17/88): Loss_D: 0.2480 Loss_G: 1.1933\n===> Epoch[244](18/88): Loss_D: 0.2365 Loss_G: 1.1620\n===> Epoch[244](19/88): Loss_D: 0.2465 Loss_G: 1.1111\n===> Epoch[244](20/88): Loss_D: 0.2347 Loss_G: 1.2466\n===> Epoch[244](21/88): Loss_D: 0.2509 Loss_G: 1.2182\n===> Epoch[244](22/88): Loss_D: 0.2487 Loss_G: 1.1542\n===> Epoch[244](23/88): Loss_D: 0.2470 Loss_G: 1.2272\n===> Epoch[244](24/88): Loss_D: 0.2153 Loss_G: 1.3821\n===> Epoch[244](25/88): Loss_D: 0.2178 Loss_G: 1.3749\n===> Epoch[244](26/88): Loss_D: 0.2591 Loss_G: 1.1429\n===> Epoch[244](27/88): Loss_D: 0.1940 Loss_G: 1.7575\n===> Epoch[244](28/88): Loss_D: 0.2308 Loss_G: 1.1420\n===> Epoch[244](29/88): Loss_D: 0.2623 Loss_G: 1.1206\n===> Epoch[244](30/88): Loss_D: 0.2627 Loss_G: 0.9861\n===> Epoch[244](31/88): Loss_D: 0.2511 Loss_G: 1.1036\n===> Epoch[244](32/88): Loss_D: 0.2437 Loss_G: 1.2496\n===> Epoch[244](33/88): Loss_D: 0.2351 Loss_G: 1.1539\n===> Epoch[244](34/88): Loss_D: 0.2525 Loss_G: 1.0048\n===> Epoch[244](35/88): Loss_D: 0.2119 Loss_G: 2.0392\n===> Epoch[244](36/88): Loss_D: 0.2477 Loss_G: 1.2003\n===> Epoch[244](37/88): Loss_D: 0.2703 Loss_G: 1.2609\n===> Epoch[244](38/88): Loss_D: 0.2383 Loss_G: 1.2145\n===> Epoch[244](39/88): Loss_D: 0.2493 Loss_G: 1.2658\n===> Epoch[244](40/88): Loss_D: 0.2502 Loss_G: 1.1226\n===> Epoch[244](41/88): Loss_D: 0.2332 Loss_G: 1.3411\n===> Epoch[244](42/88): Loss_D: 0.2206 Loss_G: 1.2718\n===> Epoch[244](43/88): Loss_D: 0.2427 Loss_G: 1.3066\n===> Epoch[244](44/88): Loss_D: 0.2419 Loss_G: 1.1760\n===> Epoch[244](45/88): Loss_D: 0.2571 Loss_G: 1.0580\n===> Epoch[244](46/88): Loss_D: 0.2487 Loss_G: 1.1562\n===> Epoch[244](47/88): Loss_D: 0.2477 Loss_G: 0.9973\n===> Epoch[244](48/88): Loss_D: 0.2505 Loss_G: 1.0728\n===> Epoch[244](49/88): Loss_D: 0.2409 Loss_G: 1.1147\n===> Epoch[244](50/88): Loss_D: 0.2301 Loss_G: 1.3969\n===> Epoch[244](51/88): Loss_D: 0.2496 Loss_G: 1.0701\n===> Epoch[244](52/88): Loss_D: 0.2332 Loss_G: 1.2645\n===> Epoch[244](53/88): Loss_D: 0.1852 Loss_G: 1.5697\n===> Epoch[244](54/88): Loss_D: 0.2426 Loss_G: 1.1671\n===> Epoch[244](55/88): Loss_D: 0.2436 Loss_G: 1.4847\n===> Epoch[244](56/88): Loss_D: 0.2292 Loss_G: 1.2870\n===> Epoch[244](57/88): Loss_D: 0.1723 Loss_G: 1.5125\n===> Epoch[244](58/88): Loss_D: 0.2540 Loss_G: 1.1183\n===> Epoch[244](59/88): Loss_D: 0.2515 Loss_G: 1.0559\n===> Epoch[244](60/88): Loss_D: 0.2471 Loss_G: 1.1648\n===> Epoch[244](61/88): Loss_D: 0.2574 Loss_G: 1.1271\n===> Epoch[244](62/88): Loss_D: 0.2520 Loss_G: 1.1165\n===> Epoch[244](63/88): Loss_D: 0.2465 Loss_G: 1.1187\n===> Epoch[244](64/88): Loss_D: 0.2520 Loss_G: 1.2850\n===> Epoch[244](65/88): Loss_D: 0.2447 Loss_G: 1.1650\n===> Epoch[244](66/88): Loss_D: 0.2501 Loss_G: 1.0404\n===> Epoch[244](67/88): Loss_D: 0.2233 Loss_G: 1.0936\n===> Epoch[244](68/88): Loss_D: 0.2400 Loss_G: 1.3314\n===> Epoch[244](69/88): Loss_D: 0.2301 Loss_G: 1.3270\n===> Epoch[244](70/88): Loss_D: 0.2329 Loss_G: 1.4000\n===> Epoch[244](71/88): Loss_D: 0.2424 Loss_G: 1.0439\n===> Epoch[244](72/88): Loss_D: 0.2448 Loss_G: 1.2165\n===> Epoch[244](73/88): Loss_D: 0.2213 Loss_G: 1.3712\n===> Epoch[244](74/88): Loss_D: 0.2332 Loss_G: 1.3118\n===> Epoch[244](75/88): Loss_D: 0.2505 Loss_G: 1.0821\n===> Epoch[244](76/88): Loss_D: 0.2345 Loss_G: 1.2163\n===> Epoch[244](77/88): Loss_D: 0.2548 Loss_G: 1.0794\n===> Epoch[244](78/88): Loss_D: 0.2438 Loss_G: 1.1146\n===> Epoch[244](79/88): Loss_D: 0.2545 Loss_G: 1.1479\n===> Epoch[244](80/88): Loss_D: 0.2283 Loss_G: 1.4536\n===> Epoch[244](81/88): Loss_D: 0.2455 Loss_G: 1.0929\n===> Epoch[244](82/88): Loss_D: 0.2558 Loss_G: 1.1445\n===> Epoch[244](83/88): Loss_D: 0.2348 Loss_G: 1.3065\n===> Epoch[244](84/88): Loss_D: 0.2458 Loss_G: 1.3473\n===> Epoch[244](85/88): Loss_D: 0.2337 Loss_G: 1.2926\n===> Epoch[244](86/88): Loss_D: 0.1749 Loss_G: 1.7232\n===> Epoch[244](87/88): Loss_D: 0.2414 Loss_G: 1.2530\n===> Epoch[244](88/88): Loss_D: 0.2546 Loss_G: 0.9760\nlearning rate = 0.0000392\nlearning rate = 0.0000392\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0004} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[245](1/88): Loss_D: 0.2315 Loss_G: 1.3604\n===> Epoch[245](2/88): Loss_D: 0.2442 Loss_G: 1.0324\n===> Epoch[245](3/88): Loss_D: 0.2375 Loss_G: 1.3062\n===> Epoch[245](4/88): Loss_D: 0.2592 Loss_G: 1.0498\n===> Epoch[245](5/88): Loss_D: 0.2505 Loss_G: 1.2009\n===> Epoch[245](6/88): Loss_D: 0.2425 Loss_G: 1.2849\n===> Epoch[245](7/88): Loss_D: 0.2466 Loss_G: 1.1503\n===> Epoch[245](8/88): Loss_D: 0.2508 Loss_G: 1.1650\n===> Epoch[245](9/88): Loss_D: 0.2440 Loss_G: 1.0891\n===> Epoch[245](10/88): Loss_D: 0.2352 Loss_G: 1.2472\n===> Epoch[245](11/88): Loss_D: 0.2471 Loss_G: 1.2411\n===> Epoch[245](12/88): Loss_D: 0.2523 Loss_G: 1.0103\n===> Epoch[245](13/88): Loss_D: 0.2121 Loss_G: 1.3633\n===> Epoch[245](14/88): Loss_D: 0.2395 Loss_G: 1.1175\n===> Epoch[245](15/88): Loss_D: 0.2417 Loss_G: 1.2561\n===> Epoch[245](16/88): Loss_D: 0.2481 Loss_G: 1.1692\n===> Epoch[245](17/88): Loss_D: 0.2516 Loss_G: 1.0948\n===> Epoch[245](18/88): Loss_D: 0.2501 Loss_G: 1.1005\n===> Epoch[245](19/88): Loss_D: 0.2424 Loss_G: 1.2332\n===> Epoch[245](20/88): Loss_D: 0.2142 Loss_G: 1.4463\n===> Epoch[245](21/88): Loss_D: 0.2545 Loss_G: 1.0615\n===> Epoch[245](22/88): Loss_D: 0.2483 Loss_G: 1.1847\n===> Epoch[245](23/88): Loss_D: 0.2329 Loss_G: 1.1407\n===> Epoch[245](24/88): Loss_D: 0.2501 Loss_G: 1.1230\n===> Epoch[245](25/88): Loss_D: 0.1849 Loss_G: 1.6197\n===> Epoch[245](26/88): Loss_D: 0.2500 Loss_G: 1.2780\n===> Epoch[245](27/88): Loss_D: 0.2497 Loss_G: 1.0596\n===> Epoch[245](28/88): Loss_D: 0.2473 Loss_G: 1.0735\n===> Epoch[245](29/88): Loss_D: 0.2520 Loss_G: 1.0782\n===> Epoch[245](30/88): Loss_D: 0.2521 Loss_G: 1.1569\n===> Epoch[245](31/88): Loss_D: 0.2460 Loss_G: 1.3649\n===> Epoch[245](32/88): Loss_D: 0.2270 Loss_G: 1.2718\n===> Epoch[245](33/88): Loss_D: 0.2246 Loss_G: 1.2333\n===> Epoch[245](34/88): Loss_D: 0.2563 Loss_G: 1.1588\n===> Epoch[245](35/88): Loss_D: 0.2551 Loss_G: 1.1957\n===> Epoch[245](36/88): Loss_D: 0.1808 Loss_G: 1.5093\n===> Epoch[245](37/88): Loss_D: 0.2522 Loss_G: 1.2010\n===> Epoch[245](38/88): Loss_D: 0.2506 Loss_G: 1.2451\n===> Epoch[245](39/88): Loss_D: 0.2175 Loss_G: 1.3560\n===> Epoch[245](40/88): Loss_D: 0.2666 Loss_G: 1.0151\n===> Epoch[245](41/88): Loss_D: 0.2628 Loss_G: 1.0476\n===> Epoch[245](42/88): Loss_D: 0.2525 Loss_G: 1.0429\n===> Epoch[245](43/88): Loss_D: 0.2551 Loss_G: 1.2286\n===> Epoch[245](44/88): Loss_D: 0.2539 Loss_G: 0.9774\n===> Epoch[245](45/88): Loss_D: 0.2337 Loss_G: 1.1890\n===> Epoch[245](46/88): Loss_D: 0.2432 Loss_G: 1.1855\n===> Epoch[245](47/88): Loss_D: 0.2402 Loss_G: 1.1728\n===> Epoch[245](48/88): Loss_D: 0.2339 Loss_G: 1.2482\n===> Epoch[245](49/88): Loss_D: 0.2445 Loss_G: 1.3300\n===> Epoch[245](50/88): Loss_D: 0.2506 Loss_G: 1.1088\n===> Epoch[245](51/88): Loss_D: 0.2464 Loss_G: 1.1142\n===> Epoch[245](52/88): Loss_D: 0.2417 Loss_G: 1.1581\n===> Epoch[245](53/88): Loss_D: 0.2538 Loss_G: 1.1223\n===> Epoch[245](54/88): Loss_D: 0.2382 Loss_G: 1.0741\n===> Epoch[245](55/88): Loss_D: 0.2180 Loss_G: 1.9809\n===> Epoch[245](56/88): Loss_D: 0.2507 Loss_G: 1.1693\n===> Epoch[245](57/88): Loss_D: 0.2455 Loss_G: 1.2900\n===> Epoch[245](58/88): Loss_D: 0.2257 Loss_G: 1.3302\n===> Epoch[245](59/88): Loss_D: 0.1794 Loss_G: 1.5911\n===> Epoch[245](60/88): Loss_D: 0.2446 Loss_G: 1.1745\n===> Epoch[245](61/88): Loss_D: 0.2526 Loss_G: 1.0817\n===> Epoch[245](62/88): Loss_D: 0.2569 Loss_G: 0.9128\n===> Epoch[245](63/88): Loss_D: 0.2451 Loss_G: 1.1961\n===> Epoch[245](64/88): Loss_D: 0.2402 Loss_G: 1.2935\n===> Epoch[245](65/88): Loss_D: 0.2582 Loss_G: 1.2131\n===> Epoch[245](66/88): Loss_D: 0.2437 Loss_G: 1.2182\n===> Epoch[245](67/88): Loss_D: 0.2477 Loss_G: 1.2364\n===> Epoch[245](68/88): Loss_D: 0.2441 Loss_G: 1.1398\n===> Epoch[245](69/88): Loss_D: 0.2497 Loss_G: 1.2015\n===> Epoch[245](70/88): Loss_D: 0.2442 Loss_G: 1.1549\n===> Epoch[245](71/88): Loss_D: 0.2544 Loss_G: 1.2232\n===> Epoch[245](72/88): Loss_D: 0.2334 Loss_G: 1.3826\n===> Epoch[245](73/88): Loss_D: 0.2391 Loss_G: 1.4265\n===> Epoch[245](74/88): Loss_D: 0.2382 Loss_G: 1.2452\n===> Epoch[245](75/88): Loss_D: 0.2482 Loss_G: 1.0651\n===> Epoch[245](76/88): Loss_D: 0.2331 Loss_G: 1.1156\n===> Epoch[245](77/88): Loss_D: 0.2629 Loss_G: 1.0344\n===> Epoch[245](78/88): Loss_D: 0.2472 Loss_G: 1.1852\n===> Epoch[245](79/88): Loss_D: 0.2444 Loss_G: 1.1443\n===> Epoch[245](80/88): Loss_D: 0.2025 Loss_G: 1.7018\n===> Epoch[245](81/88): Loss_D: 0.2484 Loss_G: 1.0057\n===> Epoch[245](82/88): Loss_D: 0.2627 Loss_G: 1.0446\n===> Epoch[245](83/88): Loss_D: 0.2506 Loss_G: 1.1127\n===> Epoch[245](84/88): Loss_D: 0.2504 Loss_G: 1.1299\n===> Epoch[245](85/88): Loss_D: 0.2607 Loss_G: 1.1078\n===> Epoch[245](86/88): Loss_D: 0.2521 Loss_G: 1.2031\n===> Epoch[245](87/88): Loss_D: 0.2489 Loss_G: 1.1647\n===> Epoch[245](88/88): Loss_D: 0.2382 Loss_G: 1.1924\nlearning rate = 0.0000389\nlearning rate = 0.0000389\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0127} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[246](1/88): Loss_D: 0.2620 Loss_G: 1.0504\n===> Epoch[246](2/88): Loss_D: 0.2408 Loss_G: 1.1065\n===> Epoch[246](3/88): Loss_D: 0.2497 Loss_G: 1.0555\n===> Epoch[246](4/88): Loss_D: 0.2460 Loss_G: 1.1481\n===> Epoch[246](5/88): Loss_D: 0.1931 Loss_G: 1.5001\n===> Epoch[246](6/88): Loss_D: 0.2415 Loss_G: 1.1153\n===> Epoch[246](7/88): Loss_D: 0.2474 Loss_G: 1.1601\n===> Epoch[246](8/88): Loss_D: 0.2392 Loss_G: 1.2816\n===> Epoch[246](9/88): Loss_D: 0.2444 Loss_G: 1.2592\n===> Epoch[246](10/88): Loss_D: 0.2535 Loss_G: 1.1114\n===> Epoch[246](11/88): Loss_D: 0.2309 Loss_G: 1.2361\n===> Epoch[246](12/88): Loss_D: 0.2416 Loss_G: 1.3069\n===> Epoch[246](13/88): Loss_D: 0.2491 Loss_G: 1.0870\n===> Epoch[246](14/88): Loss_D: 0.2484 Loss_G: 1.0901\n===> Epoch[246](15/88): Loss_D: 0.2525 Loss_G: 1.1867\n===> Epoch[246](16/88): Loss_D: 0.2495 Loss_G: 1.2240\n===> Epoch[246](17/88): Loss_D: 0.2364 Loss_G: 1.2161\n===> Epoch[246](18/88): Loss_D: 0.1860 Loss_G: 1.7195\n===> Epoch[246](19/88): Loss_D: 0.2325 Loss_G: 1.3746\n===> Epoch[246](20/88): Loss_D: 0.2385 Loss_G: 1.3535\n===> Epoch[246](21/88): Loss_D: 0.2357 Loss_G: 1.1801\n===> Epoch[246](22/88): Loss_D: 0.2418 Loss_G: 1.1094\n===> Epoch[246](23/88): Loss_D: 0.2442 Loss_G: 1.2547\n===> Epoch[246](24/88): Loss_D: 0.2379 Loss_G: 1.2320\n===> Epoch[246](25/88): Loss_D: 0.2309 Loss_G: 1.2280\n===> Epoch[246](26/88): Loss_D: 0.2017 Loss_G: 1.4546\n===> Epoch[246](27/88): Loss_D: 0.2544 Loss_G: 1.0841\n===> Epoch[246](28/88): Loss_D: 0.2525 Loss_G: 1.1667\n===> Epoch[246](29/88): Loss_D: 0.2479 Loss_G: 1.3442\n===> Epoch[246](30/88): Loss_D: 0.2530 Loss_G: 1.1334\n===> Epoch[246](31/88): Loss_D: 0.2516 Loss_G: 1.0755\n===> Epoch[246](32/88): Loss_D: 0.2485 Loss_G: 1.1493\n===> Epoch[246](33/88): Loss_D: 0.2584 Loss_G: 1.0495\n===> Epoch[246](34/88): Loss_D: 0.2001 Loss_G: 2.0994\n===> Epoch[246](35/88): Loss_D: 0.2508 Loss_G: 1.1935\n===> Epoch[246](36/88): Loss_D: 0.2596 Loss_G: 1.0704\n===> Epoch[246](37/88): Loss_D: 0.2426 Loss_G: 1.2020\n===> Epoch[246](38/88): Loss_D: 0.2546 Loss_G: 1.0439\n===> Epoch[246](39/88): Loss_D: 0.2507 Loss_G: 1.0842\n===> Epoch[246](40/88): Loss_D: 0.2509 Loss_G: 0.9894\n===> Epoch[246](41/88): Loss_D: 0.2439 Loss_G: 1.0269\n===> Epoch[246](42/88): Loss_D: 0.2465 Loss_G: 1.1645\n===> Epoch[246](43/88): Loss_D: 0.2525 Loss_G: 1.1696\n===> Epoch[246](44/88): Loss_D: 0.2588 Loss_G: 1.0366\n===> Epoch[246](45/88): Loss_D: 0.2428 Loss_G: 1.2556\n===> Epoch[246](46/88): Loss_D: 0.2534 Loss_G: 1.1702\n===> Epoch[246](47/88): Loss_D: 0.2539 Loss_G: 0.8875\n===> Epoch[246](48/88): Loss_D: 0.2459 Loss_G: 1.2121\n===> Epoch[246](49/88): Loss_D: 0.2469 Loss_G: 1.1408\n===> Epoch[246](50/88): Loss_D: 0.2163 Loss_G: 1.4100\n===> Epoch[246](51/88): Loss_D: 0.2489 Loss_G: 1.1675\n===> Epoch[246](52/88): Loss_D: 0.2447 Loss_G: 1.3526\n===> Epoch[246](53/88): Loss_D: 0.2466 Loss_G: 1.1704\n===> Epoch[246](54/88): Loss_D: 0.2478 Loss_G: 1.1719\n===> Epoch[246](55/88): Loss_D: 0.2398 Loss_G: 1.2181\n===> Epoch[246](56/88): Loss_D: 0.2462 Loss_G: 1.0550\n===> Epoch[246](57/88): Loss_D: 0.2313 Loss_G: 1.1928\n===> Epoch[246](58/88): Loss_D: 0.2478 Loss_G: 1.2503\n===> Epoch[246](59/88): Loss_D: 0.2327 Loss_G: 1.3200\n===> Epoch[246](60/88): Loss_D: 0.2314 Loss_G: 1.1594\n===> Epoch[246](61/88): Loss_D: 0.2499 Loss_G: 1.2002\n===> Epoch[246](62/88): Loss_D: 0.2403 Loss_G: 1.2872\n===> Epoch[246](63/88): Loss_D: 0.2509 Loss_G: 1.1314\n===> Epoch[246](64/88): Loss_D: 0.2356 Loss_G: 1.2552\n===> Epoch[246](65/88): Loss_D: 0.2444 Loss_G: 1.1921\n===> Epoch[246](66/88): Loss_D: 0.2512 Loss_G: 1.0636\n===> Epoch[246](67/88): Loss_D: 0.2325 Loss_G: 1.1116\n===> Epoch[246](68/88): Loss_D: 0.2454 Loss_G: 1.0603\n===> Epoch[246](69/88): Loss_D: 0.2311 Loss_G: 1.4506\n===> Epoch[246](70/88): Loss_D: 0.2239 Loss_G: 1.3845\n===> Epoch[246](71/88): Loss_D: 0.2548 Loss_G: 1.1730\n===> Epoch[246](72/88): Loss_D: 0.1919 Loss_G: 1.5390\n===> Epoch[246](73/88): Loss_D: 0.2573 Loss_G: 1.0930\n===> Epoch[246](74/88): Loss_D: 0.2683 Loss_G: 0.9697\n===> Epoch[246](75/88): Loss_D: 0.2338 Loss_G: 1.2951\n===> Epoch[246](76/88): Loss_D: 0.2406 Loss_G: 1.2627\n===> Epoch[246](77/88): Loss_D: 0.2403 Loss_G: 1.1803\n===> Epoch[246](78/88): Loss_D: 0.2507 Loss_G: 1.1658\n===> Epoch[246](79/88): Loss_D: 0.2355 Loss_G: 1.2348\n===> Epoch[246](80/88): Loss_D: 0.1765 Loss_G: 1.5154\n===> Epoch[246](81/88): Loss_D: 0.2437 Loss_G: 1.2020\n===> Epoch[246](82/88): Loss_D: 0.2405 Loss_G: 1.2943\n===> Epoch[246](83/88): Loss_D: 0.2385 Loss_G: 1.0337\n===> Epoch[246](84/88): Loss_D: 0.2536 Loss_G: 1.2521\n===> Epoch[246](85/88): Loss_D: 0.2383 Loss_G: 1.1165\n===> Epoch[246](86/88): Loss_D: 0.2482 Loss_G: 1.1816\n===> Epoch[246](87/88): Loss_D: 0.2542 Loss_G: 1.2069\n===> Epoch[246](88/88): Loss_D: 0.2347 Loss_G: 1.2040\nlearning rate = 0.0000387\nlearning rate = 0.0000387\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0026} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[247](1/88): Loss_D: 0.2520 Loss_G: 1.0234\n===> Epoch[247](2/88): Loss_D: 0.2553 Loss_G: 1.0485\n===> Epoch[247](3/88): Loss_D: 0.2324 Loss_G: 1.2544\n===> Epoch[247](4/88): Loss_D: 0.2301 Loss_G: 1.0941\n===> Epoch[247](5/88): Loss_D: 0.2452 Loss_G: 1.1491\n===> Epoch[247](6/88): Loss_D: 0.2392 Loss_G: 1.4352\n===> Epoch[247](7/88): Loss_D: 0.2588 Loss_G: 1.1935\n===> Epoch[247](8/88): Loss_D: 0.2411 Loss_G: 1.0248\n===> Epoch[247](9/88): Loss_D: 0.2554 Loss_G: 0.9928\n===> Epoch[247](10/88): Loss_D: 0.2615 Loss_G: 1.1324\n===> Epoch[247](11/88): Loss_D: 0.2464 Loss_G: 1.2340\n===> Epoch[247](12/88): Loss_D: 0.2349 Loss_G: 1.3831\n===> Epoch[247](13/88): Loss_D: 0.1643 Loss_G: 1.5241\n===> Epoch[247](14/88): Loss_D: 0.2515 Loss_G: 1.1681\n===> Epoch[247](15/88): Loss_D: 0.2579 Loss_G: 1.0060\n===> Epoch[247](16/88): Loss_D: 0.2540 Loss_G: 1.2322\n===> Epoch[247](17/88): Loss_D: 0.2461 Loss_G: 1.1949\n===> Epoch[247](18/88): Loss_D: 0.2477 Loss_G: 1.1529\n===> Epoch[247](19/88): Loss_D: 0.1864 Loss_G: 1.5968\n===> Epoch[247](20/88): Loss_D: 0.2419 Loss_G: 1.2499\n===> Epoch[247](21/88): Loss_D: 0.2329 Loss_G: 1.1862\n===> Epoch[247](22/88): Loss_D: 0.2460 Loss_G: 1.1878\n===> Epoch[247](23/88): Loss_D: 0.2496 Loss_G: 1.1383\n===> Epoch[247](24/88): Loss_D: 0.2117 Loss_G: 1.4100\n===> Epoch[247](25/88): Loss_D: 0.2497 Loss_G: 1.1702\n===> Epoch[247](26/88): Loss_D: 0.2502 Loss_G: 1.1126\n===> Epoch[247](27/88): Loss_D: 0.1779 Loss_G: 1.4478\n===> Epoch[247](28/88): Loss_D: 0.2488 Loss_G: 1.1144\n===> Epoch[247](29/88): Loss_D: 0.2541 Loss_G: 1.2407\n===> Epoch[247](30/88): Loss_D: 0.2578 Loss_G: 1.0477\n===> Epoch[247](31/88): Loss_D: 0.2466 Loss_G: 1.0365\n===> Epoch[247](32/88): Loss_D: 0.2588 Loss_G: 1.1018\n===> Epoch[247](33/88): Loss_D: 0.2544 Loss_G: 1.1397\n===> Epoch[247](34/88): Loss_D: 0.2333 Loss_G: 1.2835\n===> Epoch[247](35/88): Loss_D: 0.2244 Loss_G: 1.2761\n===> Epoch[247](36/88): Loss_D: 0.2413 Loss_G: 1.2437\n===> Epoch[247](37/88): Loss_D: 0.2659 Loss_G: 1.1665\n===> Epoch[247](38/88): Loss_D: 0.2122 Loss_G: 1.6353\n===> Epoch[247](39/88): Loss_D: 0.2389 Loss_G: 1.1990\n===> Epoch[247](40/88): Loss_D: 0.2150 Loss_G: 1.9900\n===> Epoch[247](41/88): Loss_D: 0.2362 Loss_G: 1.4426\n===> Epoch[247](42/88): Loss_D: 0.2350 Loss_G: 1.3633\n===> Epoch[247](43/88): Loss_D: 0.2551 Loss_G: 1.2674\n===> Epoch[247](44/88): Loss_D: 0.2605 Loss_G: 1.2409\n===> Epoch[247](45/88): Loss_D: 0.2568 Loss_G: 1.0147\n===> Epoch[247](46/88): Loss_D: 0.2538 Loss_G: 0.9302\n===> Epoch[247](47/88): Loss_D: 0.2444 Loss_G: 1.2481\n===> Epoch[247](48/88): Loss_D: 0.2546 Loss_G: 1.1171\n===> Epoch[247](49/88): Loss_D: 0.2581 Loss_G: 1.1285\n===> Epoch[247](50/88): Loss_D: 0.2493 Loss_G: 1.1106\n===> Epoch[247](51/88): Loss_D: 0.2318 Loss_G: 1.4348\n===> Epoch[247](52/88): Loss_D: 0.2459 Loss_G: 1.0892\n===> Epoch[247](53/88): Loss_D: 0.2521 Loss_G: 1.0843\n===> Epoch[247](54/88): Loss_D: 0.2340 Loss_G: 1.3029\n===> Epoch[247](55/88): Loss_D: 0.2436 Loss_G: 1.1148\n===> Epoch[247](56/88): Loss_D: 0.2373 Loss_G: 1.1433\n===> Epoch[247](57/88): Loss_D: 0.2385 Loss_G: 1.3829\n===> Epoch[247](58/88): Loss_D: 0.2378 Loss_G: 1.3666\n===> Epoch[247](59/88): Loss_D: 0.2467 Loss_G: 1.1246\n===> Epoch[247](60/88): Loss_D: 0.2272 Loss_G: 1.2994\n===> Epoch[247](61/88): Loss_D: 0.2523 Loss_G: 0.9750\n===> Epoch[247](62/88): Loss_D: 0.2540 Loss_G: 1.0452\n===> Epoch[247](63/88): Loss_D: 0.2386 Loss_G: 1.2867\n===> Epoch[247](64/88): Loss_D: 0.2583 Loss_G: 1.2451\n===> Epoch[247](65/88): Loss_D: 0.2381 Loss_G: 1.2099\n===> Epoch[247](66/88): Loss_D: 0.2370 Loss_G: 1.2162\n===> Epoch[247](67/88): Loss_D: 0.2329 Loss_G: 1.2063\n===> Epoch[247](68/88): Loss_D: 0.2435 Loss_G: 1.2202\n===> Epoch[247](69/88): Loss_D: 0.2279 Loss_G: 1.3453\n===> Epoch[247](70/88): Loss_D: 0.2528 Loss_G: 1.1653\n===> Epoch[247](71/88): Loss_D: 0.2523 Loss_G: 1.1546\n===> Epoch[247](72/88): Loss_D: 0.2541 Loss_G: 1.0055\n===> Epoch[247](73/88): Loss_D: 0.2246 Loss_G: 1.2073\n===> Epoch[247](74/88): Loss_D: 0.2481 Loss_G: 1.1937\n===> Epoch[247](75/88): Loss_D: 0.2460 Loss_G: 1.1134\n===> Epoch[247](76/88): Loss_D: 0.2141 Loss_G: 1.3313\n===> Epoch[247](77/88): Loss_D: 0.2217 Loss_G: 1.3089\n===> Epoch[247](78/88): Loss_D: 0.2451 Loss_G: 1.1696\n===> Epoch[247](79/88): Loss_D: 0.2486 Loss_G: 1.2646\n===> Epoch[247](80/88): Loss_D: 0.2523 Loss_G: 1.2106\n===> Epoch[247](81/88): Loss_D: 0.2554 Loss_G: 1.0479\n===> Epoch[247](82/88): Loss_D: 0.2368 Loss_G: 1.2323\n===> Epoch[247](83/88): Loss_D: 0.2353 Loss_G: 1.1813\n===> Epoch[247](84/88): Loss_D: 0.2596 Loss_G: 1.0973\n===> Epoch[247](85/88): Loss_D: 0.2546 Loss_G: 1.1572\n===> Epoch[247](86/88): Loss_D: 0.2453 Loss_G: 1.1295\n===> Epoch[247](87/88): Loss_D: 0.2594 Loss_G: 1.1780\n===> Epoch[247](88/88): Loss_D: 0.2461 Loss_G: 1.2281\nlearning rate = 0.0000384\nlearning rate = 0.0000384\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0005} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[248](1/88): Loss_D: 0.2528 Loss_G: 1.0331\n===> Epoch[248](2/88): Loss_D: 0.2445 Loss_G: 1.3319\n===> Epoch[248](3/88): Loss_D: 0.2342 Loss_G: 1.3758\n===> Epoch[248](4/88): Loss_D: 0.2504 Loss_G: 1.0377\n===> Epoch[248](5/88): Loss_D: 0.2439 Loss_G: 1.1446\n===> Epoch[248](6/88): Loss_D: 0.2361 Loss_G: 1.3849\n===> Epoch[248](7/88): Loss_D: 0.2461 Loss_G: 1.1980\n===> Epoch[248](8/88): Loss_D: 0.1916 Loss_G: 2.1423\n===> Epoch[248](9/88): Loss_D: 0.2505 Loss_G: 1.0973\n===> Epoch[248](10/88): Loss_D: 0.2448 Loss_G: 1.2573\n===> Epoch[248](11/88): Loss_D: 0.2470 Loss_G: 1.2075\n===> Epoch[248](12/88): Loss_D: 0.2376 Loss_G: 1.2123\n===> Epoch[248](13/88): Loss_D: 0.2593 Loss_G: 1.2420\n===> Epoch[248](14/88): Loss_D: 0.2615 Loss_G: 1.1168\n===> Epoch[248](15/88): Loss_D: 0.2353 Loss_G: 1.2992\n===> Epoch[248](16/88): Loss_D: 0.2518 Loss_G: 1.1408\n===> Epoch[248](17/88): Loss_D: 0.2361 Loss_G: 1.2296\n===> Epoch[248](18/88): Loss_D: 0.2451 Loss_G: 1.2106\n===> Epoch[248](19/88): Loss_D: 0.2333 Loss_G: 1.2280\n===> Epoch[248](20/88): Loss_D: 0.2411 Loss_G: 1.2746\n===> Epoch[248](21/88): Loss_D: 0.2526 Loss_G: 1.1374\n===> Epoch[248](22/88): Loss_D: 0.2467 Loss_G: 1.1843\n===> Epoch[248](23/88): Loss_D: 0.2535 Loss_G: 1.0513\n===> Epoch[248](24/88): Loss_D: 0.2384 Loss_G: 1.1511\n===> Epoch[248](25/88): Loss_D: 0.2440 Loss_G: 1.0700\n===> Epoch[248](26/88): Loss_D: 0.2398 Loss_G: 1.3181\n===> Epoch[248](27/88): Loss_D: 0.2601 Loss_G: 1.0353\n===> Epoch[248](28/88): Loss_D: 0.2392 Loss_G: 1.0747\n===> Epoch[248](29/88): Loss_D: 0.1865 Loss_G: 1.5605\n===> Epoch[248](30/88): Loss_D: 0.2527 Loss_G: 1.0355\n===> Epoch[248](31/88): Loss_D: 0.2438 Loss_G: 1.1595\n===> Epoch[248](32/88): Loss_D: 0.2458 Loss_G: 1.0097\n===> Epoch[248](33/88): Loss_D: 0.2522 Loss_G: 1.0652\n===> Epoch[248](34/88): Loss_D: 0.2506 Loss_G: 1.0953\n===> Epoch[248](35/88): Loss_D: 0.1943 Loss_G: 1.6831\n===> Epoch[248](36/88): Loss_D: 0.2515 Loss_G: 1.2499\n===> Epoch[248](37/88): Loss_D: 0.2393 Loss_G: 1.2303\n===> Epoch[248](38/88): Loss_D: 0.2473 Loss_G: 1.1725\n===> Epoch[248](39/88): Loss_D: 0.2380 Loss_G: 1.3237\n===> Epoch[248](40/88): Loss_D: 0.2515 Loss_G: 0.8730\n===> Epoch[248](41/88): Loss_D: 0.2456 Loss_G: 1.0927\n===> Epoch[248](42/88): Loss_D: 0.2450 Loss_G: 1.1399\n===> Epoch[248](43/88): Loss_D: 0.2509 Loss_G: 1.2468\n===> Epoch[248](44/88): Loss_D: 0.2323 Loss_G: 1.1273\n===> Epoch[248](45/88): Loss_D: 0.2259 Loss_G: 1.1123\n===> Epoch[248](46/88): Loss_D: 0.2622 Loss_G: 0.9858\n===> Epoch[248](47/88): Loss_D: 0.2270 Loss_G: 1.0569\n===> Epoch[248](48/88): Loss_D: 0.2454 Loss_G: 1.2102\n===> Epoch[248](49/88): Loss_D: 0.2548 Loss_G: 1.1034\n===> Epoch[248](50/88): Loss_D: 0.2332 Loss_G: 1.4059\n===> Epoch[248](51/88): Loss_D: 0.2562 Loss_G: 1.1379\n===> Epoch[248](52/88): Loss_D: 0.2445 Loss_G: 1.1884\n===> Epoch[248](53/88): Loss_D: 0.2481 Loss_G: 1.2660\n===> Epoch[248](54/88): Loss_D: 0.2353 Loss_G: 1.2234\n===> Epoch[248](55/88): Loss_D: 0.2490 Loss_G: 0.9931\n===> Epoch[248](56/88): Loss_D: 0.2395 Loss_G: 1.0495\n===> Epoch[248](57/88): Loss_D: 0.1779 Loss_G: 1.4980\n===> Epoch[248](58/88): Loss_D: 0.2466 Loss_G: 1.2554\n===> Epoch[248](59/88): Loss_D: 0.2560 Loss_G: 1.0974\n===> Epoch[248](60/88): Loss_D: 0.1807 Loss_G: 1.4052\n===> Epoch[248](61/88): Loss_D: 0.2549 Loss_G: 1.1638\n===> Epoch[248](62/88): Loss_D: 0.2608 Loss_G: 1.1296\n===> Epoch[248](63/88): Loss_D: 0.2473 Loss_G: 1.1524\n===> Epoch[248](64/88): Loss_D: 0.2569 Loss_G: 0.9358\n===> Epoch[248](65/88): Loss_D: 0.2444 Loss_G: 1.1191\n===> Epoch[248](66/88): Loss_D: 0.2482 Loss_G: 1.1710\n===> Epoch[248](67/88): Loss_D: 0.2293 Loss_G: 1.2821\n===> Epoch[248](68/88): Loss_D: 0.2520 Loss_G: 1.2170\n===> Epoch[248](69/88): Loss_D: 0.2495 Loss_G: 1.0013\n===> Epoch[248](70/88): Loss_D: 0.2501 Loss_G: 1.1486\n===> Epoch[248](71/88): Loss_D: 0.2379 Loss_G: 1.0195\n===> Epoch[248](72/88): Loss_D: 0.2469 Loss_G: 1.2032\n===> Epoch[248](73/88): Loss_D: 0.2454 Loss_G: 1.0999\n===> Epoch[248](74/88): Loss_D: 0.2482 Loss_G: 1.2248\n===> Epoch[248](75/88): Loss_D: 0.2185 Loss_G: 1.2544\n===> Epoch[248](76/88): Loss_D: 0.2487 Loss_G: 1.0363\n===> Epoch[248](77/88): Loss_D: 0.2487 Loss_G: 1.1237\n===> Epoch[248](78/88): Loss_D: 0.2474 Loss_G: 1.1564\n===> Epoch[248](79/88): Loss_D: 0.2156 Loss_G: 1.4222\n===> Epoch[248](80/88): Loss_D: 0.1937 Loss_G: 1.4481\n===> Epoch[248](81/88): Loss_D: 0.2474 Loss_G: 1.2274\n===> Epoch[248](82/88): Loss_D: 0.2433 Loss_G: 1.3053\n===> Epoch[248](83/88): Loss_D: 0.2339 Loss_G: 1.2977\n===> Epoch[248](84/88): Loss_D: 0.2477 Loss_G: 1.4335\n===> Epoch[248](85/88): Loss_D: 0.2597 Loss_G: 1.1142\n===> Epoch[248](86/88): Loss_D: 0.2549 Loss_G: 1.1723\n===> Epoch[248](87/88): Loss_D: 0.2501 Loss_G: 1.1745\n===> Epoch[248](88/88): Loss_D: 0.2456 Loss_G: 1.1109\nlearning rate = 0.0000382\nlearning rate = 0.0000382\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0005} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[249](1/88): Loss_D: 0.2450 Loss_G: 1.1878\n===> Epoch[249](2/88): Loss_D: 0.2454 Loss_G: 1.0848\n===> Epoch[249](3/88): Loss_D: 0.2383 Loss_G: 1.2670\n===> Epoch[249](4/88): Loss_D: 0.2421 Loss_G: 1.1274\n===> Epoch[249](5/88): Loss_D: 0.2140 Loss_G: 1.3420\n===> Epoch[249](6/88): Loss_D: 0.2454 Loss_G: 1.1863\n===> Epoch[249](7/88): Loss_D: 0.2539 Loss_G: 1.1145\n===> Epoch[249](8/88): Loss_D: 0.2631 Loss_G: 0.9882\n===> Epoch[249](9/88): Loss_D: 0.2468 Loss_G: 1.2402\n===> Epoch[249](10/88): Loss_D: 0.2352 Loss_G: 1.2132\n===> Epoch[249](11/88): Loss_D: 0.2329 Loss_G: 1.3022\n===> Epoch[249](12/88): Loss_D: 0.2513 Loss_G: 1.1896\n===> Epoch[249](13/88): Loss_D: 0.2362 Loss_G: 1.2517\n===> Epoch[249](14/88): Loss_D: 0.1844 Loss_G: 1.7228\n===> Epoch[249](15/88): Loss_D: 0.2463 Loss_G: 1.2751\n===> Epoch[249](16/88): Loss_D: 0.2545 Loss_G: 0.9454\n===> Epoch[249](17/88): Loss_D: 0.2477 Loss_G: 1.3471\n===> Epoch[249](18/88): Loss_D: 0.2568 Loss_G: 1.0672\n===> Epoch[249](19/88): Loss_D: 0.2572 Loss_G: 1.0289\n===> Epoch[249](20/88): Loss_D: 0.2358 Loss_G: 1.4003\n===> Epoch[249](21/88): Loss_D: 0.2490 Loss_G: 1.2419\n===> Epoch[249](22/88): Loss_D: 0.1826 Loss_G: 1.4194\n===> Epoch[249](23/88): Loss_D: 0.2460 Loss_G: 1.1652\n===> Epoch[249](24/88): Loss_D: 0.2346 Loss_G: 1.2586\n===> Epoch[249](25/88): Loss_D: 0.2494 Loss_G: 1.0337\n===> Epoch[249](26/88): Loss_D: 0.1864 Loss_G: 1.4608\n===> Epoch[249](27/88): Loss_D: 0.2559 Loss_G: 1.0168\n===> Epoch[249](28/88): Loss_D: 0.2410 Loss_G: 1.8328\n===> Epoch[249](29/88): Loss_D: 0.2510 Loss_G: 1.1993\n===> Epoch[249](30/88): Loss_D: 0.2275 Loss_G: 1.3408\n===> Epoch[249](31/88): Loss_D: 0.2307 Loss_G: 1.2162\n===> Epoch[249](32/88): Loss_D: 0.2475 Loss_G: 1.3663\n===> Epoch[249](33/88): Loss_D: 0.2333 Loss_G: 1.2847\n===> Epoch[249](34/88): Loss_D: 0.2500 Loss_G: 1.2360\n===> Epoch[249](35/88): Loss_D: 0.2425 Loss_G: 1.0266\n===> Epoch[249](36/88): Loss_D: 0.2442 Loss_G: 1.2135\n===> Epoch[249](37/88): Loss_D: 0.2575 Loss_G: 1.0450\n===> Epoch[249](38/88): Loss_D: 0.2563 Loss_G: 1.1870\n===> Epoch[249](39/88): Loss_D: 0.2457 Loss_G: 1.0756\n===> Epoch[249](40/88): Loss_D: 0.2486 Loss_G: 1.0248\n===> Epoch[249](41/88): Loss_D: 0.2449 Loss_G: 1.2240\n===> Epoch[249](42/88): Loss_D: 0.2252 Loss_G: 1.1012\n===> Epoch[249](43/88): Loss_D: 0.2357 Loss_G: 1.2544\n===> Epoch[249](44/88): Loss_D: 0.2661 Loss_G: 1.0696\n===> Epoch[249](45/88): Loss_D: 0.2553 Loss_G: 0.9880\n===> Epoch[249](46/88): Loss_D: 0.2475 Loss_G: 1.1961\n===> Epoch[249](47/88): Loss_D: 0.2485 Loss_G: 1.1554\n===> Epoch[249](48/88): Loss_D: 0.2512 Loss_G: 1.1930\n===> Epoch[249](49/88): Loss_D: 0.2555 Loss_G: 1.1684\n===> Epoch[249](50/88): Loss_D: 0.2601 Loss_G: 1.1005\n===> Epoch[249](51/88): Loss_D: 0.2320 Loss_G: 1.4449\n===> Epoch[249](52/88): Loss_D: 0.2293 Loss_G: 1.5418\n===> Epoch[249](53/88): Loss_D: 0.2414 Loss_G: 1.3456\n===> Epoch[249](54/88): Loss_D: 0.2454 Loss_G: 1.1584\n===> Epoch[249](55/88): Loss_D: 0.2590 Loss_G: 1.1448\n===> Epoch[249](56/88): Loss_D: 0.2548 Loss_G: 1.1998\n===> Epoch[249](57/88): Loss_D: 0.2403 Loss_G: 1.1562\n===> Epoch[249](58/88): Loss_D: 0.2487 Loss_G: 1.0704\n===> Epoch[249](59/88): Loss_D: 0.2535 Loss_G: 1.0171\n===> Epoch[249](60/88): Loss_D: 0.2137 Loss_G: 1.3932\n===> Epoch[249](61/88): Loss_D: 0.2568 Loss_G: 1.0809\n===> Epoch[249](62/88): Loss_D: 0.2552 Loss_G: 1.1742\n===> Epoch[249](63/88): Loss_D: 0.2483 Loss_G: 1.0694\n===> Epoch[249](64/88): Loss_D: 0.2520 Loss_G: 1.1484\n===> Epoch[249](65/88): Loss_D: 0.2423 Loss_G: 1.3788\n===> Epoch[249](66/88): Loss_D: 0.2476 Loss_G: 1.2491\n===> Epoch[249](67/88): Loss_D: 0.2533 Loss_G: 1.1993\n===> Epoch[249](68/88): Loss_D: 0.2462 Loss_G: 1.1226\n===> Epoch[249](69/88): Loss_D: 0.2441 Loss_G: 1.1675\n===> Epoch[249](70/88): Loss_D: 0.2410 Loss_G: 1.2524\n===> Epoch[249](71/88): Loss_D: 0.2513 Loss_G: 1.1685\n===> Epoch[249](72/88): Loss_D: 0.2504 Loss_G: 1.1532\n===> Epoch[249](73/88): Loss_D: 0.2511 Loss_G: 1.1686\n===> Epoch[249](74/88): Loss_D: 0.2318 Loss_G: 1.1859\n===> Epoch[249](75/88): Loss_D: 0.2473 Loss_G: 1.1085\n===> Epoch[249](76/88): Loss_D: 0.2398 Loss_G: 1.0571\n===> Epoch[249](77/88): Loss_D: 0.2445 Loss_G: 1.0933\n===> Epoch[249](78/88): Loss_D: 0.2466 Loss_G: 1.1438\n===> Epoch[249](79/88): Loss_D: 0.2470 Loss_G: 1.1187\n===> Epoch[249](80/88): Loss_D: 0.2407 Loss_G: 1.2218\n===> Epoch[249](81/88): Loss_D: 0.2551 Loss_G: 0.8913\n===> Epoch[249](82/88): Loss_D: 0.2447 Loss_G: 1.1094\n===> Epoch[249](83/88): Loss_D: 0.2525 Loss_G: 1.0038\n===> Epoch[249](84/88): Loss_D: 0.2567 Loss_G: 0.9521\n===> Epoch[249](85/88): Loss_D: 0.2513 Loss_G: 1.3496\n===> Epoch[249](86/88): Loss_D: 0.2376 Loss_G: 1.3546\n===> Epoch[249](87/88): Loss_D: 0.1618 Loss_G: 1.4791\n===> Epoch[249](88/88): Loss_D: 0.2430 Loss_G: 1.3374\nlearning rate = 0.0000379\nlearning rate = 0.0000379\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0004} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[250](1/88): Loss_D: 0.2496 Loss_G: 1.1502\n===> Epoch[250](2/88): Loss_D: 0.2495 Loss_G: 1.0095\n===> Epoch[250](3/88): Loss_D: 0.2385 Loss_G: 1.2840\n===> Epoch[250](4/88): Loss_D: 0.2450 Loss_G: 1.2145\n===> Epoch[250](5/88): Loss_D: 0.2544 Loss_G: 0.9207\n===> Epoch[250](6/88): Loss_D: 0.2495 Loss_G: 1.0791\n===> Epoch[250](7/88): Loss_D: 0.2568 Loss_G: 0.9706\n===> Epoch[250](8/88): Loss_D: 0.2469 Loss_G: 1.0366\n===> Epoch[250](9/88): Loss_D: 0.1885 Loss_G: 1.7712\n===> Epoch[250](10/88): Loss_D: 0.2401 Loss_G: 1.2106\n===> Epoch[250](11/88): Loss_D: 0.2412 Loss_G: 1.1716\n===> Epoch[250](12/88): Loss_D: 0.2384 Loss_G: 1.4198\n===> Epoch[250](13/88): Loss_D: 0.2399 Loss_G: 1.2886\n===> Epoch[250](14/88): Loss_D: 0.2528 Loss_G: 1.1927\n===> Epoch[250](15/88): Loss_D: 0.2398 Loss_G: 1.2115\n===> Epoch[250](16/88): Loss_D: 0.2578 Loss_G: 0.9651\n===> Epoch[250](17/88): Loss_D: 0.1697 Loss_G: 1.4880\n===> Epoch[250](18/88): Loss_D: 0.2500 Loss_G: 0.8810\n===> Epoch[250](19/88): Loss_D: 0.2324 Loss_G: 1.1858\n===> Epoch[250](20/88): Loss_D: 0.2533 Loss_G: 1.1695\n===> Epoch[250](21/88): Loss_D: 0.2489 Loss_G: 1.2292\n===> Epoch[250](22/88): Loss_D: 0.2246 Loss_G: 1.3765\n===> Epoch[250](23/88): Loss_D: 0.1772 Loss_G: 1.5457\n===> Epoch[250](24/88): Loss_D: 0.2588 Loss_G: 1.1688\n===> Epoch[250](25/88): Loss_D: 0.2425 Loss_G: 1.1274\n===> Epoch[250](26/88): Loss_D: 0.2550 Loss_G: 1.1933\n===> Epoch[250](27/88): Loss_D: 0.2540 Loss_G: 1.2136\n===> Epoch[250](28/88): Loss_D: 0.2494 Loss_G: 1.0528\n===> Epoch[250](29/88): Loss_D: 0.2422 Loss_G: 1.1408\n===> Epoch[250](30/88): Loss_D: 0.2448 Loss_G: 1.0284\n===> Epoch[250](31/88): Loss_D: 0.2531 Loss_G: 1.0831\n===> Epoch[250](32/88): Loss_D: 0.2332 Loss_G: 1.1999\n===> Epoch[250](33/88): Loss_D: 0.2619 Loss_G: 1.0111\n===> Epoch[250](34/88): Loss_D: 0.2310 Loss_G: 1.2791\n===> Epoch[250](35/88): Loss_D: 0.1850 Loss_G: 1.4423\n===> Epoch[250](36/88): Loss_D: 0.2025 Loss_G: 1.4235\n===> Epoch[250](37/88): Loss_D: 0.2432 Loss_G: 1.3909\n===> Epoch[250](38/88): Loss_D: 0.2470 Loss_G: 1.1192\n===> Epoch[250](39/88): Loss_D: 0.2536 Loss_G: 1.1870\n===> Epoch[250](40/88): Loss_D: 0.2551 Loss_G: 1.2064\n===> Epoch[250](41/88): Loss_D: 0.2439 Loss_G: 1.2512\n===> Epoch[250](42/88): Loss_D: 0.2492 Loss_G: 1.0766\n===> Epoch[250](43/88): Loss_D: 0.2323 Loss_G: 1.2755\n===> Epoch[250](44/88): Loss_D: 0.2511 Loss_G: 1.2041\n===> Epoch[250](45/88): Loss_D: 0.2544 Loss_G: 1.1731\n===> Epoch[250](46/88): Loss_D: 0.2496 Loss_G: 1.0800\n===> Epoch[250](47/88): Loss_D: 0.2517 Loss_G: 0.9698\n===> Epoch[250](48/88): Loss_D: 0.2475 Loss_G: 1.1318\n===> Epoch[250](49/88): Loss_D: 0.2483 Loss_G: 1.1367\n===> Epoch[250](50/88): Loss_D: 0.2475 Loss_G: 1.1254\n===> Epoch[250](51/88): Loss_D: 0.2441 Loss_G: 0.9828\n===> Epoch[250](52/88): Loss_D: 0.2356 Loss_G: 1.3249\n===> Epoch[250](53/88): Loss_D: 0.2476 Loss_G: 1.0679\n===> Epoch[250](54/88): Loss_D: 0.2356 Loss_G: 1.1444\n===> Epoch[250](55/88): Loss_D: 0.2162 Loss_G: 2.0279\n===> Epoch[250](56/88): Loss_D: 0.2574 Loss_G: 1.2041\n===> Epoch[250](57/88): Loss_D: 0.2151 Loss_G: 1.3727\n===> Epoch[250](58/88): Loss_D: 0.2628 Loss_G: 1.1311\n===> Epoch[250](59/88): Loss_D: 0.2467 Loss_G: 1.1284\n===> Epoch[250](60/88): Loss_D: 0.2492 Loss_G: 1.2161\n===> Epoch[250](61/88): Loss_D: 0.2524 Loss_G: 1.0974\n===> Epoch[250](62/88): Loss_D: 0.2625 Loss_G: 1.0320\n===> Epoch[250](63/88): Loss_D: 0.2337 Loss_G: 1.2422\n===> Epoch[250](64/88): Loss_D: 0.2434 Loss_G: 1.3153\n===> Epoch[250](65/88): Loss_D: 0.2334 Loss_G: 1.1843\n===> Epoch[250](66/88): Loss_D: 0.2349 Loss_G: 1.2410\n===> Epoch[250](67/88): Loss_D: 0.2506 Loss_G: 1.1833\n===> Epoch[250](68/88): Loss_D: 0.2528 Loss_G: 1.0040\n===> Epoch[250](69/88): Loss_D: 0.2548 Loss_G: 1.0505\n===> Epoch[250](70/88): Loss_D: 0.2383 Loss_G: 1.3081\n===> Epoch[250](71/88): Loss_D: 0.2491 Loss_G: 1.0829\n===> Epoch[250](72/88): Loss_D: 0.2554 Loss_G: 1.0585\n===> Epoch[250](73/88): Loss_D: 0.2341 Loss_G: 1.3334\n===> Epoch[250](74/88): Loss_D: 0.2431 Loss_G: 1.2435\n===> Epoch[250](75/88): Loss_D: 0.2490 Loss_G: 1.0401\n===> Epoch[250](76/88): Loss_D: 0.2436 Loss_G: 1.1008\n===> Epoch[250](77/88): Loss_D: 0.2289 Loss_G: 1.2711\n===> Epoch[250](78/88): Loss_D: 0.2488 Loss_G: 1.1285\n===> Epoch[250](79/88): Loss_D: 0.2526 Loss_G: 1.1455\n===> Epoch[250](80/88): Loss_D: 0.2616 Loss_G: 0.9788\n===> Epoch[250](81/88): Loss_D: 0.2463 Loss_G: 1.1768\n===> Epoch[250](82/88): Loss_D: 0.2449 Loss_G: 1.1870\n===> Epoch[250](83/88): Loss_D: 0.2472 Loss_G: 1.1670\n===> Epoch[250](84/88): Loss_D: 0.2302 Loss_G: 1.3231\n===> Epoch[250](85/88): Loss_D: 0.2399 Loss_G: 1.3000\n===> Epoch[250](86/88): Loss_D: 0.2427 Loss_G: 1.2246\n===> Epoch[250](87/88): Loss_D: 0.2392 Loss_G: 1.1940\n===> Epoch[250](88/88): Loss_D: 0.2318 Loss_G: 1.3623\nlearning rate = 0.0000377\nlearning rate = 0.0000377\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0008} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[251](1/88): Loss_D: 0.2427 Loss_G: 1.1306\n===> Epoch[251](2/88): Loss_D: 0.2333 Loss_G: 1.2462\n===> Epoch[251](3/88): Loss_D: 0.2351 Loss_G: 1.1768\n===> Epoch[251](4/88): Loss_D: 0.2535 Loss_G: 1.4299\n===> Epoch[251](5/88): Loss_D: 0.2613 Loss_G: 1.0395\n===> Epoch[251](6/88): Loss_D: 0.2277 Loss_G: 1.1256\n===> Epoch[251](7/88): Loss_D: 0.2478 Loss_G: 0.9774\n===> Epoch[251](8/88): Loss_D: 0.2406 Loss_G: 1.2396\n===> Epoch[251](9/88): Loss_D: 0.2518 Loss_G: 1.0080\n===> Epoch[251](10/88): Loss_D: 0.2278 Loss_G: 1.1321\n===> Epoch[251](11/88): Loss_D: 0.2407 Loss_G: 1.1722\n===> Epoch[251](12/88): Loss_D: 0.2468 Loss_G: 1.1017\n===> Epoch[251](13/88): Loss_D: 0.2463 Loss_G: 1.2119\n===> Epoch[251](14/88): Loss_D: 0.2403 Loss_G: 1.1951\n===> Epoch[251](15/88): Loss_D: 0.2435 Loss_G: 1.3404\n===> Epoch[251](16/88): Loss_D: 0.2416 Loss_G: 1.1758\n===> Epoch[251](17/88): Loss_D: 0.2330 Loss_G: 1.2194\n===> Epoch[251](18/88): Loss_D: 0.2392 Loss_G: 1.2675\n===> Epoch[251](19/88): Loss_D: 0.2468 Loss_G: 1.1197\n===> Epoch[251](20/88): Loss_D: 0.2157 Loss_G: 1.3266\n===> Epoch[251](21/88): Loss_D: 0.1825 Loss_G: 1.6426\n===> Epoch[251](22/88): Loss_D: 0.2558 Loss_G: 1.0904\n===> Epoch[251](23/88): Loss_D: 0.2648 Loss_G: 1.0517\n===> Epoch[251](24/88): Loss_D: 0.2639 Loss_G: 1.0534\n===> Epoch[251](25/88): Loss_D: 0.2397 Loss_G: 1.3100\n===> Epoch[251](26/88): Loss_D: 0.2379 Loss_G: 1.1140\n===> Epoch[251](27/88): Loss_D: 0.2554 Loss_G: 1.1093\n===> Epoch[251](28/88): Loss_D: 0.2409 Loss_G: 1.2054\n===> Epoch[251](29/88): Loss_D: 0.2450 Loss_G: 1.2496\n===> Epoch[251](30/88): Loss_D: 0.2643 Loss_G: 1.0009\n===> Epoch[251](31/88): Loss_D: 0.2467 Loss_G: 1.0901\n===> Epoch[251](32/88): Loss_D: 0.2415 Loss_G: 1.1622\n===> Epoch[251](33/88): Loss_D: 0.2429 Loss_G: 1.0714\n===> Epoch[251](34/88): Loss_D: 0.2509 Loss_G: 1.0214\n===> Epoch[251](35/88): Loss_D: 0.2520 Loss_G: 1.1062\n===> Epoch[251](36/88): Loss_D: 0.2302 Loss_G: 1.3349\n===> Epoch[251](37/88): Loss_D: 0.2483 Loss_G: 1.1694\n===> Epoch[251](38/88): Loss_D: 0.2420 Loss_G: 1.3126\n===> Epoch[251](39/88): Loss_D: 0.1906 Loss_G: 1.5368\n===> Epoch[251](40/88): Loss_D: 0.2552 Loss_G: 1.2443\n===> Epoch[251](41/88): Loss_D: 0.2370 Loss_G: 1.3399\n===> Epoch[251](42/88): Loss_D: 0.2492 Loss_G: 1.0503\n===> Epoch[251](43/88): Loss_D: 0.2538 Loss_G: 0.9653\n===> Epoch[251](44/88): Loss_D: 0.2038 Loss_G: 1.6800\n===> Epoch[251](45/88): Loss_D: 0.2399 Loss_G: 1.2089\n===> Epoch[251](46/88): Loss_D: 0.2364 Loss_G: 1.1083\n===> Epoch[251](47/88): Loss_D: 0.2521 Loss_G: 1.0752\n===> Epoch[251](48/88): Loss_D: 0.2528 Loss_G: 1.1758\n===> Epoch[251](49/88): Loss_D: 0.2475 Loss_G: 1.1215\n===> Epoch[251](50/88): Loss_D: 0.2504 Loss_G: 1.0374\n===> Epoch[251](51/88): Loss_D: 0.2519 Loss_G: 1.2743\n===> Epoch[251](52/88): Loss_D: 0.2063 Loss_G: 1.3641\n===> Epoch[251](53/88): Loss_D: 0.2488 Loss_G: 1.2065\n===> Epoch[251](54/88): Loss_D: 0.2548 Loss_G: 1.1563\n===> Epoch[251](55/88): Loss_D: 0.2502 Loss_G: 1.1602\n===> Epoch[251](56/88): Loss_D: 0.1946 Loss_G: 2.0029\n===> Epoch[251](57/88): Loss_D: 0.2432 Loss_G: 1.1135\n===> Epoch[251](58/88): Loss_D: 0.2500 Loss_G: 1.1368\n===> Epoch[251](59/88): Loss_D: 0.2574 Loss_G: 1.1919\n===> Epoch[251](60/88): Loss_D: 0.2470 Loss_G: 1.3275\n===> Epoch[251](61/88): Loss_D: 0.2519 Loss_G: 1.1827\n===> Epoch[251](62/88): Loss_D: 0.2306 Loss_G: 1.3526\n===> Epoch[251](63/88): Loss_D: 0.2400 Loss_G: 1.1558\n===> Epoch[251](64/88): Loss_D: 0.2522 Loss_G: 1.1560\n===> Epoch[251](65/88): Loss_D: 0.2534 Loss_G: 1.0865\n===> Epoch[251](66/88): Loss_D: 0.2343 Loss_G: 1.2677\n===> Epoch[251](67/88): Loss_D: 0.2505 Loss_G: 1.0089\n===> Epoch[251](68/88): Loss_D: 0.2170 Loss_G: 1.2831\n===> Epoch[251](69/88): Loss_D: 0.2440 Loss_G: 1.2691\n===> Epoch[251](70/88): Loss_D: 0.2514 Loss_G: 0.8808\n===> Epoch[251](71/88): Loss_D: 0.2076 Loss_G: 1.4186\n===> Epoch[251](72/88): Loss_D: 0.2459 Loss_G: 1.2474\n===> Epoch[251](73/88): Loss_D: 0.2558 Loss_G: 1.0264\n===> Epoch[251](74/88): Loss_D: 0.2502 Loss_G: 1.0965\n===> Epoch[251](75/88): Loss_D: 0.2515 Loss_G: 1.2087\n===> Epoch[251](76/88): Loss_D: 0.2456 Loss_G: 1.3603\n===> Epoch[251](77/88): Loss_D: 0.2342 Loss_G: 1.0380\n===> Epoch[251](78/88): Loss_D: 0.2409 Loss_G: 1.3108\n===> Epoch[251](79/88): Loss_D: 0.2512 Loss_G: 1.1317\n===> Epoch[251](80/88): Loss_D: 0.2520 Loss_G: 1.0916\n===> Epoch[251](81/88): Loss_D: 0.2422 Loss_G: 1.3161\n===> Epoch[251](82/88): Loss_D: 0.2472 Loss_G: 1.2018\n===> Epoch[251](83/88): Loss_D: 0.2501 Loss_G: 0.9398\n===> Epoch[251](84/88): Loss_D: 0.1685 Loss_G: 1.4777\n===> Epoch[251](85/88): Loss_D: 0.2535 Loss_G: 1.1951\n===> Epoch[251](86/88): Loss_D: 0.2563 Loss_G: 1.2437\n===> Epoch[251](87/88): Loss_D: 0.2320 Loss_G: 1.2664\n===> Epoch[251](88/88): Loss_D: 0.2560 Loss_G: 1.2355\nlearning rate = 0.0000374\nlearning rate = 0.0000374\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n===> FID: {0.0000} \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n","output_type":"stream"},{"name":"stdout","text":"===> Epoch[252](1/88): Loss_D: 0.2498 Loss_G: 1.1236\n===> Epoch[252](2/88): Loss_D: 0.2381 Loss_G: 1.2432\n===> Epoch[252](3/88): Loss_D: 0.2461 Loss_G: 1.1113\n===> Epoch[252](4/88): Loss_D: 0.2364 Loss_G: 1.1077\n===> Epoch[252](5/88): Loss_D: 0.2415 Loss_G: 1.2439\n===> Epoch[252](6/88): Loss_D: 0.2336 Loss_G: 1.3959\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(loss_gen,label=\"G\")\nplt.plot(loss_dis,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}